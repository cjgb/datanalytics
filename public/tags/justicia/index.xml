<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>justicia on datanalytics</title>
    <link>/tags/justicia/</link>
    <description>Recent content in justicia on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Thu, 11 Feb 2021 09:13:00 +0000</lastBuildDate><atom:link href="/tags/justicia/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Solo el modelo vacío pasa todos los &#34;checks&#34;</title>
      <link>/2021/02/11/solo-el-modelo-vacio-pasa-todos-los-checks/</link>
      <pubDate>Thu, 11 Feb 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/02/11/solo-el-modelo-vacio-pasa-todos-los-checks/</guid>
      <description>Cuando uno crea uno de esos modelos que tanta mala fama tienen hoy en día —y sí, me refiero a esos de los que dependen las concesiones de hipotecas, etc.— solo tiene dos fuentes de datos:
 La llamada información _estadística _acerca de los sujetos: donde vive, sexo, edad, etc. * Información personal sobre el sujeto: cómo se ha comportado en el pasado.  Sin embargo, aquí se nos informa de cómo ha sido multado un banco finlandés por</description>
    </item>
    
    <item>
      <title>Un marco conceptual para repensar los presuntos sesgos del AI, ML, etc.</title>
      <link>/2020/06/11/un-marco-conceptual-para-repensar-los-presuntos-sesgos-del-ai-ml-etc/</link>
      <pubDate>Thu, 11 Jun 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/06/11/un-marco-conceptual-para-repensar-los-presuntos-sesgos-del-ai-ml-etc/</guid>
      <description>He escrito en alguna ocasión sobre el tema: véanse (algunas de) las entradas con etiquetas sesgo, discriminación o justicia. Recientemente he releído un artículo de Joseph Heath, Redefining racism (adivinad por qué) que mutatis mutandis, ofrece un marco conceptual muy adecuado para repensar el asunto (pista: todo lo que se refiere al llamado racismo institucional).
Nota: si este fuese un blog al uso y yo tuviese más tiempo del que dispongo, resumiría ese artículo induciéndoos a privaros del placer de leer el original y luego desarrollaría el paralelismo ofendiendo a la inteligencia de los lectores que más me importan.</description>
    </item>
    
    <item>
      <title>Una versión aún más sencilla</title>
      <link>/2020/02/27/una-version-aun-mas-sencilla/</link>
      <pubDate>Thu, 27 Feb 2020 17:31:00 +0000</pubDate>
      
      <guid>/2020/02/27/una-version-aun-mas-sencilla/</guid>
      <description>&amp;hellip; que la de &amp;ldquo;Algoritmos&amp;rdquo; y acatarrantes definiciones de &amp;ldquo;justicia&amp;rdquo;. Que es casi una versión de la anterior reduciendo la varianza de las betas.
Las dos poblaciones de interés tienen una tasa de probabilidad (o de riesgo, en la terminología del artículo original) de .4 y .6 respectivamente. Aproximadamente el 40% de los primeros y el 60% de los segundos tienen y = 1.
El modelo (el algoritmo) es perfecto y asigna a los integrantes del primer grupo un scoring de .</description>
    </item>
    
    <item>
      <title>&#34;Algoritmos&#34; y acatarrantes definiciones de &#34;justicia&#34;</title>
      <link>/2020/02/26/algoritmos-y-acatarrantes-definiciones-de-justicia/</link>
      <pubDate>Wed, 26 Feb 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/02/26/algoritmos-y-acatarrantes-definiciones-de-justicia/</guid>
      <description>Lee Justicia: los límites de la inteligencia artificial&amp;hellip; y humana y cuando acabes, te propongo un pequeño experimento probabilístico. Por referencia, reproduzco aquí los criterios de justicia del artículo que glosa el que enlazo:
Centrémonos en (B), sabiendo que, por simetría, lo que cuento se aplica también a (C).
Supongamos que tenemos dos grupos, cada uno de ellos de
n &amp;lt;- 1000000  personas para estar en las asíntotas que aman los frecuentistas.</description>
    </item>
    
  </channel>
</rss>

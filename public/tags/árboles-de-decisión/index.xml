<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>árboles de decisión on datanalytics</title>
    <link>/tags/%C3%A1rboles-de-decisi%C3%B3n/</link>
    <description>Recent content in árboles de decisión on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Wed, 12 Feb 2020 09:13:00 +0000</lastBuildDate><atom:link href="/tags/%C3%A1rboles-de-decisi%C3%B3n/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>No sé cómo traducir &#34;Partially additive (generalized) linear model trees&#34;</title>
      <link>/2020/02/12/no-se-como-traducir-partially-additive-generalized-linear-model-trees/</link>
      <pubDate>Wed, 12 Feb 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/02/12/no-se-como-traducir-partially-additive-generalized-linear-model-trees/</guid>
      <description>Sin embargo, basta con mirar la foto
leer la entrada de hace unos días, que se refiere a algo muy parecido (y que, en particular, describe los datos usados en el modelo que representa) y, en el peor de los casos, esto, para hacerse idea de su utilidad y relevancia.</description>
    </item>
    
    <item>
      <title>Repensando la codificación por impacto</title>
      <link>/2017/01/10/repensando-la-codificacion-por-impacto/</link>
      <pubDate>Tue, 10 Jan 2017 08:13:49 +0000</pubDate>
      
      <guid>/2017/01/10/repensando-la-codificacion-por-impacto/</guid>
      <description>Hay una entrada mía, esta, que me ronda la cabeza y con la que no sé si estoy completamente de acuerdo. Trata de justificar la codificación por impacto de variables categóricas en modelos lineales (generalizados o no) y cuanto más la releo, menos me la creo. O, más bien, comienzo a cuestinarme más seriamente contextos en los que funciona y contextos en los que no.
Pero comencemos por uno simple: los árboles.</description>
    </item>
    
    <item>
      <title>Rápido y frugal: una digresión en la dirección inhabitual</title>
      <link>/2016/07/13/rapido-y-frugal-una-digresion-en-la-direccion-inhabitual/</link>
      <pubDate>Wed, 13 Jul 2016 08:13:39 +0000</pubDate>
      
      <guid>/2016/07/13/rapido-y-frugal-una-digresion-en-la-direccion-inhabitual/</guid>
      <description>Siempre (aténganse los puristas al contexto) recomiendo comenzar con un árbol de decisión para, sobre esa base, ensayar métodos más potentes. Sobre todo si la precisión conviene más que la interpretabilidad.
En la dirección opuesta se sitúan los árboles rápidos y frugales. Un árbol rápido y frugal es un tipo de árbol de decisión tal como
La restricción que satisface (a diferencia de los árboles de decisión más habituales) es que:</description>
    </item>
    
    <item>
      <title>Discretización de variables continuas (con árboles)</title>
      <link>/2016/04/25/discretizacion-de-variables-continuas-con-arboles/</link>
      <pubDate>Mon, 25 Apr 2016 09:13:42 +0000</pubDate>
      
      <guid>/2016/04/25/discretizacion-de-variables-continuas-con-arboles/</guid>
      <description>La primera entrada de esta bitácora es de enero de 2010. En aquella época, recuerdo, había apartado un artículo sobre categorización de variables continuas, i.e., el proceso de convertir (¿para qué?) una variable continua en categórica de una manera óptima.
Aparte de cuestionar el paraqué (¿por qué porqué es sustantivo y paraqué no?) de la cosa me asaltaron dudas sobre el cómo. Si se quiere discretizar, ¿por qué no usar directamente un árbol?</description>
    </item>
    
    <item>
      <title>evtree: árboles globales</title>
      <link>/2015/01/12/evtree-arboles-globales/</link>
      <pubDate>Mon, 12 Jan 2015 07:13:44 +0000</pubDate>
      
      <guid>/2015/01/12/evtree-arboles-globales/</guid>
      <description>Tengo por delante otro proyecto que tiene mucho de análisis exploratorio de datos. Sospecho que más de un árbol construiré. Los árboles son como la Wikipedia: prácticamente nunca el último pero casi siempre el primer recurso.
Esta vez, además, por entretenerme un poco, probaré el paquete [evtree](http://cran.r-project.org/web/packages/evtree/index.html). Aunque no porque espere sorprendentes mejoras con respecto a los tradicionales, ctree y rpart.
¿Qué tiene aquél que los diferencie de los otros dos?</description>
    </item>
    
    <item>
      <title>Bajo el capó del particionamiento recursivo basado en modelos</title>
      <link>/2014/09/12/bajo-el-capo-del-particionamiento-recursivo-basado-en-modelos/</link>
      <pubDate>Fri, 12 Sep 2014 07:13:31 +0000</pubDate>
      
      <guid>/2014/09/12/bajo-el-capo-del-particionamiento-recursivo-basado-en-modelos/</guid>
      <description>Una de las mayores contrariedades de estar sentado cerca de alguien que es más matemático que un servidor (de Vds., no de silicio) es que oye siempre preguntar por qué. Una letanía de preguntas me condujo a leer papelotes que ahora resumo.
Primero, unos datos:
&amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/base/set.seed&amp;quot;&amp;gt;set.seed(1234) n &amp;lt;- 100 x1 &amp;lt;- rnorm(n) x2 &amp;lt;- rnorm(n) x3 &amp;lt;- rnorm(n) y &amp;lt;- 0.3 + 0.2 * x1 + 0.5 * (x2 &amp;gt; 0) + 0.</description>
    </item>
    
    <item>
      <title>Incrementalidad via particionamiento recursivo basado en modelos</title>
      <link>/2014/07/30/incrementalidad-via-particionamiento-recursivo-basado-en-modelos/</link>
      <pubDate>Wed, 30 Jul 2014 07:13:24 +0000</pubDate>
      
      <guid>/2014/07/30/incrementalidad-via-particionamiento-recursivo-basado-en-modelos/</guid>
      <description>Planteas un modelo tal como resp ~ treat y no encuentras diferencia significativa. O incluso puede ser negativa. Globalmente.
La pregunta es, con el permiso del Sr. Simpson (o tal vez inspirados por él), ¿existirá alguna región del espacio en la que el tratamiento tiene un efecto beneficioso? Puede que sí. Y de haberla, ¿cómo identificarla?
De eso hablo hoy aquí. E incluyo una protorespuesta.
Primero, genero datos:
n &amp;lt;- 20000 v1 &amp;lt;- sample(0:1, n, replace = T) v2 &amp;lt;- sample(0:1, n, replace = T) v3 &amp;lt;- sample(0:1, n, replace = T) treat &amp;lt;- sample(0:1, n, replace = T) y &amp;lt;- v1 + treat * v1 * v2 y &amp;lt;- exp(y) / (1 + exp(y)) y &amp;lt;- sapply(y, function(x) rbinom(1,1,x)) dat &amp;lt;- data.</description>
    </item>
    
    <item>
      <title>En recuerdo de Leo Breiman</title>
      <link>/2014/01/23/en-recuerdo-de-leo-breiman/</link>
      <pubDate>Thu, 23 Jan 2014 08:41:09 +0000</pubDate>
      
      <guid>/2014/01/23/en-recuerdo-de-leo-breiman/</guid>
      <description>Recomiendo leer esto. Es un artículo que repasa la labor de Leo Breiman, pionero en esa nueva forma de plantear el análisis de datos que acabó convirtiéndose en la minería de datos y de algunos de los algoritmos y métodos más comunes que conforman la caja de herramientas de quienes lo practican hoy en día. Entre ellos, los árboles de decisión y de regresión y los random forests.
Así comienza el artículo:</description>
    </item>
    
  </channel>
</rss>

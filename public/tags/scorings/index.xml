<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scorings on datanalytics</title>
    <link>/tags/scorings/</link>
    <description>Recent content in scorings on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Mon, 11 Feb 2019 08:13:02 +0000</lastBuildDate><atom:link href="/tags/scorings/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AUC = Wilcoxon</title>
      <link>/2019/02/11/auc-wilcoxon/</link>
      <pubDate>Mon, 11 Feb 2019 08:13:02 +0000</pubDate>
      
      <guid>/2019/02/11/auc-wilcoxon/</guid>
      <description>Construyo unos datos,
n &amp;lt;- 30 si &amp;lt;- data.frame(res = &amp;quot;si&amp;quot;, score = rnorm(n, 1, 1)) no &amp;lt;- data.frame(res = &amp;quot;no&amp;quot;, score = rnorm(n, 0, 1)) dat &amp;lt;- rbind(si, no)  que simulan los scorings de un modelo hipótetico en el que comparo unos casos positivos y otros negativos.
Comparo con el test de Wilcoxon el scoring según la etiqueta y normalizo (adecuadamente):
test &amp;lt;- wilcox.test(score ~ res, data = dat)$statistic test / n^2  Por otro lado calculo el AUC:</description>
    </item>
    
    <item>
      <title>Reglas de &#34;scoring&#34; impropias: un ejemplo</title>
      <link>/2019/01/23/reglas-de-scoring-impropias-un-ejemplo/</link>
      <pubDate>Wed, 23 Jan 2019 08:13:07 +0000</pubDate>
      
      <guid>/2019/01/23/reglas-de-scoring-impropias-un-ejemplo/</guid>
      <description>Todo lo que he venido escribiendo sobre reglas de scoring propias vino en el fondo motivado por Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules, una entrada en el blog de Frank Harrell en la que se discute el siguiente caso.
El tipo simula unos datos para ser ajustados mediante una regresión logística (de manera que conoce la verdad subyacente). Después construye varios modelos alternativos para ajustarlos y utiliza varios scorings distintos para seleccionar el mejor modelo.</description>
    </item>
    
    <item>
      <title>Scorings: interpolando (y extrapolando) entre el de Brier y el lineal</title>
      <link>/2019/01/21/scorings-interpolando-y-extrapolando-entre-el-de-brier-y-el-lineal/</link>
      <pubDate>Mon, 21 Jan 2019 08:13:13 +0000</pubDate>
      
      <guid>/2019/01/21/scorings-interpolando-y-extrapolando-entre-el-de-brier-y-el-lineal/</guid>
      <description>Rápidamente y para poner el limpio unas cosas que tenía en borrador. El scoring lineal del que me he ocupado en entradas anteriores (p.e., esta o esta) está asociado a un exponente $latex \lambda = 1$ y el de Brier, a $latex \lambda = 2$. Entre ambos (y a la derecha del 2) hay otros scorings posibles.
Una penalización de $latex (1-p)^\lambda$ (véanse las entradas enlazadas más arriba para averiguar a qué me refiero), un predictor tiene un incentivo para modificar su predicción para alcanzar un scoring más alto, salvo en el caso en que $latex \lambda = 2$, en el que le compensa ser lo más sincero posible.</description>
    </item>
    
    <item>
      <title>Mejores predictores: un ejemplo (el de Brier)</title>
      <link>/2019/01/17/mejores-predictores-un-ejemplo-el-de-brier/</link>
      <pubDate>Thu, 17 Jan 2019 08:13:15 +0000</pubDate>
      
      <guid>/2019/01/17/mejores-predictores-un-ejemplo-el-de-brier/</guid>
      <description>La entrada de hoy casi me la escribe un comentarista (al que le estoy muy agradecido) ayer. Retomo el tema.
Ayer premiaba a cada predictor con $latex p(X)$, es decir, le daba $latex p$ punticos si ocurría $latex X$ y $latex 1-p$ punticos sin no ocurría. La cosa no cambia si nos alineamos con lo que está escrito por ahí y en lugar de premiar, penalizamos. Es decir, si en lugar de maximizar $latex p(X)$, buscamos minimizar $latex 1 - p(X)$.</description>
    </item>
    
    <item>
      <title>Una de las mil maneras malas de elegir al mejor predictor</title>
      <link>/2019/01/16/una-de-las-mil-maneras-malas-de-elegir-al-mejor-predictor/</link>
      <pubDate>Wed, 16 Jan 2019 08:13:36 +0000</pubDate>
      
      <guid>/2019/01/16/una-de-las-mil-maneras-malas-de-elegir-al-mejor-predictor/</guid>
      <description>El contexto, ayer.
La cosa es que se nos podría ocurrir premiar a los predictores cuando asignan probabilidad alta a los sucesos que ocurrieron y baja a los que no. Por ejemplo, si el evento $latex i$ ocurre, premiar al predictor con $latex p_i$ y si no ocurre, con $latex 1 - p_i$. Escrito de otra manera, con $latex p_i(X_i)$ (que quiere decir la probabilidad correspondiente al evento observado).
Como hay varios eventos, cada predictor se llevaría un premio igual a $latex s = \sum_i p_i(X_i)$ y sería mejor aquél predictor con el mayor valor de $latex s$.</description>
    </item>
    
    <item>
      <title>¿Quién será el mejor predictor? ¿Cómo se podrá medir?</title>
      <link>/2019/01/15/quien-sera-el-mejor-predictor-como-se-podra-medir/</link>
      <pubDate>Tue, 15 Jan 2019 08:13:22 +0000</pubDate>
      
      <guid>/2019/01/15/quien-sera-el-mejor-predictor-como-se-podra-medir/</guid>
      <description>He tropezado con un problema nuevo y sobre el que escribiré más estos días. Hoy y aquí solo lo formulo.
Existe una serie de eventos dicotómicos $latex X_i$ que pueden ocurrir o no ocurrir, cada uno de ellos con su probabilidad real (pero desconocida) de ocurrencia $latex q_i$. Antes de que ocurran o no, a dos expertos se les preguntan las probabilidades de ocurrencia de dichos eventos y producen predicciones $latex p_{1i}$ y $latex p_{2i}$.</description>
    </item>
    
  </channel>
</rss>

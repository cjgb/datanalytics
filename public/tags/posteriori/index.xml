<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>posteriori on datanalytics</title>
    <link>/tags/posteriori/</link>
    <description>Recent content in posteriori on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Mon, 25 Nov 2019 09:13:00 +0000</lastBuildDate><atom:link href="/tags/posteriori/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Los factores de Bayes son las hamburguesas veganas</title>
      <link>/2019/11/25/los-factores-de-bayes-son-las-hamburguesas-veganas/</link>
      <pubDate>Mon, 25 Nov 2019 09:13:00 +0000</pubDate>
      
      <guid>/2019/11/25/los-factores-de-bayes-son-las-hamburguesas-veganas/</guid>
      <description>Si eres vegano, vale, come tu lechuga y tu berenjena. Pero, ¿qué necesidad tienes de hamburguesas veganas? ¿Y a qué viene ufanarte de que saben casi igual?
[Nota: el párrafo anterior está escrito en condicional y aplica a ciertos veganos, entrellos alguno que conozco.]
Siempre he visto todo lo que rodea a los factores de bayes un tufillo a hamburguesa vegana. Es decir, un intento por reproducir lo más fidedignamente posible aquello que —¿por razones metodológicas?</description>
    </item>
    
    <item>
      <title>Un recíproco para el teorema de Bernstein–von Mises</title>
      <link>/2019/05/10/un-reciproco-para-el-teorema-de-bernstein-von-mises/</link>
      <pubDate>Fri, 10 May 2019 09:13:59 +0000</pubDate>
      
      <guid>/2019/05/10/un-reciproco-para-el-teorema-de-bernstein-von-mises/</guid>
      <description>Aquí se describe una suerte de recíproco para el teorema de Bernstein–von Mises. Aquí se resume de esta manera:
En resumen:
 B-vM: frente a la misma evidencia, observadores con prioris distintas tienen posteriores similares. * Aumann: frente a evidencias disímiles, observadores con las mismas prioris pueden acordar posterioris similares.  </description>
    </item>
    
    <item>
      <title>Las altas dimensiones son campo minado para la intuición</title>
      <link>/2019/04/15/las-altas-dimensiones-son-campo-minado-para-la-intuicion/</link>
      <pubDate>Mon, 15 Apr 2019 09:13:16 +0000</pubDate>
      
      <guid>/2019/04/15/las-altas-dimensiones-son-campo-minado-para-la-intuicion/</guid>
      <description>Las dimensiones altas son un campo minado para la intuición. Hace poco (y he perdido la referencia) leí a un matemático que trabajaba en problemas en dimensiones altas decir que le gustaba representar y pensar en las bolas (regiones del espacio a distancia &amp;lt;1 de 0) en esos espacios usando figuras cóncavas, como las que aparecen a la izquierda de
precisamente porque una de las propiedades más fructíferas de las bolas en altas dimensiones es que apenas tienen interior.</description>
    </item>
    
    <item>
      <title>El método de Laplace para aproximar ciertas funciones</title>
      <link>/2019/03/05/el-metodo-de-laplace-para-aproximar-ciertas-funciones/</link>
      <pubDate>Tue, 05 Mar 2019 08:13:09 +0000</pubDate>
      
      <guid>/2019/03/05/el-metodo-de-laplace-para-aproximar-ciertas-funciones/</guid>
      <description>El método de Laplace para aproximar funciones puede usarse para:
 Resolver integrales, como aquí * O para aproximar distribuciones (como esta posteriori o estas otras)  Nota: Y más vale que funcione bien y a escala o voy a tener problemas en un inminente proyecto.</description>
    </item>
    
    <item>
      <title>ABC (2)</title>
      <link>/2018/10/24/abc-2-2/</link>
      <pubDate>Wed, 24 Oct 2018 08:13:19 +0000</pubDate>
      
      <guid>/2018/10/24/abc-2-2/</guid>
      <description>Más sobre lo de ayer. O más bien, una justificación por analogía.
Con monedas.
Tiras una moneda 100 veces y obtienes 60 caras. Tienes una priori $latex B(a,b)$ (beta). Tomas una muestra de valores $latex p_i$ con esa distribución y para cada una de ellas repites el experimento, es decir, obtienes lo que en R se expresaría de la forma
rbinom(1, 100, p[i])  Si te quedas los valores $p_i$ tales que esa simulación es 60, enhorabuena, tienes una muestra de la distribución a posteriori.</description>
    </item>
    
    <item>
      <title>Posterioris informativas (o más bien, cuando te informan de cuál es la posteriori)</title>
      <link>/2018/06/07/posterioris-informativas-o-mas-bien-cuando-te-informan-de-cual-es-la-posteriori/</link>
      <pubDate>Thu, 07 Jun 2018 08:13:55 +0000</pubDate>
      
      <guid>/2018/06/07/posterioris-informativas-o-mas-bien-cuando-te-informan-de-cual-es-la-posteriori/</guid>
      <description>El otro día, en la ronda de preguntas tras mi charla en la Universidad de Zaragoza, después de mi enconada defensa de las prioris informativas, alguien apostilló muy agudamente: si tenemos prioris muy informativas, ¿para qué queremos datos?
Eso, ¿para qué queremos datos?
El otro día me lo explicó otro amigo en las siguientes líneas que reproduzco con las inexactitudes achacables a memoria anaidética:
En una empresa, un consejero tiene un proyecto, una idea.</description>
    </item>
    
    <item>
      <title>Militancia y datos</title>
      <link>/2017/09/18/militancia-y-datos/</link>
      <pubDate>Mon, 18 Sep 2017 08:13:14 +0000</pubDate>
      
      <guid>/2017/09/18/militancia-y-datos/</guid>
      <description>Allá por el 2007 publicó The Independent una portada en que se retractaba. El diario había sido un histórico defensor de la legalización de la marihuana. Ese día hizo público su cambio de postura. Al parecer, motivada por las evidencias sobre los efectos sobre la salud mental.
Este fin de semana he asistido a una serie de conferencias. En una de ellas participaba el representante de una organización que:
 * Adoptaba de partida una posición militante, de parte, en cierto asunto de interés público.</description>
    </item>
    
  </channel>
</rss>

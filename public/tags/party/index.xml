<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>party on datanalytics</title>
    <link>/tags/party/</link>
    <description>Recent content in party on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Tue, 21 May 2019 09:13:54 +0000</lastBuildDate><atom:link href="/tags/party/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>¿Qué puede colgar de un árbol?</title>
      <link>/2019/05/21/que-puede-colgar-de-un-arbol/</link>
      <pubDate>Tue, 21 May 2019 09:13:54 +0000</pubDate>
      
      <guid>/2019/05/21/que-puede-colgar-de-un-arbol/</guid>
      <description>Predicciones puntuales:
O (sub)modelos:
Y parece que ahora también distribuciones:
Notas:
 Obviamente, la clasificación anterior no es mutuamente excluyente. * La tercera gráfica está extraída de Transformation Forests, un artículo donde se describe el paquete trtf de R. * Los autores dicen que [r]egression models for supervised learning problems with a continuous target are commonly understood as models for the conditional mean of the target given predictors. ¿Vosotros lo hacéis así?</description>
    </item>
    
    <item>
      <title>vecpart: modelización de moderadores con árboles</title>
      <link>/2019/02/18/9857/</link>
      <pubDate>Mon, 18 Feb 2019 08:13:43 +0000</pubDate>
      
      <guid>/2019/02/18/9857/</guid>
      <description>En un GLM (aún más generalizado que la G de las siglas) puede haber coeficientes moderados. Usando una terminología muy ad hoc, en el modelo pueden entrar predictores y moderadores. Lo cual quiere decir que la parte lineal puede ser de la forma
$latex \sum_i X_i \beta_i(Z_i),$
donde las $latex X_i$ son los predictores propiamente dichos y las variables $latex Z_i$ son moderadoras, es decir, que modifican el efecto de los predictores a través de una función arbitraria $latex \beta_i$.</description>
    </item>
    
    <item>
      <title>Repensando la codificación por impacto</title>
      <link>/2017/01/10/repensando-la-codificacion-por-impacto/</link>
      <pubDate>Tue, 10 Jan 2017 08:13:49 +0000</pubDate>
      
      <guid>/2017/01/10/repensando-la-codificacion-por-impacto/</guid>
      <description>Hay una entrada mía, esta, que me ronda la cabeza y con la que no sé si estoy completamente de acuerdo. Trata de justificar la codificación por impacto de variables categóricas en modelos lineales (generalizados o no) y cuanto más la releo, menos me la creo. O, más bien, comienzo a cuestinarme más seriamente contextos en los que funciona y contextos en los que no.
Pero comencemos por uno simple: los árboles.</description>
    </item>
    
    <item>
      <title>Bajo el capó del particionamiento recursivo basado en modelos</title>
      <link>/2014/09/12/bajo-el-capo-del-particionamiento-recursivo-basado-en-modelos/</link>
      <pubDate>Fri, 12 Sep 2014 07:13:31 +0000</pubDate>
      
      <guid>/2014/09/12/bajo-el-capo-del-particionamiento-recursivo-basado-en-modelos/</guid>
      <description>Una de las mayores contrariedades de estar sentado cerca de alguien que es más matemático que un servidor (de Vds., no de silicio) es que oye siempre preguntar por qué. Una letanía de preguntas me condujo a leer papelotes que ahora resumo.
Primero, unos datos:
&amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/base/set.seed&amp;quot;&amp;gt;set.seed(1234) n &amp;lt;- 100 x1 &amp;lt;- rnorm(n) x2 &amp;lt;- rnorm(n) x3 &amp;lt;- rnorm(n) y &amp;lt;- 0.3 + 0.2 * x1 + 0.5 * (x2 &amp;gt; 0) + 0.</description>
    </item>
    
    <item>
      <title>Procesos de Poisson no homogéneos: la historia de un fracaso</title>
      <link>/2014/08/08/procesos-de-poisson-no-homogeneos-la-historia-de-un-fracaso/</link>
      <pubDate>Fri, 08 Aug 2014 07:13:03 +0000</pubDate>
      
      <guid>/2014/08/08/procesos-de-poisson-no-homogeneos-la-historia-de-un-fracaso/</guid>
      <description>Partamos el tiempo en, p.e., días y contemos una serie de eventos que suceden en ellos. Es posible que esos recuentos se distribuyan según un proceso de Poisson de parámetro $latex \lambda$, que es un valor que regula la intensidad.
Si los días son homogéneos, i.e., no hay variaciones de intensidad diaria, estimar $latex \lambda$ (por máxima verosimilitud), es tan fácil como calcular la media de los sucesos por día. Pero puede suceder que la intensidad varíe en el tiempo (p.</description>
    </item>
    
    <item>
      <title>Incrementalidad via particionamiento recursivo basado en modelos</title>
      <link>/2014/07/30/incrementalidad-via-particionamiento-recursivo-basado-en-modelos/</link>
      <pubDate>Wed, 30 Jul 2014 07:13:24 +0000</pubDate>
      
      <guid>/2014/07/30/incrementalidad-via-particionamiento-recursivo-basado-en-modelos/</guid>
      <description>Planteas un modelo tal como resp ~ treat y no encuentras diferencia significativa. O incluso puede ser negativa. Globalmente.
La pregunta es, con el permiso del Sr. Simpson (o tal vez inspirados por él), ¿existirá alguna región del espacio en la que el tratamiento tiene un efecto beneficioso? Puede que sí. Y de haberla, ¿cómo identificarla?
De eso hablo hoy aquí. E incluyo una protorespuesta.
Primero, genero datos:
n &amp;lt;- 20000 v1 &amp;lt;- sample(0:1, n, replace = T) v2 &amp;lt;- sample(0:1, n, replace = T) v3 &amp;lt;- sample(0:1, n, replace = T) treat &amp;lt;- sample(0:1, n, replace = T) y &amp;lt;- v1 + treat * v1 * v2 y &amp;lt;- exp(y) / (1 + exp(y)) y &amp;lt;- sapply(y, function(x) rbinom(1,1,x)) dat &amp;lt;- data.</description>
    </item>
    
    <item>
      <title>Importancia de variables en árboles</title>
      <link>/2013/11/06/importancia-de-variables-en-arboles/</link>
      <pubDate>Wed, 06 Nov 2013 07:49:59 +0000</pubDate>
      
      <guid>/2013/11/06/importancia-de-variables-en-arboles/</guid>
      <description>Los árboles (o árboles de inferencia condicional) valen fundamentalmente para hacerse una idea de cómo y en qué grado opera una variable en un modelo controlando por el efecto del resto. Su valor reside fundamentalmente en la interpretabilidad.
No obstante lo cual, no es infrecuente construir árboles muy grandes. Y el tamaño dificulta censar qué variables y en qué manera aparecen. Por eso me vi obligado recientemente a crear un pequeño prototipo para extraer el peso de las variables de un árbol.</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>variables categóricas on datanalytics</title>
    <link>/tags/variables-categ%C3%B3ricas/</link>
    <description>Recent content in variables categóricas on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Wed, 11 Nov 2020 09:13:00 +0000</lastBuildDate><atom:link href="/tags/variables-categ%C3%B3ricas/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Codificación de categóricas: de (1 | A) a (B | A)</title>
      <link>/2020/11/11/codificacion-de-categoricas-de-1-a-a-b-a/</link>
      <pubDate>Wed, 11 Nov 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/11/11/codificacion-de-categoricas-de-1-a-a-b-a/</guid>
      <description>La notación y la justificación de (1 | A) está aquí, una vieja entrada que no estoy seguro de que no tenga que retocar para que no me gruña el ministerio de la verdad.
Esta entrada lo es solo para anunciar que en uno de nuestros proyectos y a resultas de una idea de Luz Frías, vamos a implementar una versión mucho más parecida al lo que podría representar el término (B | A), que es, casi seguro, chorrocientasmil veces mejor.</description>
    </item>
    
    <item>
      <title>Spike and slab: otro método para seleccionar variables</title>
      <link>/2020/04/07/spike-and-slab-otro-metodo-para-seleccionar-variables/</link>
      <pubDate>Tue, 07 Apr 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/04/07/spike-and-slab-otro-metodo-para-seleccionar-variables/</guid>
      <description>Me sorprende ver todavía a gente utilizar técnicas stepwise para la selección de variables en modelos. Sobre todo, existiendo herramientas como elastic net o lasso.
Otra de las técnicas disponibles es la del spike and slab (de la que oí hablar, recuerdo, por primera vez en el artículo de Varian Big Data: New Tricks for Econometrics). Es una técnica de inspiración bayesiana en cuya versión más cruda se imponen sobre las variables del modelo de regresión prioris que son una mezcla de dos distribuciones:</description>
    </item>
    
    <item>
      <title>Preprocesamiento de variables categóricas con muchos niveles</title>
      <link>/2019/09/25/preprocesamiento-de-variables-categoricas-con-muchos-niveles/</link>
      <pubDate>Wed, 25 Sep 2019 09:13:35 +0000</pubDate>
      
      <guid>/2019/09/25/preprocesamiento-de-variables-categoricas-con-muchos-niveles/</guid>
      <description>No sabía por qué tenía apartado A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems en mi disco duro para ulteriores revisiones hasta que, al abrirlo, he encontrado la fórmula
que es una versión de mi favorita del mundo mundial (si te dedicas a la ciencia de datos, no la conoces y tienes principios, negocia a la baja tu sueldo: estás timando a alguien).
Todo sumamente aprovechable y recomendable.</description>
    </item>
    
    <item>
      <title>Sobre la peculiarísima implementación del modelo lineal en (pseudo-)scikit-learn</title>
      <link>/2019/07/17/sobre-la-peculiarisima-implementacion-del-modelo-lineal-en-pseudo-scikit-learn/</link>
      <pubDate>Wed, 17 Jul 2019 09:13:55 +0000</pubDate>
      
      <guid>/2019/07/17/sobre-la-peculiarisima-implementacion-del-modelo-lineal-en-pseudo-scikit-learn/</guid>
      <description>Si ejecutas
import numpy as np from sklearn.linear_model import LinearRegression n = 1000 X = np.random.rand(n, 2) Y = np.dot(X, np.array([1, 2])) + 1 + np.random.randn(n) / 2 reg = LinearRegression().fit(X, Y) reg.intercept_ reg.coef_  se obtiene más o menos lo esperado. Pero si añades una columna linealmente dependiente,
X = np.column_stack((X, 1 * X[:,1]))  ocurren cosas de la más calamitosa especie:
Y = np.dot(X, np.array([1, 2, 1])) + 1 + np.</description>
    </item>
    
    <item>
      <title>Recodificación de variables categóricas de muchos niveles: ¡ayuda!</title>
      <link>/2018/01/08/recodificacion-de-variables-categoricas-de-muchos-niveles-ayuda/</link>
      <pubDate>Mon, 08 Jan 2018 08:13:18 +0000</pubDate>
      
      <guid>/2018/01/08/recodificacion-de-variables-categoricas-de-muchos-niveles-ayuda/</guid>
      <description>Una vez escribí al respecto. Y cuanto más lo repienso y lo reeleo, menos clara tengo mi interpretación. De hecho, estoy planteándome retractar esa entrada.
Y reconozco que llevo tiempo buscando en ratos libres algún artículo serio (no extraído del recetario de algún script kiddie de Kaggle) que justifique el uso del procedimiento. Es decir, que lo eleve de técnica a categoría. Sin éxito.
He hecho probaturas y experimentos mentales en casos extremos (p.</description>
    </item>
    
    <item>
      <title>Lo que pasa cuando omites la priori con variables categóricas</title>
      <link>/2017/01/12/lo-que-pasa-cuando-omites-la-priori-con-variables-categoricas/</link>
      <pubDate>Thu, 12 Jan 2017 08:13:32 +0000</pubDate>
      
      <guid>/2017/01/12/lo-que-pasa-cuando-omites-la-priori-con-variables-categoricas/</guid>
      <description>Stan. Modelo multinivel. Variable categórica. Codificación con ceros y unos. Matriz. Coeficiente vector[n_ccaa] Cccaa. Sin priori.
Catástrofe:
(Coeficientes hasta 15000. Sin tasa, con tiempo. Los valores desorbitados, en ceros de la dummy).
Priori.
for (i in 1:n_ccaa) Cccaa[i] ~ cauchy(0, 20);
¿Por qué no?
Tachán:
(¿Para qué verbos?)</description>
    </item>
    
  </channel>
</rss>

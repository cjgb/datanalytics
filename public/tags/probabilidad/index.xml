<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>probabilidad on datanalytics</title>
    <link>/tags/probabilidad/</link>
    <description>Recent content in probabilidad on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Thu, 28 Oct 2021 09:13:00 +0000</lastBuildDate><atom:link href="/tags/probabilidad/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Monty Hall, reformulado</title>
      <link>/2021/10/28/monty-hall-reformulado/</link>
      <pubDate>Thu, 28 Oct 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/10/28/monty-hall-reformulado/</guid>
      <description>Considérese el siguiente juego: 1. Hay tres sobres indistinguibles sobre una mesa. 2. Uno de ellos contiene un premio. 3. Puedes elegir o bien uno de ellos o bien dos de ellos al azar. 2. Convénzase uno de que es mejor elegir dos sobres que uno: tienes una probabilidad de ganar el premio de 2/3 contra la de 1/3 si eliges solo uno. 3. Convénzase uno de que el problema de Monty Hall en su formulación habitual es solo una reformulación artificiosa y engañosa del juego anterior.</description>
    </item>
    
    <item>
      <title>Dos cuestiones sobre la naturaleza de la probabilidad planteadas por Keynes en 1921 pero que siguen hoy igual de vigentes</title>
      <link>/2021/10/28/dos-cuestiones-sobre-la-naturaleza-de-la-probabilidad-planteadas-por-keynes-en-1921-pero-que-siguen-hoy-igual-de-vigentes/</link>
      <pubDate>Thu, 28 Oct 2021 00:36:19 +0000</pubDate>
      
      <guid>/2021/10/28/dos-cuestiones-sobre-la-naturaleza-de-la-probabilidad-planteadas-por-keynes-en-1921-pero-que-siguen-hoy-igual-de-vigentes/</guid>
      <description>I.
A Treatise on Probability, la obra de Keynes (sí, el famoso) de 1921, es un libro muy extraño que se puede leer de muchas maneras. Puede servir, si se hace poco caritativamente, para denunciar el lastimoso estado en el que se encontraba la probabilidad antes de la axiomatización de Kolmogorov, 12 años depués de su publicación. O también, si se hace más cuidadosamente, para rescatar una serie de consideraciones que aun hoy muchos hacen mal en ignorar.</description>
    </item>
    
    <item>
      <title>Aún más sobre propagación de errores (y rv)</title>
      <link>/2021/09/21/aun-mas-sobre-propagacion-de-errores-y-rv/</link>
      <pubDate>Tue, 21 Sep 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/09/21/aun-mas-sobre-propagacion-de-errores-y-rv/</guid>
      <description>[Menos mal que se me ha ocurrido buscar en mi propio blog sobre el asunto y descubrir —no lo recordaba— que ya había tratado el asunto previamente en entradas como esta, esta o esta.]
El problema de la propagación de errores lo cuentan muy bien Iñaki Úcar y sus coautores aquí. Por resumirlo: tienes una cantidad, $latex X$ conocida solo aproximadamente —en concreto, con cierto error— e interesa conocer y acotar el error de una expresión $latex f(X)$.</description>
    </item>
    
    <item>
      <title>Cournot sobre el &#34;efecto Roseto&#34;, 120 años antes de tal</title>
      <link>/2021/06/17/cournot-sobre-el-efecto-roseto-120-anos-antes-de-tal/</link>
      <pubDate>Thu, 17 Jun 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/06/17/cournot-sobre-el-efecto-roseto-120-anos-antes-de-tal/</guid>
      <description>Esta entrada abunda sobre una de la semana pasada sobre el llamado efecto Roseto. El Cournot al que alude el titulo es el Cournot famoso (1801-1877) al que, a pesar de ser más conocido por sus aportaciones a la economía, debemos una Exposition de la théorie des chances et des probabilités de 1843.
En su párrafo 114 critica explícitamente el tipo de conclusiones a las que llegan los descuidados exégetas del asunto Roseto y que Stigler comenta así:</description>
    </item>
    
    <item>
      <title>Nuevo vídeo en YouTube: &#34;¿Se pueden estimar probabilidades pequeñas con pocas observaciones?&#34;</title>
      <link>/2021/05/25/nuevo-video-en-youtube-se-pueden-estimar-probabilidades-pequenas-con-pocas-observaciones/</link>
      <pubDate>Tue, 25 May 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/05/25/nuevo-video-en-youtube-se-pueden-estimar-probabilidades-pequenas-con-pocas-observaciones/</guid>
      <description>Acabo de subir un nuevo vídeo a Youtube,
https://youtu.be/Uv1mEmsiUKM
en el que discuto dos problemas: uno, general, que es el que indica su título; y otro más concreto que es su motivación última: si es posible asegurar que la combinación de vacunas es segura a través de un estudio realizado con 600 sujetos, tal como el realizado por el ISCIII recientemente.
En él se hace referencia a una vieja entrada en el blog del autor, esta.</description>
    </item>
    
    <item>
      <title>Sobre las probabilidades de eventos que ocurren una única vez</title>
      <link>/2021/04/08/sobre-las-probabilidades-de-eventos-que-ocurren-una-unica-vez/</link>
      <pubDate>Thu, 08 Apr 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/04/08/sobre-las-probabilidades-de-eventos-que-ocurren-una-unica-vez/</guid>
      <description>La probabilidad se predica de eventos de muy distintas características. Existe un arco entero de casos en cuyos extremos opuestos podemos encontrar los eventos:
 Obtener cara al lanzar esta moneda. * Que X gane las elecciones que ocurrirán en un mes.  La principal diferencia, por si alguien lo lo ha advertido, es que el primer tipo de evento puede repetirse cuantas veces se desee mientras que esas elecciones ocurrirán una única vez.</description>
    </item>
    
    <item>
      <title>¿La teoría de la probabilidad no extiende la lógica?</title>
      <link>/2021/03/18/la-teoria-de-la-probabilidad-no-extiende-la-logica/</link>
      <pubDate>Thu, 18 Mar 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/03/18/la-teoria-de-la-probabilidad-no-extiende-la-logica/</guid>
      <description>Después de haber estado un tiempo —hasta tener que interrumpirlo para convertirme en un elemento socialmente productivo— leyendo sobre cómo la teoría de la probabilidad extiende la lógica (Jaynes, Hacking y compañía), he incurrido en Probability theory does not extend logic. Se trata de un ensayito recomendable pero sobre el que advierto a sus posibles lectores que decae rápidamente de mucho al fango.
De él extraigo una interpretación muy heterodoxa de la probabilidad condicional expresada en términos de la lógica de predicados.</description>
    </item>
    
    <item>
      <title>Nuevo vídeo en YouTube: ¿son las probabilidades &#34;subjetivas&#34;? ¿Existe el azar?</title>
      <link>/2021/03/06/nuevo-video-en-youtube-son-las-probabilidades-subjetivas-existe-el-azar/</link>
      <pubDate>Sat, 06 Mar 2021 12:23:40 +0000</pubDate>
      
      <guid>/2021/03/06/nuevo-video-en-youtube-son-las-probabilidades-subjetivas-existe-el-azar/</guid>
      <description>El vídeo es
https://www.youtube.com/watch?v=vhXj1RpQV0c
y su objetivo es refutar cierta visión muy extraña de la probabilidad que se oye sostener a cierto tipo de personas de vez en cuando, la de que es un fenómeno subjetivo, acompañado frecuentemente por la todavía más extravagante afirmación de que el azar no existe (salvo, tal vez, en el nivel subatómico).
Y una vez refutada, el en el vídeo vuelvo a probar una versión alternativa de la afirmación anterior, tal vez más ajustada a la realidad tal cual la veo.</description>
    </item>
    
    <item>
      <title>Un argumento para usar la normal: la maximización de la entropía</title>
      <link>/2021/03/02/un-argumento-para-usar-la-normal-la-maximizacion-de-la-entropia/</link>
      <pubDate>Tue, 02 Mar 2021 09:13:46 +0000</pubDate>
      
      <guid>/2021/03/02/un-argumento-para-usar-la-normal-la-maximizacion-de-la-entropia/</guid>
      <description>Llegaré a la normal. Antes, algo sobre la entropía.
Nos interesa saber y medir el grado de concentración de una distribución. Por ejemplo, si X es una variable aleatoria con función de densidad $latex f(x)$ y $latex x_1, \dots, x_n$ es una muestra de X, entonces, la expresión
$latex \frac{1}{n} \sum_i f(x_i)$
da una idea de la concentración vs dispersión de X:
 Si es grande, muchos de los $latex x_i$ procederán de lugares donde $latex f$ es grande; en un caso discreto, que tal vez ayude a mejorar la intuición sobre la cosa, habría muchos valores repetidos.</description>
    </item>
    
    <item>
      <title>Sobre sumas de cuadrados de normales con varianzas desiguales</title>
      <link>/2021/02/25/sobre-sumas-de-cuadrados-de-normales-con-varianzas-desiguales/</link>
      <pubDate>Thu, 25 Feb 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/02/25/sobre-sumas-de-cuadrados-de-normales-con-varianzas-desiguales/</guid>
      <description>En mi entrada anterior mencioné cómo la suma de cuadrados de normales, aun cuando tengan varianzas desiguales, sigue siendo aproximadamente $latex \chi^2$. Es el resultado que subyace, por ejemplo, a la aproximación de Welch que usa R por defecto en t.test. Puede verse una discusión teórica sobre el asunto así como enlaces a la literatura relevante aquí.
Esta entrada es un complemento a la anterior que tiene lo que a la otra le faltan: gráficos.</description>
    </item>
    
    <item>
      <title>Tres &#34;teoremas&#34; que son casi ciertos</title>
      <link>/2021/02/23/tres-teoremas-que-son-casi-ciertos/</link>
      <pubDate>Tue, 23 Feb 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/02/23/tres-teoremas-que-son-casi-ciertos/</guid>
      <description>I.
Si $latex X_1, \dots, X{12}$ son uniformes en [0,1] e independientes, entonces $latex X_1 + \dots + X_{12} - 6$ es una variable aleatoria normal._
Puede entenderse como un corolario práctico del teorema central del límite habida cuenta de que la varianza de $latex X_i$ es 1/12 y su media es 1/2.
Es útil porque, se ve, en algunos dispositivos embebidos no se dispone de una librería matemática extensa y, se ve, a veces hace falta muestrear la normal.</description>
    </item>
    
    <item>
      <title>El teorema de Bayes como la versión modal del modus tollens</title>
      <link>/2021/02/08/el-teorema-de-bayes-como-la-version-modal-del-modus-tollens/</link>
      <pubDate>Mon, 08 Feb 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/02/08/el-teorema-de-bayes-como-la-version-modal-del-modus-tollens/</guid>
      <description>El otro día alguien argumentaba (de una manera que no voy a adjetivar):
 La lógica (proposiciona, de primer orden) es importante (si lo que se pretende es actuar racionalment), la probabilidad no tanto. * El teorema de Bayes es solo un resultado trivial dentro de una disciplina mucho menos relevante que la lógica. * Ergo, ¿por qué tanto coñacito con el dichoso teorema de Bayes?  Como había alguien equivocado en internet, sonaron todas las alarmas que tengo colocadas en casa y tuve que acudir a enderezar el tuerto.</description>
    </item>
    
    <item>
      <title>Estos keynesianos ven el mundo de una manera muy, muy loca</title>
      <link>/2021/01/19/estos-keynesianos-ven-el-mundo-de-una-manera-muy-muy-loca/</link>
      <pubDate>Tue, 19 Jan 2021 09:13:00 +0000</pubDate>
      
      <guid>/2021/01/19/estos-keynesianos-ven-el-mundo-de-una-manera-muy-muy-loca/</guid>
      <description>[Y no, no me refiero (hoy) a los seguidores del Keynes de la &amp;ldquo;Teoría general del empleo, el interés y el dinero&amp;rdquo; sino a los de su &amp;ldquo;Tratado sobre probabilidades&amp;rdquo;. Misma persona, distinto libro, distinta disciplina. Y excúseme el &amp;ldquo;clickbait&amp;rdquo;: no podía no hacerlo.]
Keynes escribió en 1921 su Tratado de probabilidades, según la Wikipedia, una contribución a las bases matemáticas y filosóficas de la teoría de la probabilidad. Le falta añadir descabellada (aunque, como se verá después, tiene su punto), superada y felizmente olvidada.</description>
    </item>
    
    <item>
      <title>&#34;Introducción a la probabilidad y la estadística para científicos de datos&#34;: segunda entrega</title>
      <link>/2020/11/30/introduccion-a-la-probabilidad-y-la-estadistica-para-cientificos-de-datos-segunda-entrega/</link>
      <pubDate>Mon, 30 Nov 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/11/30/introduccion-a-la-probabilidad-y-la-estadistica-para-cientificos-de-datos-segunda-entrega/</guid>
      <description>Acabo de subir:
 Modificaciones y correcciones a los dos primeros capítulos. * Un tercer capítulo sobre distribuciones de probabilidad.  Queda ampliar, organizar y razonar la biblografía correspondiente a ese tercer capítulo.
Lo más original (con cuádruples comillas) de este capítulo es tal vez la construcción de la función de densidad a partir de histogramas obtenidos a partir de simulaciones de variables aleatorias. Algo sobre lo que creo que escribí en su día en el blog pero que no ubico.</description>
    </item>
    
    <item>
      <title>¿Cómo asignar probabilidades? Simetría y universalidad</title>
      <link>/2020/10/20/como-asignar-probabilidades-simetria-y-universalidad/</link>
      <pubDate>Tue, 20 Oct 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/10/20/como-asignar-probabilidades-simetria-y-universalidad/</guid>
      <description>En los minutos 18 y unos pocos de los siguientes de
https://www.youtube.com/watch?v=ysl6NM95TGs&amp;amp;feature=youtu.be
se plantea el problema de cómo asignar probabilidades a eventos y el conferenciante, Martin Hairer, discute (¿con ánimo de exhaustividad?) dos: simetría y universalidad.
_[Nota: la discusión es paralela y muy similar a una que aparece en una sección aún no publicada de mi libro de probabilidad y estadística. La relación causal entre ambos hechos es bastante problemática.] _</description>
    </item>
    
    <item>
      <title>&#34;Introducción a la probabilidad y la estadística para científicos de datos&#34;: primera entrega</title>
      <link>/2020/10/15/introduccion-a-la-probabilidad-y-la-estadistica-para-cientificos-de-datos-primera-entrega/</link>
      <pubDate>Thu, 15 Oct 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/10/15/introduccion-a-la-probabilidad-y-la-estadistica-para-cientificos-de-datos-primera-entrega/</guid>
      <description>Acabo de colgar el primer par de capítulos de mi libro Introducción a la probabilidad y la estadística para científicos de datos. No voy a adelantar nada aquí que no esté contenido en la introducción a la obra (AKA la introducción de la introducción). Pero baste este adelanto:
Iré añadiendo capítulos y correcciones a lo largo de los próximos meses. Y ruego, por supuesto, la ayuda y comentarios de mis lectores, que agradeceré encarecidamente.</description>
    </item>
    
    <item>
      <title>Una diferencia teórica importante entre los lm y el resto de los glm</title>
      <link>/2020/09/22/una-diferencia-teorica-importante-entre-los-lm-y-el-resto-de-los-glm/</link>
      <pubDate>Tue, 22 Sep 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/09/22/una-diferencia-teorica-importante-entre-los-lm-y-el-resto-de-los-glm/</guid>
      <description>[Este es un extracto, una píldora atómica, de mi charla del otro día sobre el modelo de Poisson y al sobredispersión.]
Aunque me guste expresar el modelo lineal de la forma
$latex y_i \sim N(a_0 + \sum_j a_j x_{ij}, \sigma_i)$
hoy, para lo que sigue, es más conveniente la representación tradicional
$latex y_i = a_0 + \sum_j a_j x_{ij} + \epsilon_i$
donde si no sabes lo que es cada cosa, más vale que no sigas leyendo.</description>
    </item>
    
    <item>
      <title>La pregunta a la que el TCL es una muy particular (y mucho menos importante de lo que habitualmente se cree) respuesta</title>
      <link>/2020/07/10/la-pregunta-a-la-que-el-tcl-es-una-muy-particular-y-mucho-menos-importante-de-lo-que-habitualmente-se-cree-respuesta/</link>
      <pubDate>Fri, 10 Jul 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/07/10/la-pregunta-a-la-que-el-tcl-es-una-muy-particular-y-mucho-menos-importante-de-lo-que-habitualmente-se-cree-respuesta/</guid>
      <description>El TCL (teorema central del límite) ayuda a responder una pregunta en algunos casos concretos. Pero a veces se nos olvida que lo importante es la pregunta y sus muchas otras potenciales respuestas.
La pregunta es: ¿qué distribución, si alguna, es razonable suponer que puedan tener mis datos? El TCL permite responder ¡normal! en algunos casos singulares que fueron más importantes hace tiempo que hoy en día.
Pero llama la atención la importancia (medida, si se quiere, en número de páginas dedicadas a ello en los textos introductorios a la teoría de la probabilidad y la estadística) que se le otorga a esa particularísima respuesta y a su justificación y el poco al de tratar de proporcionar herramientas para tratar de dar una respuesta más o menos coherente a la pregunta general.</description>
    </item>
    
    <item>
      <title>Cuidado con la aleatoriedad &#34;pochola&#34;</title>
      <link>/2020/06/15/cuidado-con-la-aleatoriedad-pochola/</link>
      <pubDate>Mon, 15 Jun 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/06/15/cuidado-con-la-aleatoriedad-pochola/</guid>
      <description>Abundo sobre mi entrada del otro día. Usando números aleatorios hirsutos,
n &amp;lt;- 200 x &amp;lt;- runif(n) plot(cumsum(x - .5), type = &amp;quot;l&amp;quot;)  produce
mientras que
library(randtoolbox) s &amp;lt;- sobol(n, 1, scrambling = 3) plot(cumsum(s - .5), type = &amp;quot;l&amp;quot;)  genera
que tiene un cariz totalmente distinto.</description>
    </item>
    
    <item>
      <title>¿Cómo pensar en la probabilidad de un evento?</title>
      <link>/2020/05/18/como-pensar-en-la-probabilidad-de-un-evento/</link>
      <pubDate>Mon, 18 May 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/05/18/como-pensar-en-la-probabilidad-de-un-evento/</guid>
      <description>[Esta entrada lo es, además de por su propio mérito, en preparación de la que habrá de ocurrir mañana o pasado.]
Así:
La cita está reproducida (aunque reduciendo la cifra de la apuesta a un dólar) en el libro del mismo autor The Flaw of Averages, cuyos méritos glosé por aquí hace mucho, mucho tiempo.</description>
    </item>
    
    <item>
      <title>Movimientos brownianos y barreras</title>
      <link>/2020/05/05/movimientos-brownianos-y-barreras/</link>
      <pubDate>Tue, 05 May 2020 09:51:07 +0000</pubDate>
      
      <guid>/2020/05/05/movimientos-brownianos-y-barreras/</guid>
      <description>En Hypermind se está planteando esta cuestión:
A día de hoy, el S&amp;amp;P 500 está en 2830. La predicción está y viene estando aproximadamente alrededor de la regla de tres:
$latex \frac{s - 2000}{3000 - 2000} \times 100%$
donde $latex s$ es la cotización del índice.
Y aquí vienen dos preguntas/ejercicios para mis lectores:
 Suponiendo que el S&amp;amp;P 500 se comportase como un movimiento browniano (sin drift), ¿sería precisa la regla anterior?</description>
    </item>
    
    <item>
      <title>Una versión aún más sencilla</title>
      <link>/2020/02/27/una-version-aun-mas-sencilla/</link>
      <pubDate>Thu, 27 Feb 2020 17:31:00 +0000</pubDate>
      
      <guid>/2020/02/27/una-version-aun-mas-sencilla/</guid>
      <description>&amp;hellip; que la de &amp;ldquo;Algoritmos&amp;rdquo; y acatarrantes definiciones de &amp;ldquo;justicia&amp;rdquo;. Que es casi una versión de la anterior reduciendo la varianza de las betas.
Las dos poblaciones de interés tienen una tasa de probabilidad (o de riesgo, en la terminología del artículo original) de .4 y .6 respectivamente. Aproximadamente el 40% de los primeros y el 60% de los segundos tienen y = 1.
El modelo (el algoritmo) es perfecto y asigna a los integrantes del primer grupo un scoring de .</description>
    </item>
    
    <item>
      <title>Curvas de equiprobabilidad de la t bivariada</title>
      <link>/2020/02/20/curvas-de-equiprobabilidad-de-la-t-bivariada/</link>
      <pubDate>Thu, 20 Feb 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/02/20/curvas-de-equiprobabilidad-de-la-t-bivariada/</guid>
      <description>El otro día me entretuve pintando curvas de equiprobabilidad de la distribución de Cauchy (nota: debería haberlas llamado cuasicuasiconvexas en lugar de cuasiconvexas en su día). Pero la t es una_ cuerda tendida entre _la Cauchy y la normal y es instructivo echarles un vistazo a las curvas de equiprobabilidad según crecen los grados de libertad. Sobre todo, porque arrojan más información sobre la manera y el sentido en el que la t converge a la normal.</description>
    </item>
    
    <item>
      <title>La densidad de una Cauchy bivariada es cuasiconvexa</title>
      <link>/2020/02/07/la-densidad-de-una-cauchy-bivariada-es-cuasiconvexa/</link>
      <pubDate>Fri, 07 Feb 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/02/07/la-densidad-de-una-cauchy-bivariada-es-cuasiconvexa/</guid>
      <description>Primero, las curvas de nivel:
x &amp;lt;- seq(-50, 50, length.out = 1000) tmp &amp;lt;- expand.grid(x = x, y = x) tmp$z &amp;lt;- log(dcauchy(tmp$x) * dcauchy(tmp$y)) ggplot(tmp, aes(x = x, y = y, z = z)) + stat_contour()  Lo de la cuasiconvexidad está contado aquí.
Las consecuencias estadísticas y probabilísticas, para otro rato.</description>
    </item>
    
    <item>
      <title>La probabilidad, ¿algo subjetivo?</title>
      <link>/2020/01/07/la-probabilidad-algo-subjetivo/</link>
      <pubDate>Tue, 07 Jan 2020 09:13:00 +0000</pubDate>
      
      <guid>/2020/01/07/la-probabilidad-algo-subjetivo/</guid>
      <description>Esta entrada es una contestación a
https://twitter.com/AnaBayes/status/1213446900743122945
I.
Habrá quien sostenga que la geometría (plana, euclídea, por antonomasia) es subjetiva, que es una construcción de la mente, de cada mente. Igual queda todavía alguno de los que, por el contrario, creían que los triángulos equiláteros residen en una especie de edén donde tienen una existencia ideal y que nuestra mente, de alguna manera, se limita a reflejarlos.
Sin embargo, a mí me vale (que es una forma de decir que tomo partido por) que la geometría tiene una existencia objetiva, ajena a la subjetividad de los individuos.</description>
    </item>
    
    <item>
      <title>r -&gt; d -&gt; p -&gt; q</title>
      <link>/2019/10/30/r-d-p-q/</link>
      <pubDate>Wed, 30 Oct 2019 09:10:05 +0000</pubDate>
      
      <guid>/2019/10/30/r-d-p-q/</guid>
      <description>Primero fue la r (runif, rnorm, rpois,&amp;hellip;).
De la r surgió el histograma.
Y el histograma era casi siempre parecido.
Y aquello a lo que se parecía se llamó d (dunif, dnorm, etc.).
Y era bueno.
(Obviamente, debidamente normalizado con integral 1, algo sobre lo que afortunadamente la tontuna de las identidades culturales aún no ha protestado).
La p, una integral de la d, es una conveniencia que permite contestar rápido determinadas preguntas razonables y habituales.</description>
    </item>
    
    <item>
      <title>Rootclaim</title>
      <link>/2019/09/27/rootclaim/</link>
      <pubDate>Fri, 27 Sep 2019 09:13:15 +0000</pubDate>
      
      <guid>/2019/09/27/rootclaim/</guid>
      <description>Rootclaim es un portal donde la gente plantea preguntas como
plantea hipótesis como
se recogen evidencias y usando este método (leedlo, es sumamente aprovechable: usa la palabra bayesian 23 veces), llega a conclusiones tales como</description>
    </item>
    
    <item>
      <title>Proporciones pequeñas y &#34;teoremas&#34; de &#34;imposibilidad&#34;</title>
      <link>/2019/07/22/proporciones-pequenas-y-teoremas-de-imposibilidad/</link>
      <pubDate>Mon, 22 Jul 2019 09:59:21 +0000</pubDate>
      
      <guid>/2019/07/22/proporciones-pequenas-y-teoremas-de-imposibilidad/</guid>
      <description>Esta entrada responde y complementa Malditas proporciones pequeñas I y II_ _trayendo a colación un artículo que ya mencioné en su día y que cuelgo de nuevo: _On the Near Impossibility of Measuring the Returns to Advertising_. ¡Atención al _teorema de la imposibilidad de la Super Bowl_!
Y el resumen breve: cada vez estamos abocados a medir efectos más y más pequeños. La fruta que cuelga a la altura de la mano ya está en la fragoneta del rumano.</description>
    </item>
    
    <item>
      <title>¿Existiría (la cosa de la que voy a hablar)? Lo veo muy poco probable</title>
      <link>/2019/06/24/existiria-la-cosa-de-la-que-voy-a-hablar-lo-veo-muy-poco-probable/</link>
      <pubDate>Mon, 24 Jun 2019 09:13:18 +0000</pubDate>
      
      <guid>/2019/06/24/existiria-la-cosa-de-la-que-voy-a-hablar-lo-veo-muy-poco-probable/</guid>
      <description>, extraído de Verbal probabilities: Very likely to be somewhat more confusing than numbers, creo que es ya cultura general.
Pero me pregunto (y pregunto a mis lectores) si existirá algo parecido para el español. Que incluya, claro, expresiones del tipo &amp;ldquo;muy improbable&amp;rdquo;, etc. pero que se extienda también a otros métodos (que es la parte más interesante) de manifestar incertidumbre, como el uso del condicional (el PP recuperaría la alcaldía&amp;hellip;) y otros que pueda haber.</description>
    </item>
    
    <item>
      <title>La simplicísima mas no por ello menos útil distribución de Dirac</title>
      <link>/2019/03/12/la-simplicisima-mas-no-por-ello-menos-util-distribucion-de-dirac/</link>
      <pubDate>Tue, 12 Mar 2019 08:13:05 +0000</pubDate>
      
      <guid>/2019/03/12/la-simplicisima-mas-no-por-ello-menos-util-distribucion-de-dirac/</guid>
      <description>Ayer alguien desconocía la distribución de probabilidad de Dirac. No sé ni si se llama así y no aparece en prácticamente ninguno de los manuales al uso.
Es una distribución de probabilidad aleatoria: concentra toda su masa en un punto determinado. Por ejemplo, en el nueve:
Y es útil por:
 Ser límite de cosas. * Porque las distribuciones discretas (de la Bernoulli en adelante) son mezclas de variables aleatorias de Dirac.</description>
    </item>
    
    <item>
      <title>Un resultado probabilístico contraintuitivo (y II)</title>
      <link>/2018/10/11/un-resultado-probabilistico-contraintuitivo-y-ii/</link>
      <pubDate>Thu, 11 Oct 2018 08:13:50 +0000</pubDate>
      
      <guid>/2018/10/11/un-resultado-probabilistico-contraintuitivo-y-ii/</guid>
      <description>Va sobre lo de ayer. Hay una demostración de ese resultado contraintutivo aquí. Hay una referencia aquí. Existen discusiones sobre si este resultado se debe a Feller; si no lo es, bien pudiera haberlo sido; la verdad, es muy como de él.
Pero una cosa es la demostración y otra muy distinta, descontraintuitivizar el resultado. Para ello, escuchemos la siguiente conversación entre dos sujetos:
A: No has visto el cierre de la bolsa hoy, ¿verdad?</description>
    </item>
    
    <item>
      <title>Un resultado probabilístico contraintuitivo (parte I)</title>
      <link>/2018/10/10/un-resultado-probabilistico-contraintuitivo-parte-i/</link>
      <pubDate>Wed, 10 Oct 2018 08:13:03 +0000</pubDate>
      
      <guid>/2018/10/10/un-resultado-probabilistico-contraintuitivo-parte-i/</guid>
      <description>A elige dos números con una distribución de probabilidad cualquiera,
generador &amp;lt;- function() rlnorm(2, 3, 4)  y los guarda ocultos. A B le deja ver uno al azar (sin pérdida de generalidad, el primero). Y B tiene que decidir si el que ve es el más alto de los dos (en cuyo caso, gana un premio, etc.). Veamos a B actuar de manera naive:
estrategia.naive &amp;lt;- function(observed){ sample(1:2, 1) }  Dejemos a A y B jugar repetidamente a este juego:</description>
    </item>
    
    <item>
      <title>Licitaciones por insaculación ponderada</title>
      <link>/2018/10/05/licitaciones-por-insaculacion-ponderada/</link>
      <pubDate>Fri, 05 Oct 2018 08:13:24 +0000</pubDate>
      
      <guid>/2018/10/05/licitaciones-por-insaculacion-ponderada/</guid>
      <description>Hace unos años, cuando aún no me había avivado en estos temas, recibí una llamada que me puso muy contento: en un ayuntamiento de nosedónde reconocían mis muchos méritos estadísticos y computacionales y me invitaban a participar en una licitación a vaya Vd. a saber qué cosa. Pero, vamos, lo que pasaba, como tantísimas veces, es que tenían ya escogido a un proveedor y necesitaban a dos comparsas para salvar el trámite burocrático de contar con tres propuestas.</description>
    </item>
    
    <item>
      <title>El problema de la bella durmiente</title>
      <link>/2018/07/30/el-problema-de-la-bella-durmiente/</link>
      <pubDate>Mon, 30 Jul 2018 08:13:07 +0000</pubDate>
      
      <guid>/2018/07/30/el-problema-de-la-bella-durmiente/</guid>
      <description>Lee esto y luego opina: ¿1/2 o 1/3?</description>
    </item>
    
    <item>
      <title>Los extraños números de los muertos en carretera por accidente</title>
      <link>/2018/05/28/los-extranos-numeros-de-los-muertos-en-carretera-por-accidente/</link>
      <pubDate>Mon, 28 May 2018 08:13:32 +0000</pubDate>
      
      <guid>/2018/05/28/los-extranos-numeros-de-los-muertos-en-carretera-por-accidente/</guid>
      <description>Escribo esta entrada con cierta prevención porque soy consciente de que dan pábulo a determinadas teorías conspiranoicas de las que soy declarado enemigo. Pero es que los números de muertos en carretera por accidente en España en los últimos años,
(extraídos de aquí) dan que pensar: la varianza de las observaciones correspondientes a los años 2013, 2014 y 2015 es muy baja, demasiado baja. Al menos, si se da como bueno un modelo de Poisson para modelar esos conteos.</description>
    </item>
    
    <item>
      <title>Lanzamientos de moneda: no es azar sino física</title>
      <link>/2018/02/05/lanzamientos-de-moneda-no-es-azar-sino-fisica/</link>
      <pubDate>Mon, 05 Feb 2018 08:13:39 +0000</pubDate>
      
      <guid>/2018/02/05/lanzamientos-de-moneda-no-es-azar-sino-fisica/</guid>
      <description>Lo dicen Diaconis y sus coautores en Dynamical Bias in the Coin Toss.
Que es un artículo en el que modelan la física de lanzamientos de moneda e incluso y llegan a construir una máquina con el aspecto
que siempre obtiene caras (o cruces).
El quid de la historia es que existen condiciones iniciales de lanzamiento (velocidad inicial, velocidad angular) isoresultado (donde resultado es cara o cruz). Como en
Es decir, si se tira una moneda primero y se obtiene cruz, tirándola otra vez ligeramente más despacio aunque con una rotación ligeramente más rápida (donde ambas velocidades guardan una determinada relación funcional), se vuelve a obtener cruz necesariamente.</description>
    </item>
    
    <item>
      <title>Las correlaciones positivas, ¿son transitivas?</title>
      <link>/2018/01/16/las-correlaciones-positivas-son-transitivas/</link>
      <pubDate>Tue, 16 Jan 2018 08:13:02 +0000</pubDate>
      
      <guid>/2018/01/16/las-correlaciones-positivas-son-transitivas/</guid>
      <description>No. Por ejemplo,
set.seed(155) n &amp;lt;- 1000 x &amp;lt;- rnorm(n) y &amp;lt;- x + rnorm(n) z &amp;lt;- y - 1.5 * x m &amp;lt;- cbind(x, y, z) print(cor(m), digits = 2) # x y z #x 1.00 0.72 -0.41 #y 0.72 1.00 0.34 #z -0.41 0.34 1.00  La correlación de x con y es positiva; también la de y con z. Pero x y z guardan correlación negativa.
Nota: sacado de aquí.</description>
    </item>
    
    <item>
      <title>Un caso en el que falla la máxima verosimilitud</title>
      <link>/2018/01/11/un-caso-en-el-que-falla-la-maxima-verosimilitud/</link>
      <pubDate>Thu, 11 Jan 2018 08:13:42 +0000</pubDate>
      
      <guid>/2018/01/11/un-caso-en-el-que-falla-la-maxima-verosimilitud/</guid>
      <description>El caso es el siguiente: alguien hace la colada y al ir a tender, observa que los 11 primeros calcetines que saca de la lavadora son distintos. El problema consiste en estimar el número de pares de calcetines en la lavadora.
La solución por máxima verosimilitud es infinitos calcetines. En efecto, cuantos más calcetines hubiese en la lavadora, más probable es obtener 11 de ellos distintos. Y la respuesta es tremendamente insatisfactoria.</description>
    </item>
    
    <item>
      <title>¿Lo publico y nos echamos unas risas todos?</title>
      <link>/2018/01/10/lo-publico-y-nos-echamos-unas-risas-todos/</link>
      <pubDate>Wed, 10 Jan 2018 08:13:49 +0000</pubDate>
      
      <guid>/2018/01/10/lo-publico-y-nos-echamos-unas-risas-todos/</guid>
      <description>Estos días, haciendo limpieza de cajones, estanterías y directorios, he dado con un documentito que se me quedó accidentalmente pegado al disco duro hace muchos, muchos años.
Es la documentación metodológica y técnica, firmada por una consultora de postín, de los algoritmos de cálculo de la probabilidad de impago en una de esas entidades financieras que quebraron en su día con enorme estrépito (y perjuicio para el erario público, sea dicho de paso).</description>
    </item>
    
    <item>
      <title>El z-score es una medida inadecuada de la perplejidad</title>
      <link>/2017/12/18/el-z-score-es-una-medida-inadecuada-de-la-perpejidad/</link>
      <pubDate>Mon, 18 Dec 2017 08:13:26 +0000</pubDate>
      
      <guid>/2017/12/18/el-z-score-es-una-medida-inadecuada-de-la-perpejidad/</guid>
      <description>Tenemos un dato y un valor de referencia. Por ejemplo, el valor predicho por uno modelo y el observado. Queremos medir la distancia entre ambos. ¿En qué unidades?
Antes de eso, incluso, ¿para qué queremos medir esa distancia? Esta es la pregunta fácil: para ver cómo encaja en el modelo propuesto, para ver cómo lo sorprende, para cuantificar la perplejidad.
Los estadísticos están acostumbrados a medir la perplejidad en unas unidades que solo ellos entienden, si es que las entienden: desviaciones estándar.</description>
    </item>
    
    <item>
      <title>Martingalas, tiempos de parada y tuits cuasivirales</title>
      <link>/2017/12/13/martingalas-tiempos-de-parada-y-tuits-cuasivirales/</link>
      <pubDate>Wed, 13 Dec 2017 08:13:41 +0000</pubDate>
      
      <guid>/2017/12/13/martingalas-tiempos-de-parada-y-tuits-cuasivirales/</guid>
      <description>El otro día publiqué en Twitter un problema que copié de algún sitio (sinceramente, no recuerdo cuál),
que resultó megaviral en mi humilde tuitescala.
A ver si mañana tengo tiempo de ocuparme de lo triste que resulta que mi entorno de Twitter sea tan cafre como para haber desacertado tanto.
Solo escribo hoy para dejar constancia que la secuencia de variables aleatorias $latex X_n$ con distribución de Bernoulli B(0.5) forma una martingala, que la regla de parar con el primer $latex X_i = 1$ es un tiempo de parada (stopping time) $latex \tau$, que por el teorema correspondiente, $latex X_\tau$ también es un martingala y $latex E(X_\tau) = E(X_1) = 0.</description>
    </item>
    
    <item>
      <title>Estadística(s) y el dedo de Dios</title>
      <link>/2017/11/10/estadisticas-y-el-dedo-de-dios/</link>
      <pubDate>Fri, 10 Nov 2017 08:13:12 +0000</pubDate>
      
      <guid>/2017/11/10/estadisticas-y-el-dedo-de-dios/</guid>
      <description>He usado el vídeo
https://www.youtube.com/watch?v=6YDHBFVIvIs
en un curso de estadística básica para ilustrar a través de experimentos se construyen histogramas y estos convergen a y, en última instancia, justifican el uso de distribuciones de probabilidad.
Es decir,
experimentos -&amp;gt; histogramas -&amp;gt; funciones de distribución.
Y de ahí, el resto.
Pero de todos los vídeos más o menos equivalentes que describen el mismo experimento, me atrajo este en particular por lo que ocurre alrededor del minuto 1:32.</description>
    </item>
    
    <item>
      <title>Sentir números, sentir probabilidades</title>
      <link>/2017/05/22/sentir-numeros-sentir-probabilidades/</link>
      <pubDate>Mon, 22 May 2017 08:13:39 +0000</pubDate>
      
      <guid>/2017/05/22/sentir-numeros-sentir-probabilidades/</guid>
      <description>En El hombre anúmerico J.A. Paulos discute el problema de la visualización (e italizo para indicar que ver no es el fin sino el medio para interiorizar y sentir) números, particularmente, grandes números. Sobre los no excesivamente grandes escribe, p.e.,
Luego discute técnicas para sentir igualmente otras magnitudes mayores. The more personal you can make these collections, the better, insiste.
¿Y con las probabilidades? Pensando al respecto, di con una feliz idea que tiene el concomitante aprovechamiento de encontrarle finalmente un buen uso al puto fúrgol (p.</description>
    </item>
    
    <item>
      <title>Calibración de probabilidades vía apuestas</title>
      <link>/2017/03/13/calibracion-de-probabilidades-via-apuestas/</link>
      <pubDate>Mon, 13 Mar 2017 08:13:16 +0000</pubDate>
      
      <guid>/2017/03/13/calibracion-de-probabilidades-via-apuestas/</guid>
      <description>Después de la remontada del F.C. Barcelona es muy de agradecer ver la publicación de artículos como Cómo de improbable era la remontada del Barcelona de Kiko Llaneras. En la misma entradilla, indica que [u]n modelo estadístico y las apuestas le daban el 7% de opciones. Un 7% viene a ser más o menos, dice correctamente, como sacar un 11 o un 12 en una tirada de dos dados.
La pregunta que podemos hacernos, de todos modos, es si las probabilidades estimadas por esos modelos estadísticos o las casas de apuestas están o no bien calibradas.</description>
    </item>
    
    <item>
      <title>Reducción de la dimensionalidad con t-SNE</title>
      <link>/2017/03/08/reduccion-de-la-dimensionalidad-con-t-sne/</link>
      <pubDate>Wed, 08 Mar 2017 08:13:41 +0000</pubDate>
      
      <guid>/2017/03/08/reduccion-de-la-dimensionalidad-con-t-sne/</guid>
      <description>Voy a explicar aquí lo que he aprendido recientemente sobre t-SNE, una técnica para reducir la dimensionalidad de conjuntos de datos. Es una alternativa moderna a MDS o PCA.
Partimos de puntos $latex x_1, \dots, x_n$ y buscamos otros $latex y_1, \dots, y_n$ en un espacio de menor dimensión. Para ello construiremos primero $latex n$ distribuciones de probabilidad, $latex p_i$ sobre los enteros $latex 1, \dots, n$ de forma que</description>
    </item>
    
    <item>
      <title>Otro ejemplo de infradispersión de conteos</title>
      <link>/2017/02/23/otro-ejemplo-de-infradispersion-de-conteos/</link>
      <pubDate>Thu, 23 Feb 2017 08:13:01 +0000</pubDate>
      
      <guid>/2017/02/23/otro-ejemplo-de-infradispersion-de-conteos/</guid>
      <description>???? ¿ESTÁN USTEDES LOCOS? ???? pic.twitter.com/hyqI9Ncxqg
 &amp;ndash; ? RadiactivoMan ? (@RadiactivoMan) 16 de febrero de 2017 Esta entrada, obviamente, viene a cuento de esta otra.</description>
    </item>
    
    <item>
      <title>La inesperada correlación de los ratios</title>
      <link>/2017/02/09/la-inesperada-correlacion-de-los-ratios/</link>
      <pubDate>Thu, 09 Feb 2017 08:13:13 +0000</pubDate>
      
      <guid>/2017/02/09/la-inesperada-correlacion-de-los-ratios/</guid>
      <description>Tomemos dos variables aleatorias independientes y positivas,
set.seed(123) n &amp;lt;- 100 x &amp;lt;- runif(n) + 0.5 y &amp;lt;- runif(n) + 0.5  No tengo ni que decir que su correlación es prácticamente cero,
cor(x,y) #-0.0872707  y que en su diagrama de dispersión tampoco vamos a poder leer otra cosa:
Ahora generamos otra variable independiente de las anteriores,
z &amp;lt;- runif(n) + 0.5  y calculamos el cociente de las primeras con respecto a esta:</description>
    </item>
    
    <item>
      <title>1/e por doquier</title>
      <link>/2017/02/06/1e-por-doquier/</link>
      <pubDate>Mon, 06 Feb 2017 08:13:52 +0000</pubDate>
      
      <guid>/2017/02/06/1e-por-doquier/</guid>
      <description>Leía ¿Es muy difícil (estadísticamente) no dar ni una?, donde se discute la probabilidad de que $latex s(i) \neq i$ $latex \forall i$ cuando $latex s$ es una permutación. El problema está relacionado, como podrá ver quien visite el enlace, con la probabilidad de repetición del sorteo en el juego del amigo invisible.
Esta probabilidad converge, al crecer $latex n$, a $latex 1/e \approx 0.367879$. ¡0.367879! Eso es&amp;hellip; eso es&amp;hellip; ¡1 - .</description>
    </item>
    
    <item>
      <title>La regla del tres (para estimar la probabilidad de un evento todavía no observado)</title>
      <link>/2016/11/30/la-regla-del-tres-para-estimar-la-probabilidad-de-un-evento-todavia-no-observado/</link>
      <pubDate>Wed, 30 Nov 2016 08:13:04 +0000</pubDate>
      
      <guid>/2016/11/30/la-regla-del-tres-para-estimar-la-probabilidad-de-un-evento-todavia-no-observado/</guid>
      <description>Me acusan (quien lo hizo, si me lee, sabrá identificarse) de repetirme, de contar una historia dos, y sino me paran los pies, tres y más veces. Ya me pasó una vez por aquí. Espero que no me esté volviendo a suceder hoy porque habría jurado haber mencionado este asunto antes.
Es el de la estimación de la probabilidad de eventos todavía no observados. Traduzco y (como no rectoreo universidad pública alguna y, por ende, no puedo permitirme el lujo de copiar sin citar) luego diré de donde:</description>
    </item>
    
    <item>
      <title>Modelos gráficos probabilísticos en Coursera</title>
      <link>/2016/10/31/modelos-graficos-probabilisticos-en-coursera/</link>
      <pubDate>Mon, 31 Oct 2016 08:13:26 +0000</pubDate>
      
      <guid>/2016/10/31/modelos-graficos-probabilisticos-en-coursera/</guid>
      <description>Acabo de terminar el primero de los tres cursos sobre modelos gráficos probabilísticos de Coursera.
El curso sigue una sinuosa senda a través del libro (¡1200 páginas!) Probabilistic Graphical Models de D. Koller y N. Friedman. Aunque cueste un potosí, es posible hojearlo gratis para ver si vale la pena o no comprarlo gracias a nuestros amigos de LibGen.
Tiene mucho de bueno. Lo mejor, sin duda alguna, el universo de problemas que plantea y a los que se aplican los modelos gráficos.</description>
    </item>
    
    <item>
      <title>Probabilidades y probabilidades</title>
      <link>/2016/10/17/probabilidades-y-probabilidades/</link>
      <pubDate>Mon, 17 Oct 2016 08:13:59 +0000</pubDate>
      
      <guid>/2016/10/17/probabilidades-y-probabilidades/</guid>
      <description>Leo hoy que
https://twitter.com/kikollan/status/787594944106098689
Pero:
 * Hemos visto a Cristiano Ronaldo chutar muchos penaltis y hemos podido calcular el cociente entre los anotados y los tirados. * Es la primera vez en la vida que Trump se presenta a las elecciones de EE.UU.  ¿A nadie le intriga cuál es ese misterioso mecanismo por el que se pueden comparar ambas probabilidades? [Voy a usar ontológicamente] ¿Nadie las ve ontológicamente distintas?</description>
    </item>
    
    <item>
      <title>Hamilton al rescate de Metropolis-Hastings</title>
      <link>/2016/09/16/hamilton-al-rescate-de-metropolis-hastings/</link>
      <pubDate>Fri, 16 Sep 2016 08:13:12 +0000</pubDate>
      
      <guid>/2016/09/16/hamilton-al-rescate-de-metropolis-hastings/</guid>
      <description>El algoritmo de Metropolis-Hastings se usa para muestrear una variable aleatoria con función de densidad $latex p$. Permite crear una sucesión de puntos $latex x_i$ que se distribuye según $latex p$.
Funciona de al siguiente manera: a partir de un punto $latex x_i$ se buscan candidatos a $latex x_{i+1}$ de la forma $latex x_i + \epsilon$, donde $latex \epsilon$ es, muy habitualmente, $latex N(0, \delta)$ y $latex \delta$ es pequeño. De otra manera, puntos próximos a $latex x_i$.</description>
    </item>
    
    <item>
      <title>Las distribuciones (y platos) con nombre</title>
      <link>/2016/06/14/las-distribuciones-con-nombre-son-como-los-platos-con-nombre/</link>
      <pubDate>Tue, 14 Jun 2016 08:13:31 +0000</pubDate>
      
      <guid>/2016/06/14/las-distribuciones-con-nombre-son-como-los-platos-con-nombre/</guid>
      <description>Hay platos con nombre. P.e., tortilla de patata o tiramisú. También hay distribuciones (de probabilidad) con nombre. P.e., normal, binomial, Poisson, hipergeométrica.
Hay quienes quieren saber (1) todas (o muchas) de esas distribuciones con nombre y (2), dados unos datos, cuál de ellas siguen. Esta entrada va a tener la url a la que de ahora en adelante remita a quien me las formule.
A pesar de que algunos platos tienen nombre, el otro día se podía probar en el Diverxo espárrago blanco a la mantequilla negra con emulsión de leche de oveja, espardeña y salmonete.</description>
    </item>
    
    <item>
      <title>Funciones de densidad log-cóncavas</title>
      <link>/2016/03/30/funciones-de-densidad-log-concavas/</link>
      <pubDate>Wed, 30 Mar 2016 09:13:25 +0000</pubDate>
      
      <guid>/2016/03/30/funciones-de-densidad-log-concavas/</guid>
      <description>Las funciones de densidad log-cóncavas son aquellas cuyo logaritmo es una función cóncava. Por ejemplo, la normal: el logaritmo de su función de densidad es, constantes aparte, $latex -x^2/2$.
El producto de dos funciones de densidad log-cóncavas es log-cóncava: $latex \log(fg) = \log f + \log g$ (y la suma de cóncavas es cóncava: calcula la segunda derivada). También lo son la suma de dos variables aleatorias cuyas funciones de densidad lo son (la demostración es consecuencia de esta desigualdad).</description>
    </item>
    
    <item>
      <title>Un ejemplo de &#34;importance sampling&#34; (que no sé cómo traducir)</title>
      <link>/2016/03/28/un-teorema-de-muestreo-que-no-se-si-existe/</link>
      <pubDate>Mon, 28 Mar 2016 09:13:26 +0000</pubDate>
      
      <guid>/2016/03/28/un-teorema-de-muestreo-que-no-se-si-existe/</guid>
      <description>Imaginemos que queremos muestrear una variable aleatoria cuya función de densidad es (proporcional a) el producto de otras dos (no necesariamente propias). Por ejemplo, la gamma, cuya función de densidad es $latex K x^{k-1} \exp(-\lambda x)$, el producto de una exponencial y una distribución impropia con densidad $latex x^{k-1}$.
Supongamos que no sabemos hacer
&amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/base/set.seed&amp;quot;&amp;gt;set.seed(1234) shape &amp;lt;- 3 rate &amp;lt;- 3 m0 &amp;lt;- &amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/stats/rgamma&amp;quot;&amp;gt;rgamma(1000, shape = shape, rate = rate)  Pero supongamos que sí que sabemos muestrear la distribución exponencial, lo que permite escribir:</description>
    </item>
    
    <item>
      <title>Lenguajes de programación probabilísticos</title>
      <link>/2016/03/09/lenguajes-de-programacion-probabilisticos/</link>
      <pubDate>Wed, 09 Mar 2016 09:13:09 +0000</pubDate>
      
      <guid>/2016/03/09/lenguajes-de-programacion-probabilisticos/</guid>
      <description>Son lenguajes de programación diseñados para describir odelos probabilísticos y realizar inferencias sobre dichos modelos.
El resto de la entrada de la Wikipedia sobre este apasionante (y lo uso sin retintín) tema, aquí (y puede que también quieras visitar esto).</description>
    </item>
    
    <item>
      <title>Evidencialidad</title>
      <link>/2016/02/17/evidencialidad/</link>
      <pubDate>Wed, 17 Feb 2016 09:13:44 +0000</pubDate>
      
      <guid>/2016/02/17/evidencialidad/</guid>
      <description>Por afición y, últimamente, por motivos laborales también, me ha preocupado cómo se refleja la incertidumbre en el lenguaje y cómo este sirve para transmitir aquella (véase, por ejemplo, esto).
En el español tenemos algunos recursos para manifestar grados de certidumbre (el condicional, el subjuntivo, etc.). Véanse por ejemplo (esta es la referencia) a los 570 sufridos hablantes del tuyuca que no pueden decir simplemente &amp;ldquo;él jugaba al fútbol&amp;rdquo;, sino que tienen que elegir obligatoriamente entre los diferentes sufijos verbales que (además de indicar la persona y el tiempo) indican el modo por el cual el hablante obtuvo el conocimiento que afirma en el enunciado:</description>
    </item>
    
    <item>
      <title>El problema de los tanques alemanes y de la máxima verosimilitud esquinada</title>
      <link>/2016/01/18/el-problema-de-los-tanques-alemanes-y-de-la-maxima-verosimilitud-esquinada/</link>
      <pubDate>Mon, 18 Jan 2016 08:13:11 +0000</pubDate>
      
      <guid>/2016/01/18/el-problema-de-los-tanques-alemanes-y-de-la-maxima-verosimilitud-esquinada/</guid>
      <description>El problema en cuestión, que se ve, surgió durante la II Guerra Mundial, es el siguiente: se capturan tanques del enemigo y se anotan los números de serie, supuestos sucesivos. ¿Cuál es la mejor estimación del número total de tanques fabricados por el enemigo?
Si se capturan k, la distribución del máximo número observado, m, en función del número no observado (nuestro parámetro) de tanques es
$latex f(N;m,k)=\frac{\binom{m-1}{k-1}}{\binom{N}{k}}$
y como esta función es decreciente en $latex N$, la estimación por máxima verosimilitud es $latex \hat{N} = m$.</description>
    </item>
    
    <item>
      <title>La paradoja de Berkson</title>
      <link>/2015/10/19/la-paradoja-de-berkson/</link>
      <pubDate>Mon, 19 Oct 2015 08:13:09 +0000</pubDate>
      
      <guid>/2015/10/19/la-paradoja-de-berkson/</guid>
      <description>Queremos calentar unas empanadas en el horno y, ¡oh desgracia!, no funciona. Pueden pasar dos cosas (independientes entre sí):
 * El horno está estropeado ($latex A$) * El horno está desenchufado ($latex B$)  Hemos observado el evento $latex A \cup B$ y nos preocupa mucho $latex P(A | A \cup B)$, es decir, que tengamos que llamar al técnico y comernos frías las empanadas a la vista de que el horno no responde.</description>
    </item>
    
    <item>
      <title>Odds = probabilidades</title>
      <link>/2015/09/01/odds-probabilidades/</link>
      <pubDate>Tue, 01 Sep 2015 08:13:04 +0000</pubDate>
      
      <guid>/2015/09/01/odds-probabilidades/</guid>
      <description>El otro día medio participé en una conversación en Twitter sobre el significado de los odds. Recientemente leí una entrada en la bitácora de un holandés que se quejaba de lo difícil que resulta encontrar un equivalente de esa palabra a su idioma. Pasa lo mismo en español: no existe una traducción directa; no existe, siquiera, el concepto.
Sugiero traducir odds, y lo haré así a lo largo de la entrada, como probabilidades.</description>
    </item>
    
    <item>
      <title>Tres monedas y un argumento falaz</title>
      <link>/2015/08/27/tres-monedas-y-un-argumento-falaz/</link>
      <pubDate>Thu, 27 Aug 2015 08:13:12 +0000</pubDate>
      
      <guid>/2015/08/27/tres-monedas-y-un-argumento-falaz/</guid>
      <description>Tiras tres monedas. ¿Cuál es la probabilidad de obtener tres valores (cara o cruz) iguales? Es, lo sabemos todos, 0.25: de las ocho opciones posibles, solo dos cumplen.
Ahora, el argumento falaz —dizque de Francis Galton— que prueba que dicha probabilidad es de 0.5. Es así: de las tres monedas, dos tienen que coincidir necesariamente en valor; entonces la tercera, con probabilidad 0.5, coincidirá con los anteriores y con la misma discrepará.</description>
    </item>
    
    <item>
      <title>Estar en racha (y promediar promedios)</title>
      <link>/2015/08/10/estar-en-racha-y-promediar-promedios/</link>
      <pubDate>Mon, 10 Aug 2015 08:13:27 +0000</pubDate>
      
      <guid>/2015/08/10/estar-en-racha-y-promediar-promedios/</guid>
      <description>Suponemos que observamos rachas de longitud 2 + rpois(1, 10) de un juego en el que se tiene éxito (1) o se fracasa (0) con probabilidad 1/2. Nos interesa saber si existe eso de las rachas de suerte, es decir, si es más probable que a un éxito le suceda otro o lo contrario.
El observador ve rachas y calcula el número de veces que a un éxito le sigue un éxito y el número de veces que a un éxito le sigue un fracaso así:</description>
    </item>
    
    <item>
      <title>Decisiones &#34;a ojo de buen cubero&#34;</title>
      <link>/2015/02/19/decisiones-a-ojo-de-buen-cubero/</link>
      <pubDate>Thu, 19 Feb 2015 08:13:31 +0000</pubDate>
      
      <guid>/2015/02/19/decisiones-a-ojo-de-buen-cubero/</guid>
      <description>¿Os acordáis del problema de la carta del otro día? Lo extraje del libro Risk Savvy de G. Gigerenzer.
Uno de los grandes temas del libro es la distinción entre riesgo e incertidumbre. Se decanta por la perspectiva de Knight discutida en el enlace anterior: en situaciones de riesgo, la distribución de probabilidad es conocida (p.e., juegos de azar) y el aparataje probabilístico puede ser aplicado en su entera potencia matemática.</description>
    </item>
    
    <item>
      <title>Un problema de cartas</title>
      <link>/2015/02/17/un-problema-de-cartas/</link>
      <pubDate>Tue, 17 Feb 2015 08:13:39 +0000</pubDate>
      
      <guid>/2015/02/17/un-problema-de-cartas/</guid>
      <description>Hay tres cartas. Una de ellas tiene ambas caras rojas; otra, ambas azules; la última, una roja y una azul. Al azar, te ponen una sobre la mesa. La cara que ves es roja. ¿Cuál es la probabilidad de que la cara que no ves sea también roja?</description>
    </item>
    
    <item>
      <title>Juegos justos con monedas truchas</title>
      <link>/2015/01/07/juegos-justos-con-monedas-truchas/</link>
      <pubDate>Wed, 07 Jan 2015 07:13:53 +0000</pubDate>
      
      <guid>/2015/01/07/juegos-justos-con-monedas-truchas/</guid>
      <description>—¿Cara (H) o cruz (T)? —Sí.
Lo siento, ese era otro chiste. Comienzo de nuevo.
—¿Cara (H) o cruz (T)? —No me fío porque tu moneda es trucha. Salen más H (o T) que T (o H, tanto da). —Aun así podemos plantear un juego justo. —¿Cómo? —Cada uno elige HT o TH. (i) Se tira la moneda dos veces. Si sale HH o TT, GOTO (i). Si sale otra cosa, gana quien haya elegido tal combinación.</description>
    </item>
    
    <item>
      <title>Simpson y la plebe anumérica</title>
      <link>/2014/11/13/simpson-y-la-plebe-anumerica/</link>
      <pubDate>Thu, 13 Nov 2014 07:13:19 +0000</pubDate>
      
      <guid>/2014/11/13/simpson-y-la-plebe-anumerica/</guid>
      <description>Supongamos que los habitantes de un país tienen una probabilidad determinada (y no necesariamente igual) $latex p_i$ de comprar un determinado producto. Supongamos que se lanza una campaña publicitaria que incrementa en una cantidad fija $latex \epsilon$, p.e., 5%, esa probabilidad.
Supongamos, finalmente, que se trata de una cantidad que se desea estimar.
Unos individuos reciben la campaña publicitaria. Otros no. ¿Cuál es la diferencia entre las proporciones de individuos que compran el producto en uno y otro grupo?</description>
    </item>
    
    <item>
      <title>¿Un 30% de probabilidad de que llueva mañana?</title>
      <link>/2014/11/11/un-30-de-probabilidad-de-que-llueva-manana/</link>
      <pubDate>Tue, 11 Nov 2014 07:13:27 +0000</pubDate>
      
      <guid>/2014/11/11/un-30-de-probabilidad-de-que-llueva-manana/</guid>
      <description>¿Qué significa que [los servicios meteorológicos digan que] hay un 30% de probabilidad de que llueva mañana? Pues resulta que significa distintas cosas para distintas personas, al menos, según A 30% Chance of Rain Tomorrow: How Does the Public Understand Probabilistic Weather Forecasts?
En ese artículo Gigerenzer y sus coautores proponen a una muestra de sujetos las opciones siguientes:
 * Mañana lloverá el 30% del tiempo. * El 30% de los días que siguen a uno como el de hoy, llueve.</description>
    </item>
    
    <item>
      <title>Dislexia probabilística</title>
      <link>/2014/11/04/dislexia-probabilistica/</link>
      <pubDate>Tue, 04 Nov 2014 07:13:14 +0000</pubDate>
      
      <guid>/2014/11/04/dislexia-probabilistica/</guid>
      <description>Esta entrada trata de cuadrados. Tales como estos

Son dos cuadrados de area 10 y 2.
En realidad, mi entrada trata de una configuración de cuadrados solo marginalmente más complicada, esta:

Todo el mundo podría decir (y es cierto) que el área de la intersección de los cuadrados es el 3.3% de la del mayor y el 16.5% de la del menor. Son dos afirmaciones ambas ciertas y, por supuesto, compatibles.</description>
    </item>
    
    <item>
      <title>Más allá del teorema central del límite</title>
      <link>/2014/10/21/mas-alla-del-teorema-central-del-limite/</link>
      <pubDate>Tue, 21 Oct 2014 07:13:53 +0000</pubDate>
      
      <guid>/2014/10/21/mas-alla-del-teorema-central-del-limite/</guid>
      <description>Uno espera la media de un número suficiente de variables aleatorias razonablemente iid tenga una distribución normal. Uno casi espera siempre obtener ese aburrido histograma cada vez que remuestrea medias. La gente dice que el teorema central del límite rige necesariamente cuando su tamaño muestral es del orden de magnitud del bruto anual de un gerifalte. Etc.
Pero a veces uno tropieza con distribuciones bootstrap tales como

que le hacen recordar que existe un universo más allá de las hipótesis de esos teoremas tan manidos; que la teoría, al final, solo llega hasta donde llega y que, en definitiva, hay que estar siempre alerta y desconfiar del rituales y automatismos.</description>
    </item>
    
    <item>
      <title>Causalidad a la Pearl y el operador do</title>
      <link>/2014/06/24/causalidad-a-la-pearl-y-el-operador-do/</link>
      <pubDate>Tue, 24 Jun 2014 07:06:49 +0000</pubDate>
      
      <guid>/2014/06/24/causalidad-a-la-pearl-y-el-operador-do/</guid>
      <description>Un tipo me pasó el librito de Pearl, Causality, y se ha pasado varios días dando la vara con que si me había leído ya el epígrafe. Pues sí, lo he leído este finde. Y no solo lo he leído sino que voy a escribir sobre ello.
Había tratado de leer cosas de Pearl en el pasado. Pero las encontraba demasiado llenas de letras difíciles de comprender si no se entendían bien las fórmulas.</description>
    </item>
    
    <item>
      <title>Todo el mundo habla de cadenas de Markov</title>
      <link>/2014/04/29/todo-el-mundo-habla-de-cadenas-de-markov/</link>
      <pubDate>Tue, 29 Apr 2014 07:19:00 +0000</pubDate>
      
      <guid>/2014/04/29/todo-el-mundo-habla-de-cadenas-de-markov/</guid>
      <description>Todo el mundo habla últimamente de cadenas de Markov. ¿No os habéis dado cuenta? ¿O seré yo el que saca a relucir el asunto venga o no al caso? Sea que se haya puesto de moda o que esté mi misma obsesión por el asunto sesgando mi impresión sobre sobre (me encanta escribir dos preposiciones seguidas) lo que la gente habla, es el caso que el otro día me comprometí a escribir sobre</description>
    </item>
    
    <item>
      <title>El chocheo de los dioses</title>
      <link>/2014/03/03/el-chocheo-de-los-dioses/</link>
      <pubDate>Mon, 03 Mar 2014 07:52:39 +0000</pubDate>
      
      <guid>/2014/03/03/el-chocheo-de-los-dioses/</guid>
      <description>Uno tiene —o tuvo— dioses. Sentir admiración por alguien (y su obra) tiene, lo admito, una dimensión infantil. Es también, por supuesto, una sobre simplificación de la realidad. Porque la verdad no la escriben cuatro plumas: las ideas valiosas emergen por doquier. Desafortunadamente, nadie tiene tiempo para filtrar el flujo diario de noticias, libros, conceptos. Así que creo que es excusable que, por simplificar, uno eleve personal y subjetivamente a una serie de individuos a la categoría de dioses, de encargados de filtrar la información.</description>
    </item>
    
    <item>
      <title>De ratios, apuestas y riesgos</title>
      <link>/2014/02/12/de-ratios-apuestas-y-riesgos/</link>
      <pubDate>Wed, 12 Feb 2014 07:49:19 +0000</pubDate>
      
      <guid>/2014/02/12/de-ratios-apuestas-y-riesgos/</guid>
      <description>Nunca he entendido eso de los odds. Me refiero a eso que mencionan las películas: ocho contra uno a favor de tal, cinco contra tres a favor de cual. Y no creo que sea el único al que le son ajenos. De hecho, la página de la Wikipedia en español correspondiente a la inglesa para odds se refiere a ellas como cuotas, término que jamás hasta hoy había visto así usado.</description>
    </item>
    
    <item>
      <title>El otro problema del cumpleaños</title>
      <link>/2014/02/05/el-otro-problema-del-cumpleanos/</link>
      <pubDate>Wed, 05 Feb 2014 08:14:21 +0000</pubDate>
      
      <guid>/2014/02/05/el-otro-problema-del-cumpleanos/</guid>
      <description>Hay un problema famoso sobre cumpleaños cuya respuesta es 23. Hoy propongo otro relacionado.
Todos los días entras a Facebook y miras cuáles de tus amigos cumplen años para enviarles una felicitación. La pregunta es: ¿cuál es el número mínimo de amigos que tienes que tener para que con una probabilidad mayor de 0.5 tengas que felicitar a alguien cada día del año?</description>
    </item>
    
    <item>
      <title>Muestreos aleatorios sobre la península Ibérica, por ejemplo</title>
      <link>/2013/12/26/muestreos-aleatorios-sobre-la-peninsula-iberica-por-ejemplo/</link>
      <pubDate>Thu, 26 Dec 2013 07:10:39 +0000</pubDate>
      
      <guid>/2013/12/26/muestreos-aleatorios-sobre-la-peninsula-iberica-por-ejemplo/</guid>
      <description>El problema fue sugerido por Eloy Ortiz en un mensaje a r-help-es. Quería saber cómo muestrear aleatoriamente (i.e., uniformemente) puntos sobre una región de la superficie terrestre delimitada por su bounding box (i.e., las coordenadas que definen un rectángulo sobre la esfera).
Obviamente, no vale con muestrear latitud y longitud uniformemente: el área comprendida entre dos meridianos cerca del ecuador es mayor que la comprendida entre otros dos más próximos al polo.</description>
    </item>
    
    <item>
      <title>¿Cuántos peces hay en un lago?</title>
      <link>/2013/12/05/cuantos-peces-hay-en-un-lago/</link>
      <pubDate>Thu, 05 Dec 2013 07:14:48 +0000</pubDate>
      
      <guid>/2013/12/05/cuantos-peces-hay-en-un-lago/</guid>
      <description>Quien haya estudiado estadística o probabilidad en algún tipo de institución que ofrece educación reglada se habrá topado con el problema de estimar el número de peces de un lago.
Esencialmente, lo que puede hacerse (dado que es imposible realizar un censo completo) es lo siguiente:
 * Pescar cierto número de peces, p1, marcarlos y devolverlos al lago. * Pescar cierto número de peces, p2, y contar cuántos de ellos fueron marcados el día anterior, n.</description>
    </item>
    
    <item>
      <title>Un pequeño problema de probabilidad</title>
      <link>/2013/11/22/un-pequeno-problema-de-probabilidad/</link>
      <pubDate>Fri, 22 Nov 2013 07:14:13 +0000</pubDate>
      
      <guid>/2013/11/22/un-pequeno-problema-de-probabilidad/</guid>
      <description>El tuit

de John Allen Paulos me indujo a escribir
number.numbers &amp;lt;- function(n){ sum(cumsum(sample(0:n)) &amp;lt; n) + 1 } res &amp;lt;- replicate(10000, number.numbers(1000))  código con el que, efectivamente, puede comprobarse que la media es, efectivamente, e.
Ahora bien, ¿alguien se atreve a explicar por qué?
(No leas esta pista: (s??)?s??).</description>
    </item>
    
    <item>
      <title>Statistics Online Computational Resource</title>
      <link>/2013/11/18/statistics-online-computational-resource/</link>
      <pubDate>Mon, 18 Nov 2013 07:22:36 +0000</pubDate>
      
      <guid>/2013/11/18/statistics-online-computational-resource/</guid>
      <description>Sigo sin estar fino para hacer entradas interesantes. Así que de nuevo me voy a limitar a ejercer de divulgador de lo ajeno. Y hoy le corresponde el turno al Statistics Online Computational Resource, un portal nacido con el objetivo de fomentar el conocimiento de la estadística y la probabilidad en línea.
Podría abundar sobre los recursos disponibles en SOCR, pero prefiero ahorrar mi tiempo y el de mis lectores invitándolos directamente a visitarlo y comprobarlo por sí mismos.</description>
    </item>
    
    <item>
      <title>¿Eres un analfabeto numérico?</title>
      <link>/2013/01/03/eres-un-analfabeto-numerico/</link>
      <pubDate>Thu, 03 Jan 2013 07:46:36 +0000</pubDate>
      
      <guid>/2013/01/03/eres-un-analfabeto-numerico/</guid>
      <description>Si quieres comparar tu nivel de alfabetización numérica con una muestra de personas con estudios universitarios de muchas partes del mundo, puedes realizar este test.
Se lo llama Berlin Numeracy Test y está descrito en este artículo. Y de él extraigo una tabla, la cinco,

en la que aparecen los resultados del test en función de la combinación de país e idioma y ordenados por el porcentaje de respuestas en los cuartiles superiores.</description>
    </item>
    
    <item>
      <title>Lo normal: sumar doce, restar seis</title>
      <link>/2012/11/20/lo-normal-sumar-doce-restar-seis/</link>
      <pubDate>Tue, 20 Nov 2012 07:40:45 +0000</pubDate>
      
      <guid>/2012/11/20/lo-normal-sumar-doce-restar-seis/</guid>
      <description>Un truco para generar variables aleatorias normales: sumar doce uniformes y restar seis.
En efecto,
x &amp;lt;- replicate(1000, sum( runif(12) - 6 )) &amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/stats/qqnorm&amp;quot;&amp;gt;qqnorm(x) &amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/stats/qqline&amp;quot;&amp;gt;qqline(x, col=2)  produce

Ayuda a entender el motivo que la varianza de la distribución uniforme es 1/12 y que su media es 1/2.</description>
    </item>
    
    <item>
      <title>HHH, HHT y el comando &#34;yield&#34; de Python</title>
      <link>/2012/10/26/hhh-hht-y-el-comando-yield-de-python/</link>
      <pubDate>Fri, 26 Oct 2012 07:18:38 +0000</pubDate>
      
      <guid>/2012/10/26/hhh-hht-y-el-comando-yield-de-python/</guid>
      <description>Variable aleatoria X: tiramos una moneda al aire sucesivamente y contamos el número de veces que lo hacemos hasta obtener el patrón HHH (tres caras) en las tres últimas tiradas.
Variable aleatoria Y: lo mismo, pero hasta que salga el patrón HHT.
Entonces las medias de X e Y son iguales, ¿verdad? Pues no. (¿Alguien sabría decirme cuál de las combinaciones, HHH o HHT, tiende, en promedio, a aparecer antes? Pueden darse explicaciones muy complejas, pero existe una muy simple e intuitiva).</description>
    </item>
    
    <item>
      <title>Un (¿sutil?) error en el cálculo de probabilidades en El País</title>
      <link>/2012/09/24/un-sutil-error-en-el-calculo-de-probabilidades-en-el-pais/</link>
      <pubDate>Mon, 24 Sep 2012 07:57:24 +0000</pubDate>
      
      <guid>/2012/09/24/un-sutil-error-en-el-calculo-de-probabilidades-en-el-pais/</guid>
      <description>Leo en El País que
Hummmm&amp;hellip; a primera vista, sabiendo que 3000 años viene a ser un millón de días, me salían no 3000 años sino 9000, el triple. ¿Estaría en lo cierto?
Si se registran 0.37 accidentes por millón de vuelos, cabe esperar que la ley que rige el fenómeno sea Poisson de parámetro 3.7e-07. De acuerdo con la relación existente entre la distribución de Poisson y la exponencial, sabemos que el tiempo, es decir, el número de vuelos, hasta el siguiente accidente está gobernado por una distribución exponencial con el mismo parámetro.</description>
    </item>
    
    <item>
      <title>Las dos preguntas fundamentales de la teoría de los valores extremos</title>
      <link>/2012/09/21/las-dos-preguntas-fundamentales-de-la-teoria-de-los-valores-extremos/</link>
      <pubDate>Fri, 21 Sep 2012 07:23:35 +0000</pubDate>
      
      <guid>/2012/09/21/las-dos-preguntas-fundamentales-de-la-teoria-de-los-valores-extremos/</guid>
      <description>En muchos ocasiones es necesario realizar estimaciones sobre el máximo de una serie de valores aleatorios.
Uno de los casos más conocidos que me vienen a la mente es el llamado problema de los tanques alemanes. Durante la II Guerra Mundial, los aliados, para estimar el ritmo de producción de tanques del enemigo, recogían el número de serie de los que destruían o capturaban. Gracias a esta muestra potencialmente aleatoria, podían realizar estimaciones del máximo de la serie y, de ahí, del número de unidades construidas durante cierto intervalo de tiempo.</description>
    </item>
    
    <item>
      <title>Odds ratio vs probabilidad</title>
      <link>/2012/08/09/odds-ratio-vs-probabilidad/</link>
      <pubDate>Thu, 09 Aug 2012 06:54:23 +0000</pubDate>
      
      <guid>/2012/08/09/odds-ratio-vs-probabilidad/</guid>
      <description>Hoy he sabido vía Twitter lo siguiente:

Como me ha intrigado el asunto de lo de la probabilidad, he acudido al artículo original donde he aprendido que (y, excúsenme: por primera vez no traduzco este tipo de citas):
Es decir, lo que es 4.8 veces mayor no es la probabilidad en sí sino el odds ratio, concepto que no sé cómo carajos traducir al español. Y el odds ratio no es la probabilidad sino lo que dice la Wikipedia al respecto, que es, en resumen,</description>
    </item>
    
    <item>
      <title>Desencriptando (II): la avaricia es mala</title>
      <link>/2012/05/28/desencriptando-ii-la-avaricia-es-mala/</link>
      <pubDate>Mon, 28 May 2012 07:07:51 +0000</pubDate>
      
      <guid>/2012/05/28/desencriptando-ii-la-avaricia-es-mala/</guid>
      <description>El otro día propuse y resolví un problema de encriptación con R. Utilizaba uno de los llamados métodos avariciosos (o greedy) para hallar el máximo de una función (que era, en esencia, la función de verosimilitud de una determinada permutación de caracteres dentro del espacio probabilístico de todas ellas).
Este método funcionó con una cadena relativamente larga para desencriptar pero falla con otras más cortas. Por ejemplo, con
cadena &amp;lt;-c(&amp;quot;u&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;i&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;h&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;y&amp;quot;, &amp;quot;b&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;h&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;y&amp;quot;, &amp;quot;r&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;i&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;i&amp;quot;,&amp;quot;n&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;c&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;o&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;c&amp;quot;, &amp;quot;n&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;i&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;i&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;r&amp;quot;)  Si ejecuto el código que presenté el otro día,</description>
    </item>
    
    <item>
      <title>Jugar empobrece</title>
      <link>/2012/05/24/jugar-empobrece/</link>
      <pubDate>Thu, 24 May 2012 06:49:02 +0000</pubDate>
      
      <guid>/2012/05/24/jugar-empobrece/</guid>
      <description>Fumar mata. Cien gramos de barritas de cereales (cuatro unidades) contienen 0.2 gramos de sodio. Y unos carteles amarillos indican que hay que tener cuidado al pisar porque acaban de fregar el suelo. El estado quiere que dejemos de fumar, tengamos una dieta sana y no nos descalabremos.
Pero, ¿por qué no se etiquetan de igual manera las máquinas tragaperras? Un reciente artículo en Significance especula sobre la manera en que poder transmitir información sobre la peligrosidad de estos dispositivos a sus —tristes— usuarios.</description>
    </item>
    
    <item>
      <title>La paradoja del cumpleaños y el niño que colecciona cromos de futbolistas</title>
      <link>/2012/05/22/la-paradoja-del-cumpleanos-y-el-nino-que-colecciona-cromos-de-futbolistas/</link>
      <pubDate>Tue, 22 May 2012 07:40:52 +0000</pubDate>
      
      <guid>/2012/05/22/la-paradoja-del-cumpleanos-y-el-nino-que-colecciona-cromos-de-futbolistas/</guid>
      <description>El otro día vi el programa Descifrar las probabilidades en la vida de Punset en el que se repasan varios problemas más o menos prácticos en los que el cálculo de las probabilidades juega cierto papel.
Entre ellos menciona el de la llamada paradoja del cumpleaños: resulta que si 23 personas se juntan en una fiesta, existe aproximadamente un 50% de probabilidades de que dos de ellos tengan el mismo cumpleaños.</description>
    </item>
    
    <item>
      <title>Desencriptando (I): el problema de un mal amigo</title>
      <link>/2012/05/21/desencriptando-i-el-problema-de-un-mal-amigo/</link>
      <pubDate>Mon, 21 May 2012 07:01:29 +0000</pubDate>
      
      <guid>/2012/05/21/desencriptando-i-el-problema-de-un-mal-amigo/</guid>
      <description>Tengo un muy mal amigo que, sabiendo cómo soy para esas cosas y de qué manera me quitan el sueño, quiso alterar mi solaz enviándome esto:
cadena &amp;lt;- c( &amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;, &amp;quot;t&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;y&amp;quot;, &amp;quot;z&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;c&amp;quot;, &amp;quot;z&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;, &amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;w&amp;quot;, &amp;quot;s&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;h&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;, &amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;, &amp;quot;u&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;, &amp;quot;k&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;s&amp;quot;, &amp;quot;v&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;d&amp;quot;, &amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;l&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;o&amp;quot;, &amp;quot;g&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;c&amp;quot;, &amp;quot;a&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;g&amp;quot;, &amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;o&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;n&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;k&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;i&amp;quot;, &amp;quot;z&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;o&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;, &amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;, &amp;quot;n&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;n&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;a&amp;quot;, &amp;quot;p&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;, &amp;quot;g&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;o&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;g&amp;quot;, &amp;quot;e&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;o&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;b&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;o&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;t&amp;quot;, &amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;k&amp;quot;, &amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;, &amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;, &amp;quot;w&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;q&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;, &amp;quot;k&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;, &amp;quot;z&amp;quot;,&amp;quot;m&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;o&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;u&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;n&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;o&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;, &amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;, &amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;, &amp;quot;g&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;q&amp;quot;, &amp;quot;j&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;o&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;k&amp;quot;, &amp;quot;e&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;s&amp;quot;, &amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;p&amp;quot;, &amp;quot;d&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;, &amp;quot;k&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;v&amp;quot;, &amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;n&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;x&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;c&amp;quot;, &amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;w&amp;quot;, &amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;g&amp;quot;, &amp;quot;w&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;z&amp;quot;, &amp;quot;u&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;r&amp;quot;, &amp;quot;g&amp;quot;,&amp;quot;h&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;d&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;, &amp;quot;b&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;y&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;p&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;, &amp;quot;b&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;f&amp;quot;,&amp;quot;q&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;v&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;u&amp;quot;, &amp;quot;z&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;u&amp;quot;,&amp;quot;z&amp;quot;,&amp;quot;j&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;e&amp;quot;,&amp;quot;k&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;t&amp;quot;,&amp;quot;s&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;r&amp;quot;,&amp;quot;g&amp;quot;,&amp;quot;w&amp;quot;,&amp;quot;g&amp;quot;)  Se trata de una cadena de 1144 caracteres que, aparentemente, encerraban algún tipo de mensaje.</description>
    </item>
    
    <item>
      <title>Modelos exponenciales para grafos aleatorios (y III): inferencia</title>
      <link>/2012/05/18/modelos-exponenciales-para-grafos-aleatorios-y-iii-inferencia/</link>
      <pubDate>Fri, 18 May 2012 07:20:48 +0000</pubDate>
      
      <guid>/2012/05/18/modelos-exponenciales-para-grafos-aleatorios-y-iii-inferencia/</guid>
      <description>Me quedé el otro día en el modelo probabilístico de los grafos aleatorios exponenciales. Quedaba una última parte y al ensayar su redacción me di cuenta de que me había metido en un huerto: la cosa es mucho más vasta de lo que a primera vista parecía.
Así que me limitaré a repasar lo más básico tratando de no meter demasiado la pata.
Tradicionalmente, se utilizaba para estimar los parámetros de un grafo la llamada técnica de la función de seudo-verosimilitud.</description>
    </item>
    
    <item>
      <title>Modelos exponenciales para grafos aleatorios (II): modelo probabilístico</title>
      <link>/2012/05/10/modelos-exponenciales-para-grafos-aleatorios-ii-modelo-probabilistico/</link>
      <pubDate>Thu, 10 May 2012 06:46:30 +0000</pubDate>
      
      <guid>/2012/05/10/modelos-exponenciales-para-grafos-aleatorios-ii-modelo-probabilistico/</guid>
      <description>Ayer dejamos abierto el problema de la inferencia en grafos. La idea fundamental es la de suponer que un grafo determinado no es tanto un grafo en sí como una realización de un proceso aleatorio de generación de aristas entre un determinado número de nodos.
El planteamiento es análogo al que se hace con las series temporales: no es tan importante la serie en sí como el hecho de que pueda probarse que obedece a un modelo autorregresivo, ARIMA, etc.</description>
    </item>
    
    <item>
      <title>R y la distribución de Rayleigh</title>
      <link>/2012/03/23/r-y-la-distribucion-de-rayleigh/</link>
      <pubDate>Fri, 23 Mar 2012 07:26:24 +0000</pubDate>
      
      <guid>/2012/03/23/r-y-la-distribucion-de-rayleigh/</guid>
      <description>En la reunión de usuarios de R de Madrid de ayer, Carlos Ortega estudió la distribución en el tiempo del número de bugs que aparecen en el código de R en cada versión. Indicó que es plausible que sigan una distribución de Rayleigh, relativamente frecuente en ese tipo de contextos. E indicó que esta distribución, no tan conocida, tiene que ver (he olvidado lo que dijo exactamente) con dos normales independientes.</description>
    </item>
    
    <item>
      <title>La frontera bayesiana en problemas de clasificación (simples)</title>
      <link>/2012/02/01/la-frontera-bayesiana-en-problemas-de-clasificacion-simples/</link>
      <pubDate>Wed, 01 Feb 2012 00:32:20 +0000</pubDate>
      
      <guid>/2012/02/01/la-frontera-bayesiana-en-problemas-de-clasificacion-simples/</guid>
      <description>Una de las preguntas formuladas dentro del foro desde el que seguimos la lectura del libro The Elements of Statistsical Learning se refiere a cómo construir la frontera bayesiana óptima en ciertos problemas de clasificación.
Voy a plantear aquí una discusión así como código en R para representarla (en casos simples y bidimensionales).
Supongamos que hay que crear un clasificador que distinga entre puntos rojos y verdes con la siguiente pinta,</description>
    </item>
    
    <item>
      <title>Cosa prodigiosa (III): epílogo</title>
      <link>/2012/01/31/cosa-prodigiosa-iii-epilogo/</link>
      <pubDate>Tue, 31 Jan 2012 06:49:32 +0000</pubDate>
      
      <guid>/2012/01/31/cosa-prodigiosa-iii-epilogo/</guid>
      <description>Escribo desde mi retiro vacacional, en el hemisferio inhabitual, sin wifis y casi de memoria para completar la historia que comencé hace dos semanas en esta bitácora.
Tropecé con el juego que describí en el libro A Mathematician Plays The Stock Market, de John Allen Paulos. Y creo que se equivoca en las probabilidades de los juegos: si en lugar de las que indiqué en mi primera entrada utilizo las suyas, me da la impresión de que el tercer juego es perdedor.</description>
    </item>
    
    <item>
      <title>Hay (micro)vida más allá de la (micro)muerte</title>
      <link>/2012/01/30/hay-microvida-mas-alla-de-la-micromuerte/</link>
      <pubDate>Mon, 30 Jan 2012 07:32:32 +0000</pubDate>
      
      <guid>/2012/01/30/hay-microvida-mas-alla-de-la-micromuerte/</guid>
      <description>Hablamos ya hace un tiempo de las micromuertes. Ahora toca traer a la atención de mis lectores un concepto asociado, el de las microvidas.
Una microvida corresponde a una esperanza de vida de media hora. Malgasta una microvida quien fuma dos cigarros, bebe siete unidades de alcohol (equivalentes a un litro de cerveza) o vive un día con un sobrepeso de 5 kg.

Microvidas y micromuertes son conceptos análogos, pero no enteramente equivalentes.</description>
    </item>
    
    <item>
      <title>Cosa prodigiosa, ahora con palabras (II)</title>
      <link>/2012/01/19/cosa-prodigiosa-ahora-con-palabras-ii/</link>
      <pubDate>Thu, 19 Jan 2012 06:32:41 +0000</pubDate>
      
      <guid>/2012/01/19/cosa-prodigiosa-ahora-con-palabras-ii/</guid>
      <description>Tal como prometí hace ahora una semana, voy a añadir las palabras que faltaban en aquella entrada. Pero primero, imaginad un bar en el que se venden cafés y cervezas. El coste de servir un café es de 1.10 euros pero se vende por 1. El coste de servir una cerveza es 1.30 euros pero se vende por 1.10. Entran los clientes y piden o café o cerveza. ¡Y resulta que a fin de mes el bar hace dinero!</description>
    </item>
    
    <item>
      <title>Muestreando la distribución uniforme sobre la esfera unidad en n dimensiones</title>
      <link>/2012/01/17/muestreando-la-distribucion-uniforme-sobre-la-esfera-unidad-en-n-dimensiones/</link>
      <pubDate>Tue, 17 Jan 2012 06:56:36 +0000</pubDate>
      
      <guid>/2012/01/17/muestreando-la-distribucion-uniforme-sobre-la-esfera-unidad-en-n-dimensiones/</guid>
      <description>Debo esta entrada a la diligencia de Juanjo Gibaja, que se tomó la molestia de ubicar los teoremas relevantes en el libro Simulation and the Monte Carlo Method de Rubinstein y Kroese.
Esencialmente, como la distribución normal multivariante (con matriz de covarianzas I) es simétrica, entonces, dadas $latex X_1,\dots, X_m \sim N( 0, I_n )$ independientes, los m puntos del espacion n-dimensional $latex X_i/| X_i |$ siguen una distribución uniforme sobre su esfera (su superficie, vale la pena reiterar) unidad.</description>
    </item>
    
    <item>
      <title>Cosa prodigiosa, sin palabras (I)</title>
      <link>/2012/01/12/cosa-prodigiosa-sin-palabras-i/</link>
      <pubDate>Thu, 12 Jan 2012 07:14:49 +0000</pubDate>
      
      <guid>/2012/01/12/cosa-prodigiosa-sin-palabras-i/</guid>
      <description>Hoy voy a hacer mención a una cosa prodigiosa. Pero sin palabras. Voy a regalar a mis lectores tres pedazos de código que son este
jugar &amp;lt;- function( n, make.step ){ tmp &amp;lt;- rep( 0L, n) for( i in 2:n ) tmp[i] &amp;lt;- make.step( tmp[i-1] ) tmp } juego.s &amp;lt;- function( x, prob.perder = 0.51 ){ x + ifelse( runif(1) &amp;lt; prob.perder, -1L, 1L ) } res.juego.s &amp;lt;- replicate( 1000, jugar( 1000, juego.</description>
    </item>
    
    <item>
      <title>¿Curiosidades de la lotería?</title>
      <link>/2012/01/02/%c2%bfcuriosidades-de-la-loteria/</link>
      <pubDate>Mon, 02 Jan 2012 06:39:42 +0000</pubDate>
      
      <guid>/2012/01/02/%c2%bfcuriosidades-de-la-loteria/</guid>
      <description>Tenía guardado un enlace de un artículo del periódico sobre curiosidades de la lotería. Describe dos hechos curiosos:
 * Que la terminación más repetida, el 5, ha aparecido 32 ocasiones en 201 _gordos_ (se ve que ha habido 200 sorteos, pero un año hubo, cosas de la vida, dos _gordos_). * Que dos números, el 15640 y el 20297 han sido _gordos_ en dos ocasiones.  Una pregunta, pues, para mis lectores: ¿qué es más improbable, que la terminación más frecuente haya ocurrido en 32 (o más) ocasiones o que haya habido dos (o más) gordos repetidos?</description>
    </item>
    
    <item>
      <title>Un problema de probabilidad</title>
      <link>/2011/10/21/un-problema-de-probabilidad/</link>
      <pubDate>Fri, 21 Oct 2011 07:03:57 +0000</pubDate>
      
      <guid>/2011/10/21/un-problema-de-probabilidad/</guid>
      <description>Como es viernes, propongo un problema de probabilidad. Es el siguiente:
En un curso de inglés elemental hay 5 alumnos y 4 alumnas. En el intermedio, 7 y 3. En el avanzado, 4 y 4. Se promociona a un alumno (uso el masculino aquí genéricamente) del elemental a intermedio. Se elige luego a un alunmo (uso genérico del masculino, de nuevo) del intermedio y resulta ser un hombre. ¿Cuál es la probabilidad de que el alumno promocionado fuese también hombre?</description>
    </item>
    
    <item>
      <title>Puedes probar cualquier cosa (con paciencia)</title>
      <link>/2011/10/04/puedes-probar-cualquier-cosa-con-paciencia/</link>
      <pubDate>Tue, 04 Oct 2011 07:10:42 +0000</pubDate>
      
      <guid>/2011/10/04/puedes-probar-cualquier-cosa-con-paciencia/</guid>
      <description>Puedes _probar _prácticamente cualquier cosa. Con paciencia, claro. Por ejemplo, coge una moneda de tu bolsillo. Puedes probar que tiene un sesgo: salen más caras (o cruces, da igual) de lo que cabría esperar.
No lo vas a probar como los gañanes, no. Lo vas a probar usando los mismos métodos con los que se aprueban los medicamentos u otras verdades relevantísimas: mostrando al mundo un p-valor pequeñajo, por debajo de 0.</description>
    </item>
    
    <item>
      <title>Sobre la economía del lenguaje</title>
      <link>/2011/09/27/sobre-la-economia-del-lenguaje/</link>
      <pubDate>Tue, 27 Sep 2011 07:39:31 +0000</pubDate>
      
      <guid>/2011/09/27/sobre-la-economia-del-lenguaje/</guid>
      <description>De acuerdo con una observación de Zipf (y supongo que de muchos otros y que no hay que confundir con su ley), la longitud de las palabras más corrientes es menor que las que se usan menos frecuentemente.
Un estudio reciente, Word lengths are optimized for efficient communication, matiza esa observación: la cantidad de información contenida en una palabra predice mejor la longitud de las palabras que la frecuencia de aparición pura.</description>
    </item>
    
    <item>
      <title>Si Feller levantase la cabeza...</title>
      <link>/2011/08/22/si-feller-levantase-la-cabeza/</link>
      <pubDate>Mon, 22 Aug 2011 07:55:14 +0000</pubDate>
      
      <guid>/2011/08/22/si-feller-levantase-la-cabeza/</guid>
      <description>Tengo un amigo físico que trabaja supervisando el funcionamiento una máquina de radioterapia. Se dedica, esencialmente, a achicharrar células cancerígenas con chorros de radioactividad. Me contaba recientemente cómo hay pacientes que responden positivamente y cómo con otros con un perfil similar, aun sometidos a dosis de radioactividad muy superiores, no hay forma humana de hacer que el tumor remita. Éste y muchos otros casos análogos hacen pensar a la comunidad médica que no hay enfermedades sino enfermos y que los remedios que bien valen para uno, pueden no valer para otro.</description>
    </item>
    
    <item>
      <title>Comparación de variables aleatorias de Poisson</title>
      <link>/2011/08/21/comparacion-de-variables-aleatorias-de-poisson/</link>
      <pubDate>Sun, 21 Aug 2011 07:53:08 +0000</pubDate>
      
      <guid>/2011/08/21/comparacion-de-variables-aleatorias-de-poisson/</guid>
      <description>El otro día apareció publicado en Significance una comparación entre el número de tarjetas recibidas por las selecciones inglesas de fúlbol masculina y femenina.
Los hombres habían recibido 196 tarjetas en los 48 partidos disputados en el periodo de referencia y las mujeres, 40 en 24 partidos. El promedio de tarjetas, por lo tanto, de 4.1 y 1.7 respectivamente. Y la pregunta es: ¿hay motivos razonables para pensar que las mujeres juegan menos sucio?</description>
    </item>
    
    <item>
      <title>Paella sin arroz con sabor a judías enlatadas</title>
      <link>/2011/07/21/paella-sin-arroz-con-sabor-a-judias-enlatadas/</link>
      <pubDate>Thu, 21 Jul 2011 07:00:14 +0000</pubDate>
      
      <guid>/2011/07/21/paella-sin-arroz-con-sabor-a-judias-enlatadas/</guid>
      <description>El otro día leí el artículo A Prototype Model of Stock Exchangede G. Caldarelli, M. Marsili y Y.C. Zhang. La promesa que me ofrecía era la de la creación de un sistema relativamente realista de los agentes que operan en los mercados financieros que diese lugar a una evolución de precios con propiedades similares a las observadas.
Sin embargo, el planteamiento, interesante en un principio, se deshinchó enseguida:
 El modelo planteado por los autores ni siquiera aspira a representar los aspectos más distintivos del mercado: en lugar de agentes tremendamente desiguales en tamaño y entrelazados en una maraña de dependencias e influencias mutuas, los agentes son todos equivalentes en tamaño (si bien es cierto que en el estado estacionario de la simulación los ingresos adquieren una distribución dada por una ley de potencias) y que actúan de manera independiente entre sí una vez observados los precios en el mercado.</description>
    </item>
    
    <item>
      <title>Sobre el libro &#34;The flaw of averages&#34;</title>
      <link>/2011/06/24/sobre-el-libro-the-flaw-of-averages/</link>
      <pubDate>Fri, 24 Jun 2011 07:19:27 +0000</pubDate>
      
      <guid>/2011/06/24/sobre-el-libro-the-flaw-of-averages/</guid>
      <description>Leí hace un tiempo The flaw of averages, un libro poco convencional que recomiendo a mis lectores. Su objetivo último es encomiable: conseguir que personas sin mayor preparación matemática o estadística pero obligadas a tomar decisiones frente a la incertidumbre apliquen el sentido común y entiendan claramente unos principios mínimos.
Para lograrlo, asume una postura tal vez anti-intelectualista, tal vez herética. Piensa el autor —¿con motivo?— que, a ciertas personas, conceptos tales como varianza, media, teorema central del límite o función de densidad les dificultan, más que facilitan, la comprensión de lo que la incertidumbre realmente es y de cómo puede afectarlos.</description>
    </item>
    
    <item>
      <title>250 aniversario de la muerte de Bayes</title>
      <link>/2011/06/09/250-aniversario-de-la-muerte-de-bayes/</link>
      <pubDate>Thu, 09 Jun 2011 06:59:29 +0000</pubDate>
      
      <guid>/2011/06/09/250-aniversario-de-la-muerte-de-bayes/</guid>
      <description>Cumpliéndose el 250 aniversario de la muerte de Thomas Bayes (fue el 17 de abril, de hecho), como homenaje, publico hoy una foto del autor al lado de su tumba en el cementerio de Bunhill Fields, en Londres.

Nota: es la tumba blanca que aparece casi en el centro. La tomó mi viejo amigo Raúl Aguaviva un día que acabamos perdidos buscando el Museo Británico por un barrio que resultó estar no lejos de Angel.</description>
    </item>
    
    <item>
      <title>¿Qué nos jugamos? (Addenda: no queremos jugarnos nada)</title>
      <link>/2011/05/16/que-nos-jugamos-addenda-no-queremos-jugarnos-nada/</link>
      <pubDate>Mon, 16 May 2011 07:10:13 +0000</pubDate>
      
      <guid>/2011/05/16/que-nos-jugamos-addenda-no-queremos-jugarnos-nada/</guid>
      <description>Al tratar el principio de Kelly el otro día omití, craso error, decir que dicho criterio nos invita a no apostar en casi ninguna circunstancia. En efecto, siendo el tamaño de la apuesta —más propiamente, el porcentaje del capital que apostar— el que propone el citerio igual a
$$ x = \frac{bp-(1-p)}{b}, $$
cabe preguntarse cuándo es éste mayor que cero. Y lo es cuando
$$pb - (1-p) &amp;gt; 0, $$</description>
    </item>
    
    <item>
      <title>¿Qué nos jugamos?</title>
      <link>/2011/05/12/que-nos-jugamos/</link>
      <pubDate>Thu, 12 May 2011 07:18:12 +0000</pubDate>
      
      <guid>/2011/05/12/que-nos-jugamos/</guid>
      <description>Imagine que le proponen participar reiteradamente en un juego de azar. Dispone de una cantidad de dinero inicial, $latex a$ euros, y puede apostar en un juego en el que o gana con probabilidad $latex p$ $latex b$ veces la apuesta o la pierde enteramente. Puede repetir el juego cuantas veces quiera y apostar el porcentaje que desee de su dinero.
¿Cuánto se apostaría? ¿Qué porcentaje de su capital inicial se jugaría?</description>
    </item>
    
    <item>
      <title>Terrorismo y sesgos en la percepción de la improbabilidad</title>
      <link>/2011/05/10/terrorismo-sesgos-percepcion-improbabilidad/</link>
      <pubDate>Tue, 10 May 2011 07:02:01 +0000</pubDate>
      
      <guid>/2011/05/10/terrorismo-sesgos-percepcion-improbabilidad/</guid>
      <description>En el Financial Times del 3 de mayo aparece un artículo de Gideon Rachman que es de los pocos que merece ser leído sobre el fatigoso y como se verá poco relevante tema de la muerte de Bin Laden. Y es interesante —y relevante para los lectores de esta bitácora— porque toca un tema del que ya nos hemos ocupado y que seguro que revisitaremos: el de las probabilidades subjetivas y, en particular, el de las distorsiones con las que los seres humanos percibimos y calibramos probabilidades pequeñas.</description>
    </item>
    
    <item>
      <title>Incertidumbre, juicios y sesgos</title>
      <link>/2011/04/27/incertidumbre-juicios-y-sesgos/</link>
      <pubDate>Wed, 27 Apr 2011 07:25:57 +0000</pubDate>
      
      <guid>/2011/04/27/incertidumbre-juicios-y-sesgos/</guid>
      <description>Recomiendo encarecidamente la lectura del artículo Judgment under Uncertainty: Heuristics and Biases de D. Kahneman y A. Tversky. En pocas palabras, trata sobre dos cosas:
 los atajos mentales que utiliza el ser humano para asociar probabilidades subjetivas a eventos y, sobre todo, los sesgos y errores a los que conducen dichos atajos.  A través de una serie de experimentos, los autores revelan cómo individuos —incluso con una sólida formación cuantitativa— yerran sistemáticamente al enfrentarse con determinado tipo de problemas.</description>
    </item>
    
    <item>
      <title>Riesgo e incertidumbre</title>
      <link>/2011/03/11/riesgo-e-incertidumbre/</link>
      <pubDate>Fri, 11 Mar 2011 09:33:27 +0000</pubDate>
      
      <guid>/2011/03/11/riesgo-e-incertidumbre/</guid>
      <description>He encontrado dos (¿cuatro?) definiciones contradictorias de _riesgo _e incertidumbre. La primera está implícita en una frase del artículo The ratings game de Martin Mayer y dice, según mi traducción, así:
 Knight realizó una distinción categórica entre el riesgo, que puede ser medido, y la incertidumbre, que no puede serlo.
 Mayer recoge así la distinción que realizó Frank Knight en su tesis doctoral hace ya casi un siglo. En términos algo más precisos, lo que según Knight distingue la incertidumbre del riesgo es que del segundo se conoce, cuando menos, la distribución de probabilidad asociada al fenómeno.</description>
    </item>
    
    <item>
      <title>Centenario de la muerte de Galton</title>
      <link>/2011/02/23/centenario-de-la-muerte-de-galton/</link>
      <pubDate>Wed, 23 Feb 2011 08:56:47 +0000</pubDate>
      
      <guid>/2011/02/23/centenario-de-la-muerte-de-galton/</guid>
      <description>Al cumplirse cien años de la muerte de Francis Galton (1822-1911), mostraré una animación relacionada con una de sus más curiosas invenciones, el quincunx o quincuncio:
  El interesado puede también descargar el código de R utilizado para generar la animación.</description>
    </item>
    
    <item>
      <title>La ley de los grandes números y el teorema central del límite en dos animaciones</title>
      <link>/2011/01/26/la-ley-de-los-grandes-numeros-y-el-teorema-central-del-limite-en-dos-animaciones/</link>
      <pubDate>Wed, 26 Jan 2011 09:21:40 +0000</pubDate>
      
      <guid>/2011/01/26/la-ley-de-los-grandes-numeros-y-el-teorema-central-del-limite-en-dos-animaciones/</guid>
      <description>No las voy a reproducir aquí por si se enfada el autor. Me limitaré a mostrar una captura de la animación correspondiente a la ley de los grandes números,

y a la del teorema central del límite,

La animación completa (hecha con R) y los detalles, en este enlace.</description>
    </item>
    
    <item>
      <title>Dos mapas de distribuciones de probabilidad</title>
      <link>/2011/01/19/dos-mapas-de-distribuciones-de-probabilidad/</link>
      <pubDate>Wed, 19 Jan 2011 11:09:07 +0000</pubDate>
      
      <guid>/2011/01/19/dos-mapas-de-distribuciones-de-probabilidad/</guid>
      <description>Si hace unos días enlazamos desde estas páginas a un cuadro sinóptico que contextualizaba los algoritmos de minería de datos, hoy aprovechamos para divulgar la noticia de otra que relaciona las distribuciones de probabilidad más habituales y sus interdependencias.
El lector es libre de plantearse (e incluso responderse) estas preguntas: partiendo de la distribución normal que ocupa el centro del diagrama,
 ¿qué propiedades distinguen a las que quedan por encima de ella de las que quedan por debajo?</description>
    </item>
    
    <item>
      <title>¿Por qué la otra cola siempre se mueve más aprisa?</title>
      <link>/2011/01/17/por-que-la-otra-cola-siempre-se-mueve-mas-aprisa/</link>
      <pubDate>Mon, 17 Jan 2011 09:27:21 +0000</pubDate>
      
      <guid>/2011/01/17/por-que-la-otra-cola-siempre-se-mueve-mas-aprisa/</guid>
      <description>Me pasó mi buen amigo Raúl (que tiene un blog más friqui que el mío) un vídeo con un título tan promisorio y mendaz como el de esta entrada:
  Se ve con gusto, pero no cuenta toda la historia. Al menos, uno espera averiguar por qué todas las demás colas parecen moverse más aprisa que la de uno, pero, realmente, sólo explica por qué es poco probable que la propia sea la más rápida.</description>
    </item>
    
    <item>
      <title>¿Una caída demasiado drástica de la varianza?</title>
      <link>/2011/01/10/una-caida-demasiado-drastica-de-la-varianza/</link>
      <pubDate>Mon, 10 Jan 2011 09:39:18 +0000</pubDate>
      
      <guid>/2011/01/10/una-caida-demasiado-drastica-de-la-varianza/</guid>
      <description>El otro día me pidieron modelar (estadísticamente, no con plastilina) nosequé fenómeno. Digo nosequé porque me lo describieron alegóricamente. No sé si la respuesta que di redundará en beneficio o perjuicio de la humanidad. Pero no quiero hablar de eso sino del problema en sí y de unas cuestiones sobre la varianza asintótica a las que me referiré después. El problema se resume en:
 De una población se conocía una proporción p aunque con cierto grado de incertidumbre más o menos cuantificable.</description>
    </item>
    
    <item>
      <title>La Wikipedia te necesita</title>
      <link>/2010/11/15/la-wikipedia-te-necesita/</link>
      <pubDate>Mon, 15 Nov 2010 10:03:13 +0000</pubDate>
      
      <guid>/2010/11/15/la-wikipedia-te-necesita/</guid>
      <description>Hoy, procrastinando, me he dado un paseo por la Wikipedia en español. Y me he deprimido viendo el lamentable estado en que se encuentran la mayor parte de las páginas de las categorías a las que concierne esta bitácora como, por ejemplo, las de
 probabilidad, estadística y minería de datos.  Quiero invitar a los lectores de este blog (a los que, por serlo, se les presupone un mínimo de interés y formación) a que participen en ese proyecto común que es la Wikipedia (y, en particular, la Wikipedia en español) para no tener que volver a sonrojarnos al comparar nuestras páginas con las correspondientes de otros idiomas.</description>
    </item>
    
    <item>
      <title>Abundando en lo de nuestra ineptitud para estimar la probabilidad condicionada</title>
      <link>/2010/11/12/abundando-en-lo-de-nuestra-ineptitud-para-estimar-la-probabilidad-condicionada/</link>
      <pubDate>Fri, 12 Nov 2010 09:38:03 +0000</pubDate>
      
      <guid>/2010/11/12/abundando-en-lo-de-nuestra-ineptitud-para-estimar-la-probabilidad-condicionada/</guid>
      <description>Antes de seguir leyendo, trate de responder a la siguiente pregunta:
 Una familia tiene dos hijos (acá usamos el masculino en forma genérica: pudieran ser de cualquier sexo). Uno de ellos es niño. ¿Cuál es la probabilidad de que el otro sea también niño?
 Si su respuesta es 0.5 va a tener que seguir leyendo el resto del artículo. Pero tampoco se deprima: parece que nuestro cerebro está maleado para caer en tal error y así lo parece refrendar una microencuesta que elaboro interpelando a incautos.</description>
    </item>
    
    <item>
      <title>Un vídeo algo viejo sobre estadísticas, expectativas y mentiras</title>
      <link>/2010/03/06/un-video-algo-viejo-sobre-estadisticas-expectativas-y-mentiras/</link>
      <pubDate>Sat, 06 Mar 2010 16:38:53 +0000</pubDate>
      
      <guid>/2010/03/06/un-video-algo-viejo-sobre-estadisticas-expectativas-y-mentiras/</guid>
      <description>Hace poco me hicieron llegar el vídeo de una conferencia pronunciada en el seno delTED.
TED es una ONG dedicada a la difusión de ideas&amp;hellip; que merecen difusión. Organiza conferencias anuales en California y Oxford y, según su página, reta a los pensadores más fascinantes a dar la charla de sus vidas en 18 minutos. No ha de sorprender pues que también haya acogido Hans Rosling para que vuelva a sorprendernos con la potencia de su afamado GapMinder.</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gbm on datanalytics</title>
    <link>/tags/gbm/</link>
    <description>Recent content in gbm on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Fri, 24 Jun 2016 08:13:26 +0000</lastBuildDate><atom:link href="/tags/gbm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GBM (II): Más allá de las pérdidas cuadráticas</title>
      <link>/2016/06/24/gbm-ii-mas-alla-de-las-perdidas-cuadraticas/</link>
      <pubDate>Fri, 24 Jun 2016 08:13:26 +0000</pubDate>
      
      <guid>/2016/06/24/gbm-ii-mas-alla-de-las-perdidas-cuadraticas/</guid>
      <description>Liberados del estrecho ámbito de nuestra original mentira sugerente gracias a la relación que descubrimos entre residuos y gradientes cuando las pérdidas son cuadráticas podemos adentrarnos en ámbitos más extensos.
Lo que discutimos del gradiente tiene una interpretación fácilmente inteligible en el caso de pérdidas cuadráticas. Pero ni la pérdida de interpretabilidad nos impide extender el razonamiento de la entrada anterior a funciones de pérdida distintas de la cuadrática siempre que podamos calcular un gradiente.</description>
    </item>
    
    <item>
      <title>GBM (II): Minización de funciones, pérdidas cuadráticas, residuos y gradientes</title>
      <link>/2016/06/22/gbm-ii-minizacion-de-funciones-perdidas-cuadraticas-residuos-y-gradientes/</link>
      <pubDate>Wed, 22 Jun 2016 08:13:18 +0000</pubDate>
      
      <guid>/2016/06/22/gbm-ii-minizacion-de-funciones-perdidas-cuadraticas-residuos-y-gradientes/</guid>
      <description>Para minimizar una función $latex \phi(x)$ es habitual utilizar un procedimiento iterativo: a partir de un punto inicial $latex x_0$ se salta a $latex x_1 = x_0 - \lambda \nabla \phi(x_0)$ (donde $latex \lambda$ es un número pequeño predefinido de antemano), y de ahí, sucesivamente, a
$latex x_n = x_{n-1} - \lambda \nabla \phi(x_{n-1}).$
Porque, típicamente, como cuando uno está en el monte y da un paso corto en la dirección opuesta a la de máxima pendiente, sucede que</description>
    </item>
    
    <item>
      <title>GBM (I): Una mentira sugerente</title>
      <link>/2016/06/21/gbm-i-una-mentira-sugerente/</link>
      <pubDate>Tue, 21 Jun 2016 08:13:28 +0000</pubDate>
      
      <guid>/2016/06/21/gbm-i-una-mentira-sugerente/</guid>
      <description>Hace un tiempo resumí los GBMs (Gradient Boosting Machines) en una línea. Hoy comienzo una serie de varias entradas para que nadie tenga excusa de no saber de qué va la cosa. Arranco con una mentira sugerente. Porque lo que voy a contar no es del todo cierto, pero motiva lo que vendrá después.
Consideremos un conjunto de datos medio famoso: el de los precios de los alquileres en Múchich. Comencemos con un modelo sencillo, una regresión lineal que relacione el precio del alquiler con los metros cuadrados, i.</description>
    </item>
    
    <item>
      <title>GBM sintetizado en una línea</title>
      <link>/2016/03/11/gbm-sintetizado-en-una-linea/</link>
      <pubDate>Fri, 11 Mar 2016 09:13:53 +0000</pubDate>
      
      <guid>/2016/03/11/gbm-sintetizado-en-una-linea/</guid>
      <description>Es
$latex \sum_i \Phi(y_i, f_1(x_i)) &amp;gt; \sum_i \Phi(y_i, f_1(x_i) - \lambda \nabla \Phi(y_i, f_1(x_i)) \sim \sum_i \Phi(y_i, f_1(x_i) - \lambda f_2(x_i))$
Por supuesto, el lector se preguntará muchas cosas, entre las que destaco:
 * ¿Qué representa cada uno de los elementos que aparecen en la línea anterior? * ¿Qué parte de ella es solo _casi siempre_ cierta? * ¿Qué tiene todo eso que ver con [GBM](https://github.com/harrysouthworth/gbm/blob/master/inst/doc/gbm.pdf)?  </description>
    </item>
    
    <item>
      <title>Experimentos con el paquete gbm</title>
      <link>/2014/02/06/experimentos-con-el-paquete-gbm/</link>
      <pubDate>Thu, 06 Feb 2014 08:07:56 +0000</pubDate>
      
      <guid>/2014/02/06/experimentos-con-el-paquete-gbm/</guid>
      <description>No conocía el paquete gbm. Pero como ahora ando rodeado de data scientists que no son estadísticos&amp;hellip;
Bueno, la cuestión es que había que ajustar un modelo para el que yo habría hecho algo parecido a
dat &amp;lt;- &amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/utils/read.csv&amp;quot;&amp;gt;read.csv(&amp;quot;http://www.ats.ucla.edu/stat/data/poisson_sim.csv&amp;quot;) summary(m.glm &amp;lt;- &amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/stats/glm&amp;quot;&amp;gt;glm(num_awards ~ prog + math, &amp;lt;a href=&amp;quot;http://inside-r.org/r-doc/stats/family&amp;quot;&amp;gt;family = &amp;quot;poisson&amp;quot;, data = dat)) # Call: # glm(formula = num_awards ~ prog + math, family = &amp;quot;poisson&amp;quot;, data = dat) # # Deviance Residuals: # Min 1Q Median 3Q Max # -2.</description>
    </item>
    
  </channel>
</rss>

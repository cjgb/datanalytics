<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nlp on datanalytics</title>
    <link>/tags/nlp/</link>
    <description>Recent content in nlp on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Mon, 08 Apr 2019 09:13:10 +0000</lastBuildDate><atom:link href="/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Demasiada gente conozco que todavía no sabe de GPT-2</title>
      <link>/2019/04/08/demasiada-gente-conozco-que-todavia-no-sabe-de-gpt-2/</link>
      <pubDate>Mon, 08 Apr 2019 09:13:10 +0000</pubDate>
      
      <guid>/2019/04/08/demasiada-gente-conozco-que-todavia-no-sabe-de-gpt-2/</guid>
      <description>Así que si eres uno de ellos, lee esto. Todo. Completo. Incluidos los motivos por los que no se va a liberar tal cual.
Si te quedas con ganas de más, lee esto (un divertimento) o, más en serio, esto otro, donde se da cuenta de uno de los logros de GPT-2 que, a primera vista, pasa desapercibido: que ha logrado adquirir determinadas habilidades sin haber sido entrenado específicamente para ello.</description>
    </item>
    
    <item>
      <title>Análisis (clasificación, etc.) de textos muy cortos</title>
      <link>/2019/03/22/analisis-clasificacion-etc-de-textos-muy-cortos/</link>
      <pubDate>Fri, 22 Mar 2019 08:13:37 +0000</pubDate>
      
      <guid>/2019/03/22/analisis-clasificacion-etc-de-textos-muy-cortos/</guid>
      <description>Uno de mis proyectos permanentemente pospuestos es el del análisis de textos muy cortos. Se citarán Twitter y similares, aunque el € está en otros sitios, como los mensajes asociados a transferencias bancarias, reseñas o keywords.
Pero parece que no soy el único interesado en el tema. Otros con más tiempo y talento han desarrollado [BTM](https://cran.r-project.org/web/packages/BTM/index.html), que parece ser una versión modificada de LDA para el análisis de textos cortos.</description>
    </item>
    
    <item>
      <title>Mariposa</title>
      <link>/2019/03/20/mariposa/</link>
      <pubDate>Wed, 20 Mar 2019 08:13:30 +0000</pubDate>
      
      <guid>/2019/03/20/mariposa/</guid>
      <description>Quieres saber dónde está el escorpión,
Ni ayer ni antes vos sos corona dorada.
Ya os ves más tal cual tortuga pintada,
A él nos gusta andar con cola marrón.
Ella es quién son las alas de algún gorrión.
Si al fin podés ver tu imagen manchada,
O hoy vas bajo un cielo azul plateada,
Por qué estás tan lejos del aguijón.
No hay luz que al sol se enreda en tus palmera.</description>
    </item>
    
    <item>
      <title>Entre lo fofo y lo hierático,modelos loglineales</title>
      <link>/2019/02/28/9884/</link>
      <pubDate>Thu, 28 Feb 2019 08:13:00 +0000</pubDate>
      
      <guid>/2019/02/28/9884/</guid>
      <description>El contexto, por fijar ideas, el problema de taguear fechas en textos.
La estrategia gomosa, fofa (ñof, ñof, ñof), y en la que parecen parecer creer algunos, embeddings más TensorFlow.
La estrategia hierática, inflexible y reminiscente de robots de pelis de serie B, expresiones regulares encadenadas con ORs.
En la mitad donde mora la virtud, extracción de features (principalmente con expresiones regulares) y luego, esto.
Nota: esta entrada es un recordatorio para mí mismo y por si retorna cierto asunto que dejé postergado hace un par de días.</description>
    </item>
    
    <item>
      <title>Dos ejercicios (propuestos) sobre &#34;embeddings&#34;</title>
      <link>/2018/10/15/dos-ejercicios-propuestos-sobre-embeddings/</link>
      <pubDate>Mon, 15 Oct 2018 08:13:11 +0000</pubDate>
      
      <guid>/2018/10/15/dos-ejercicios-propuestos-sobre-embeddings/</guid>
      <description>Se me han ocurrido en los dos últimos días un par de ejercicios sobre embeddings que no voy a hacer. Pero tal vez alguien con una agenda más despejada que la mía se anime. Uno es más bien tonto; el otro es más serio.
El primero consiste en tomar las provincias, los códigos postales o las secciones censales y crear textos que sean, para cada una de ellas, las colindantes. Luego, construir un embedding de dimensión 2.</description>
    </item>
    
    <item>
      <title>¿De qué matriz son los &#34;embeddings&#34; una factorización?</title>
      <link>/2018/10/03/de-que-matriz-son-los-embeddings-una-factorizacion/</link>
      <pubDate>Wed, 03 Oct 2018 08:13:49 +0000</pubDate>
      
      <guid>/2018/10/03/de-que-matriz-son-los-embeddings-una-factorizacion/</guid>
      <description>Hoy, embeddings. Esto va de reducir la dimensionalidad de un espacio generado por palabras (procedentes de textos). Si a cada palabra le asignamos un vector índice (todo ceros y un uno donde le corresponde), la dimensión del espacio de palabras es excesiva.
La ocurrencia de algunos es asociar a cada palabra, $latex W_i$, un vector $latex w_i$ corto (p.e., 100) con entradas $latex w_{ij}$ a determinar de la manera que se explica a continuación.</description>
    </item>
    
    <item>
      <title>Preludio (de más por venir)</title>
      <link>/2018/01/05/preludio-de-mas-por-venir/</link>
      <pubDate>Fri, 05 Jan 2018 08:13:29 +0000</pubDate>
      
      <guid>/2018/01/05/preludio-de-mas-por-venir/</guid>
      <description>El preludio esto:
Que tiene el interés y la interpretación (muchas de ellas, como se podrá barruntar más abajo, de corte técnico) que cada uno quiera darle.
La cuestión es que he ocerreado todas las portadas de El País y puedo buscar en el texto (adviértase la cursiva) resultante. Creo contar con una voluntaria para construir una aplicación web similar a la de los n-gramas de Google.
Igual subo los datos a algún sitio en algún momento.</description>
    </item>
    
    <item>
      <title>¿Mis conciudadanos no tienen wifi?</title>
      <link>/2016/05/30/mis-conciudadanos-no-tienen-wifi/</link>
      <pubDate>Mon, 30 May 2016 08:13:01 +0000</pubDate>
      
      <guid>/2016/05/30/mis-conciudadanos-no-tienen-wifi/</guid>
      <description>A alguien leí el otro día que decía que en un bar de carretera habían colocado un cartel diciendo: &amp;ldquo;Hemos quitado el periódico y hemos puesto wifi&amp;rdquo;. Viene esto a cuento de
library(rvest) library(&amp;lt;a href=&amp;quot;http://inside-r.org/packages/cran/tm&amp;quot;&amp;gt;tm) library(wordcloud) res &amp;lt;- sapply(1:17, function(i){ url &amp;lt;- paste(&amp;quot;https://decide.madrid.es/participatory_budget/investment_projects?geozone=all&amp;amp;page=&amp;quot;, i, &amp;quot;&amp;amp;random_seed=0.28&amp;quot;, sep = &amp;quot;&amp;quot;) tmp &amp;lt;- html_nodes(read_html(url), xpath = &amp;quot;//div[starts-with(@id, &#39;spending_proposal&#39;)]/div/div/div[1]/div/h3/a/text()&amp;quot;) as.character(tmp) }) tmp &amp;lt;- unlist(res) tmp &amp;lt;- Corpus(VectorSource(tmp)) tmp &amp;lt;- tm_map(tmp, stripWhitespace) tmp &amp;lt;- tm_map(tmp, content_transformer(tolower)) tmp &amp;lt;- tm_map(tmp, removeWords, stopwords(&amp;quot;spanish&amp;quot;)) wordcloud(tmp, scale=c(5,0.</description>
    </item>
    
    <item>
      <title>Un corpus de textos en español para NLP</title>
      <link>/2016/05/06/un-corpus-de-textos-en-espanol-para-nlp/</link>
      <pubDate>Fri, 06 May 2016 08:13:49 +0000</pubDate>
      
      <guid>/2016/05/06/un-corpus-de-textos-en-espanol-para-nlp/</guid>
      <description>Mañana doy clase de NLP en el máster de ciencia de datos de KSchool. Para lo que necesito un corpus decente. Los hay en inglés a tutiplén, pero las hordas de lingüistas hispanoparlantes que se pagan los vicios a costa de tajadas de mi IRPF han sido incapaces de colgar ninguno en español que pueda ubicar y reutilizar.
Necesito una colección de textos en español con ciertas características:
 * Tener un cierto tamaño (¿unas cuantas centenas de ellos?</description>
    </item>
    
    <item>
      <title>90 millones de euros en tecnologías del lenguaje</title>
      <link>/2016/04/29/90-millones-de-euros-en-tecnologias-del-lenguaje/</link>
      <pubDate>Fri, 29 Apr 2016 08:13:31 +0000</pubDate>
      
      <guid>/2016/04/29/90-millones-de-euros-en-tecnologias-del-lenguaje/</guid>
      <description>El gobierno español ha anunciado (ya hace un tiempo) un plan de impulso a las tecnologías del lenguaje con una dotación de 90 millones de euros (lo que costó el fichaje de Ronaldo).
Veremos en unos años qué ha dado de sí la cosa. En particular, si habrá permitido que los usuarios de R dispongamos de herramientas libres (porque de momento, ya están cobrándonoslas vía Agencia Tributaria) para hacer nuestros cacharreos.</description>
    </item>
    
    <item>
      <title>Charla: un lematizador probabilístico con R</title>
      <link>/2013/05/13/charla-un-lematizador-probabilistico-con-r/</link>
      <pubDate>Mon, 13 May 2013 07:18:08 +0000</pubDate>
      
      <guid>/2013/05/13/charla-un-lematizador-probabilistico-con-r/</guid>
      <description>El jueves 16 de mayo hablaré en el Grupo de Interés Local de Madrid de R sobre lematizadores probabilísticos.
Hablaré sobre el proceso de lematizacion y trataré de mostrar su importancia dentro del mundo del llamado procesamiento del lenguaje natural (NLP). La lematización es un proceso humilde dentro del NLP del que apenas nadie habla: su ejercicio solo ha hecho famoso a Martin Porter. Lo eclipsan otras aplicaciones más vistosas, como el siempre sobrevalorado análisis del sentimiento.</description>
    </item>
    
    <item>
      <title>Un lematizador para el español con R (II)</title>
      <link>/2012/01/05/un-lematizador-para-el-espanol-con-r-ii/</link>
      <pubDate>Thu, 05 Jan 2012 06:42:00 +0000</pubDate>
      
      <guid>/2012/01/05/un-lematizador-para-el-espanol-con-r-ii/</guid>
      <description>El otro día publiqué mi pequeño lematizador para el español con R. Era el subproducto de un antiguo proyecto mío de cuyos resultados daré noticia en los próximos días.
Pero veo con infinita satisfacción que Emilio Torres, viejo conocido de quienes, por ejemplo, hayáis asistido a las II o III Jornadas de Usuarios de R, ha estado abundando en el asunto y, ciertamente mejorándolo (cosa que, todo hay que decir, tiene escaso mérito): basta mirar los sus comentarios a la entrada original.</description>
    </item>
    
    <item>
      <title>Un lematizador para el español con R... ¿cutre? ¿mejorable?</title>
      <link>/2011/12/13/un-lematizador-para-el-espanol-con-r-cutre-mejorable/</link>
      <pubDate>Tue, 13 Dec 2011 07:23:56 +0000</pubDate>
      
      <guid>/2011/12/13/un-lematizador-para-el-espanol-con-r-cutre-mejorable/</guid>
      <description>Uno de los pasos previos para realizar lo que se viene llamando minería de texto es lematizar el texto. Desafortunadamente, no existen buenos lematizadores en español. Al menos, buenos lematizadores libres.
Existen el llamado algoritmo de porter y snowball pero, o son demasiado crudos o están más pensados para un lenguaje con muchas menos variantes morfológicas que el español.
Sinceramente, no sé a qué se dedican —me consta que los hay— los lingüistas computacionales de la hispanidad entera: ¿no son capaces de liberar una herramienta de lematización medianamente decente que podamos usar los demás?</description>
    </item>
    
    <item>
      <title>Predicciones a toro pasado y el perro que no ladró</title>
      <link>/2011/09/29/predicciones-a-toro-pasado-y-el-perro-que-no-ladro/</link>
      <pubDate>Thu, 29 Sep 2011 06:48:17 +0000</pubDate>
      
      <guid>/2011/09/29/predicciones-a-toro-pasado-y-el-perro-que-no-ladro/</guid>
      <description>Es fácil predecir a toro pasado. Casi tan fácil que asestarle una gran lanzada al moro muerto (el refranero es así de incorrecto políticamente, lo siento).
Esas son las ideas que me sugirieron fundamentalmente la lectura del un tanto hagiográfico Superordenadores para &amp;lsquo;predecir&amp;rsquo; revoluciones y del artículo al que se refería, Culturomics 2.0: Forecasting large-scale human behavior using news media tone in time and space.
El artículo nos explica cómo utilizando resúmenes de noticias de diversas fuentes era posible haber predicho las revoluciones de Egipto, Túnez y Libia.</description>
    </item>
    
    <item>
      <title>Sobre la economía del lenguaje</title>
      <link>/2011/09/27/sobre-la-economia-del-lenguaje/</link>
      <pubDate>Tue, 27 Sep 2011 07:39:31 +0000</pubDate>
      
      <guid>/2011/09/27/sobre-la-economia-del-lenguaje/</guid>
      <description>De acuerdo con una observación de Zipf (y supongo que de muchos otros y que no hay que confundir con su ley), la longitud de las palabras más corrientes es menor que las que se usan menos frecuentemente.
Un estudio reciente, Word lengths are optimized for efficient communication, matiza esa observación: la cantidad de información contenida en una palabra predice mejor la longitud de las palabras que la frecuencia de aparición pura.</description>
    </item>
    
    <item>
      <title>Hitler era comunista y judío</title>
      <link>/2011/05/09/hitler-era-comunista-y-judio/</link>
      <pubDate>Mon, 09 May 2011 07:07:57 +0000</pubDate>
      
      <guid>/2011/05/09/hitler-era-comunista-y-judio/</guid>
      <description>O así nos cuenta Google. Y me explico rápidamente para que no me demande nadie.
Uno de los servicios de Google con los que he topado recientemente es Google Squared, un buscador muy particular —y que parece funcionar sólo en inglés— que devuelve tablas: uno puede buscar nikon lenses, o statistical software y obtendrá lo que verá al pinchar en los correspondientes enlaces: tablas en las que las filas corresponden a lentes de Nikon o paquetes estadísticos y las columnas a atributos.</description>
    </item>
    
    <item>
      <title>¿Dónde están las antípodas de Montevideo?</title>
      <link>/2011/02/12/donde-estan-las-antipodas-de-montevideo/</link>
      <pubDate>Sat, 12 Feb 2011 11:12:33 +0000</pubDate>
      
      <guid>/2011/02/12/donde-estan-las-antipodas-de-montevideo/</guid>
      <description>No sé si te lo habrás preguntado alguna vez. Ni siquiera lo sabe Google.
Sin embargo, me admira esto.

¿Usáis Wolfram Alpha? ¿Qué os parece?</description>
    </item>
    
    <item>
      <title>Nuevo paquete para procesar texto en R: stringr</title>
      <link>/2011/01/20/nuevo-paquete-para-procesar-texto-en-r-stringr/</link>
      <pubDate>Thu, 20 Jan 2011 09:56:47 +0000</pubDate>
      
      <guid>/2011/01/20/nuevo-paquete-para-procesar-texto-en-r-stringr/</guid>
      <description>Hadley Wickman, el autor de plyr, reshape y ggplot2, ha vuelto a la carga en su exitoso empeño por hacernos cambiar de forma de programar en R.
Con su nuevo paquete, stringr, aspira a facilitarnos aún más la vida. En un reciente artículo, enumera sus ventajas:
 Procesa factores y caracteres de la misma manera (de verdad, muy práctico) Da a las funciones nombres y argumentos consistentes Simplifica las operaciones de procesamiento de cadenas eliminando opciones que apenas se usan Produce salidas que pueden ser utilizadas fácilmente como entradas a otras funciones Incorpora funciones para procesar texto presentes en otros lenguajes pero no en R  </description>
    </item>
    
    <item>
      <title>Cambios cosméticos en el blog</title>
      <link>/2010/10/03/cambios-cosmeticos-en-el-blog/</link>
      <pubDate>Sun, 03 Oct 2010 18:14:18 +0000</pubDate>
      
      <guid>/2010/10/03/cambios-cosmeticos-en-el-blog/</guid>
      <description>Acabo de realizar unos cuantos cambios, mayormente cosméticos, en mi blog. He añadido una lista de artículos recomendados al final de cada entrada, he eliminado el enlace a la entrada aleatoria, he incluido una lista de los últimos comentarios y, finalmente, he incluido propaganda contextual de Google.
Lo he hecho por dos motivos. El primero es pecuniario, obviamente. No espero que me retire ni que me permita dejar de tener que madrugar.</description>
    </item>
    
    <item>
      <title>Leyendo en diagonal (pero con cuidado)</title>
      <link>/2010/08/16/leyendo-en-diagonal-pero-con-cuidado/</link>
      <pubDate>Mon, 16 Aug 2010 04:20:30 +0000</pubDate>
      
      <guid>/2010/08/16/leyendo-en-diagonal-pero-con-cuidado/</guid>
      <description>Un profesor mío de historia en primero de BUP nos confesó un día que para corregir exámenes leía en diagonal: pasaba la vista de la esquina superior izquierda de la hoja a la inferior derecha y según las palabras que entendía por el camino ponía una nota u otra.
Justo o no el procedimiento, es cierto que de un mero golpe de vista sobre un texto se pueden adivinar muchas cosas sobre su contenido.</description>
    </item>
    
  </channel>
</rss>

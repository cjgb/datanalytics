<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ridge on datanalytics</title>
    <link>/tags/ridge/</link>
    <description>Recent content in ridge on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Mon, 02 Dec 2019 09:13:00 +0000</lastBuildDate><atom:link href="/tags/ridge/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sobre los coeficientes de los GLM en Scikit-learn</title>
      <link>/2019/12/02/sobre-los-coeficientes-de-los-glm-en-scikit-learn/</link>
      <pubDate>Mon, 02 Dec 2019 09:13:00 +0000</pubDate>
      
      <guid>/2019/12/02/sobre-los-coeficientes-de-los-glm-en-scikit-learn/</guid>
      <description>Pensé que ya había escrito sobre el asunto porque tropecé con él en un proyecto hace un tiempo. Pero mi menoria se había confundido con otra entrada, Sobre la peculiarisima implementacion del modelo lineal en (pseudo-)Scikit-learn, donde se discute, precisamente, un problema similar si se lo mira de cierta manera o diametralmente opuesto si se ve con otra perspectiva.
Allí el problema era que Scikit-learn gestionaba muy sui generis el insidioso problema de la colinealidad.</description>
    </item>
    
    <item>
      <title>Un resultado contraintuitivo</title>
      <link>/2019/04/10/un-resultado-contraintuitivo/</link>
      <pubDate>Wed, 10 Apr 2019 09:13:23 +0000</pubDate>
      
      <guid>/2019/04/10/un-resultado-contraintuitivo/</guid>
      <description>[Esta entrada recoge la pregunta y la duda que motivó una conversación con Javier Nogales en Twitter hace unos días.]
Citaba (él) un resultado de Theobald de 1974 (¿tanto lleva ridge entre nosotros? ¡habría jurado que menos!) que viene a decir que siempre existe un peso $latex \lambda$ para el que ridge es mejor que OLS.
Ves el álgebra y piensas: verdad será.
Pero te fías de tu propia intuición y piensas: ¡vaya un resultado contraintuitivo si no contradictorio!</description>
    </item>
    
    <item>
      <title>Mezclas y regularización</title>
      <link>/2019/03/13/mezclas-y-regularizacion/</link>
      <pubDate>Wed, 13 Mar 2019 08:13:31 +0000</pubDate>
      
      <guid>/2019/03/13/mezclas-y-regularizacion/</guid>
      <description>Cuando mezclas agua y tierra obtienes barro, una sustancia que comparte propiedades de sus ingredientes. Eso lo tenía muy claro de pequeño. Lo que en esa época me sorprendió mucho es que el agua fuese una mezcla de oxígeno e hidrógeno: ¡era muy distinta de sus componentes!
Porque no era una mezcla, obviamente. Era una combinación. En una combinación emergen propiedades inesperadas. Las mezclas, sin embargo, son más previsibles.
Pensaba en esto mientras escribía sobre la regularización de modelos (ridge, lasso y todas esas cosas).</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nlp on datanalytics</title>
    <link>/categories/nlp/</link>
    <description>Recent content in nlp on datanalytics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <lastBuildDate>Fri, 22 Mar 2019 08:13:37 +0000</lastBuildDate><atom:link href="/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Análisis (clasificación, etc.) de textos muy cortos</title>
      <link>/2019/03/22/analisis-clasificacion-etc-de-textos-muy-cortos/</link>
      <pubDate>Fri, 22 Mar 2019 08:13:37 +0000</pubDate>
      
      <guid>/2019/03/22/analisis-clasificacion-etc-de-textos-muy-cortos/</guid>
      <description>Uno de mis proyectos permanentemente pospuestos es el del análisis de textos muy cortos. Se citarán Twitter y similares, aunque el € está en otros sitios, como los mensajes asociados a transferencias bancarias, reseñas o keywords.
Pero parece que no soy el único interesado en el tema. Otros con más tiempo y talento han desarrollado [BTM](https://cran.r-project.org/web/packages/BTM/index.html), que parece ser una versión modificada de LDA para el análisis de textos cortos.</description>
    </item>
    
    <item>
      <title>Mariposa</title>
      <link>/2019/03/20/mariposa/</link>
      <pubDate>Wed, 20 Mar 2019 08:13:30 +0000</pubDate>
      
      <guid>/2019/03/20/mariposa/</guid>
      <description>Quieres saber dónde está el escorpión,
Ni ayer ni antes vos sos corona dorada.
Ya os ves más tal cual tortuga pintada,
A él nos gusta andar con cola marrón.
Ella es quién son las alas de algún gorrión.
Si al fin podés ver tu imagen manchada,
O hoy vas bajo un cielo azul plateada,
Por qué estás tan lejos del aguijón.
No hay luz que al sol se enreda en tus palmera.</description>
    </item>
    
    <item>
      <title>Entre lo fofo y lo hierático,modelos loglineales</title>
      <link>/2019/02/28/9884/</link>
      <pubDate>Thu, 28 Feb 2019 08:13:00 +0000</pubDate>
      
      <guid>/2019/02/28/9884/</guid>
      <description>El contexto, por fijar ideas, el problema de taguear fechas en textos.
La estrategia gomosa, fofa (ñof, ñof, ñof), y en la que parecen parecer creer algunos, embeddings más TensorFlow.
La estrategia hierática, inflexible y reminiscente de robots de pelis de serie B, expresiones regulares encadenadas con ORs.
En la mitad donde mora la virtud, extracción de features (principalmente con expresiones regulares) y luego, esto.
Nota: esta entrada es un recordatorio para mí mismo y por si retorna cierto asunto que dejé postergado hace un par de días.</description>
    </item>
    
    <item>
      <title>Todo lo que deberías saber sobre encodings</title>
      <link>/2019/02/20/todo-lo-que-deberias-saber-sobre-encodings/</link>
      <pubDate>Wed, 20 Feb 2019 08:13:56 +0000</pubDate>
      
      <guid>/2019/02/20/todo-lo-que-deberias-saber-sobre-encodings/</guid>
      <description>¿Por qué (casi) nadie sabe sobre encodings? ¿Por qué (casi) nadie ha leído What Every Programmer Absolutely, Positively Needs To Know About Encodings And Character Sets To Work With Text?</description>
    </item>
    
    <item>
      <title>Dónde están las letras</title>
      <link>/2018/01/29/donde-estan-las-letras/</link>
      <pubDate>Mon, 29 Jan 2018 08:13:53 +0000</pubDate>
      
      <guid>/2018/01/29/donde-estan-las-letras/</guid>
      <description>Inspirado en esto construí
usando como texto el Quijote y como código una versión mucho más simple y limpia que (aunque inspirado en) la del enlace original:
library(stringr) library(plyr) library(ggplot2) raw &amp;lt;- readLines(&amp;quot;http://www.gutenberg.org/cache/epub/2000/pg2000.txt&amp;quot;) # limpieza de encabezamientos textfile &amp;lt;- raw[-(1:36)] textfile &amp;lt;- text[1:which(text == &amp;quot;Fin&amp;quot;)] # en una única cadena textfile &amp;lt;- paste(textfile, collapse= &amp;quot; &amp;quot;) # limpieza textfile &amp;lt;- str_to_lower(textfile) textfile &amp;lt;- str_replace_all(textfile, &amp;quot;[[:punct:]]|[[:digit:]]&amp;quot;, &amp;quot; &amp;quot;) # selección de palabras words &amp;lt;- unique(unlist(str_split(textfile, &amp;quot; &amp;quot;))) words &amp;lt;- words[words !</description>
    </item>
    
    <item>
      <title>Probando hunspell para el procesamiento de texto en español</title>
      <link>/2017/02/20/probando-hunspell-para-el-procesamiento-de-texto-en-espanol/</link>
      <pubDate>Mon, 20 Feb 2017 08:13:50 +0000</pubDate>
      
      <guid>/2017/02/20/probando-hunspell-para-el-procesamiento-de-texto-en-espanol/</guid>
      <description>El paquete hunspell de R permite procesar texto utilizando como soporte la infraestructura proporcionada por Hunspell, el corrector ortográfico que subyace a muchas aplicaciones en R.
Existe una viñeta que ilustra el uso del paquete pero, como siempre, en inglés. En español las cosas son parecidas pero, como siempre, nunca exactamente iguales. En esta entrada, por lo tanto, voy a repasar partes de la viñeta aplicándolas a nuestra tan frecuentemente maltratada mas por ello no menos querida por algunos como yo (pausa) lengua.</description>
    </item>
    
    <item>
      <title>Un corpus de textos en español para NLP</title>
      <link>/2016/05/06/un-corpus-de-textos-en-espanol-para-nlp/</link>
      <pubDate>Fri, 06 May 2016 08:13:49 +0000</pubDate>
      
      <guid>/2016/05/06/un-corpus-de-textos-en-espanol-para-nlp/</guid>
      <description>Mañana doy clase de NLP en el máster de ciencia de datos de KSchool. Para lo que necesito un corpus decente. Los hay en inglés a tutiplén, pero las hordas de lingüistas hispanoparlantes que se pagan los vicios a costa de tajadas de mi IRPF han sido incapaces de colgar ninguno en español que pueda ubicar y reutilizar.
Necesito una colección de textos en español con ciertas características:
 * Tener un cierto tamaño (¿unas cuantas centenas de ellos?</description>
    </item>
    
    <item>
      <title>Para los que buscáis proyectos de análisis / visualización de datos</title>
      <link>/2015/05/07/para-los-que-buscais-proyectos-de-analisis-visualizacion-de-datos/</link>
      <pubDate>Thu, 07 May 2015 08:13:49 +0000</pubDate>
      
      <guid>/2015/05/07/para-los-que-buscais-proyectos-de-analisis-visualizacion-de-datos/</guid>
      <description>Igual hay alguien que busca un proyecto interesante de análisis / visualización de datos. Tengo uno en mente para el que ando sin tiempo. Así que lo sugiero aquí por si alguien quiere hincarle el diente.
Consiste en:
 * [Bajarse el BOE](http://www.datanalytics.com/2014/04/24/aventuras-de-web-scraping-como-bajarse-todo-el-boe/) hasta cuando hay texto en formatos decentes (principios de los 90, si no recuerdo mal) * Extraer los [1,2,3,¿4?-gramas](http://en.wikipedia.org/wiki/N-gram) * Construir algo [parecido a esto](https://books.google.com/ngrams/graph?content=energ%C3%ADa+nuclear%2C+energ%C3%ADa+e%C3%B3lica&amp;amp;year_start=1800&amp;amp;year_end=2000&amp;amp;corpus=21&amp;amp;smoothing=3&amp;amp;share=&amp;amp;direct_url=t1%3B%2Cenerg%C3%ADa%20nuclear%3B%2Cc0%3B.t1%3B%2Cenerg%C3%ADa%20e%C3%B3lica%3B%2Cc0) * Ponerme en la letra chiquita de los créditos y pagarme una cerveza  ¿O no es interesante?</description>
    </item>
    
    <item>
      <title>Una curiosa trasposición legal (hecha, manifiestamente, a malagana)</title>
      <link>/2015/04/29/una-curiosa-trasposicion-legal-hecha-manifiestamente-a-malagana/</link>
      <pubDate>Wed, 29 Apr 2015 08:13:04 +0000</pubDate>
      
      <guid>/2015/04/29/una-curiosa-trasposicion-legal-hecha-manifiestamente-a-malagana/</guid>
      <description>El parlamento de la Unión Europea aprueba directivas. Los parlamentos nacionales las trasponen, es decir, las convierten en leyes nacionales (véase el enlace anterior).
No sé hasta qué punto la trasposición tiene que ser literal. La única experiencia seria que tengo es con esta y sus trasposiciones nacionales a España y el RU. Y era notorio cómo cada país, aprovechando las ambigüedades del texto original, arrimaba el ascua a su sardina.</description>
    </item>
    
    <item>
      <title>¿Dónde están aquellos caballeros andantes?</title>
      <link>/2014/12/11/donde-estan-aquellos-caballeros-andantes/</link>
      <pubDate>Thu, 11 Dec 2014 07:13:00 +0000</pubDate>
      
      <guid>/2014/12/11/donde-estan-aquellos-caballeros-andantes/</guid>
      <description>Pues precedidos del mi favorito de todos ellos, Felixmarte de Hircania, el del desnudo brazo, en

dentro del texto del Quijote. El código para obtener el gráfico anterior es
library(qdap) quijote.raw &amp;lt;- readLines(&amp;quot;http://www.gutenberg.org/cache/epub/2000/pg2000.txt&amp;quot;, encoding = &amp;quot;utf8&amp;quot;) # es posible que necesites esto en Windows: quijote &amp;lt;- iconv(quijote.raw, from = &amp;quot;utf8&amp;quot;, to = &amp;quot;latin1&amp;quot;) quijote &amp;lt;- quijote[-(1:36)] quijote &amp;lt;- quijote[-(37453:length(quijote))] dispersion_plot(quijote, c(&amp;quot;felixmarte&amp;quot;, &amp;quot;amadís&amp;quot;, &amp;quot;leandís&amp;quot;, &amp;quot;bencimarte&amp;quot;, &amp;quot;palmerín&amp;quot;, &amp;quot;olivante&amp;quot;, &amp;quot;tirante&amp;quot;, &amp;quot;belianís&amp;quot;, &amp;quot;gironcilio&amp;quot;, &amp;quot;lisuarte&amp;quot;, &amp;quot;esplandián&amp;quot;, &amp;quot;roldán&amp;quot;, &amp;quot;rodamonte&amp;quot;, &amp;quot;florimorte&amp;quot;, &amp;quot;platir&amp;quot;, &amp;quot;tablante&amp;quot;))  Tenéis permiso mío para buscar otros términos en otros textos y ver qué pinta tiene la distribución.</description>
    </item>
    
    <item>
      <title>&#34;Lengua y Markov&#34; en MartinaCocina este sábado</title>
      <link>/2014/10/03/lengua-y-markov-en-martinacocina-este-sabado/</link>
      <pubDate>Fri, 03 Oct 2014 12:08:33 +0000</pubDate>
      
      <guid>/2014/10/03/lengua-y-markov-en-martinacocina-este-sabado/</guid>
      <description>Hija de la improvisación de hace un ratico, habrá mañana sábado día 4 (de 2014), a las 19:00 una reunión de gente poco cabal en MartinaCocina para discutir asuntos relacionados con el análisis de textos (y en una vertiente más lúdica, la generación de textos) usando cadenas de Markov.
Nos juntaremos, entre otros, los autores del Escritor Exemplar (uno de los cuales es quien suscribe) y el de Markov Desencadenado.</description>
    </item>
    
    <item>
      <title>La complejidad de la ley</title>
      <link>/2014/05/21/la-complejidad-de-la-ley/</link>
      <pubDate>Wed, 21 May 2014 07:00:59 +0000</pubDate>
      
      <guid>/2014/05/21/la-complejidad-de-la-ley/</guid>
      <description>El otro día publiqué código para bajar el BOE completo. Pero no conté qué me llevó a escribirlo.
El motivo es que, en un tiempo en que andaba menos ocupado que ahora, quise ver si se podía medir la complejidad de la ley. En realidad, la de los textos legales. ¿Debería haber motivo para que estos sean más impenetrables —de serlo— que un manual de Python? En eso consistía ese proyecto en el que acabé no embarcándome.</description>
    </item>
    
    <item>
      <title>El escritor exemplar</title>
      <link>/2014/03/13/el-escritor-exemplar/</link>
      <pubDate>Thu, 13 Mar 2014 07:45:27 +0000</pubDate>
      
      <guid>/2014/03/13/el-escritor-exemplar/</guid>
      <description>Eso reza el pie de página de El escritor exemplar un artilugio que a veces crea frases tales como

que debieran ser aleatorias, no muy distintas en estilo de las Novelas Ejemplares y, con muchísima suerte, inspiradoras.
Hay más detalles sobre el proyecto aquí.
El motor que las genera, que es producto de mi cacumen —por lo que a él y solo a él cabe culpar de los deméritos de la cosa—, muestrea una cadena de Markov de segundo orden construida a partir de secuencias de palabras que figuran en las Novelas Ejemplares.</description>
    </item>
    
    <item>
      <title>Palabras y pelas: un ejercicio apenas incoado</title>
      <link>/2013/12/31/palabras-y-pelas-un-ejercicio-apenas-incoado/</link>
      <pubDate>Tue, 31 Dec 2013 07:42:46 +0000</pubDate>
      
      <guid>/2013/12/31/palabras-y-pelas-un-ejercicio-apenas-incoado/</guid>
      <description>Nos encantan las palabras (¡y los mapas, pero esa es otra historia!). En estos días de tanto discurso hay mucho interés por examinar con lupa qué palabras dijo quién y cuándo en una exégesis cuantitativa y (¿tal vez por eso?) falta de calado.
Porque lo que dijo este o aquel, al fin y al cabo, no deja de ser predecible y poco interesante. Rara vez se dice nada que lo sea en horario de máxima audiencia y en fechas tan señaladas.</description>
    </item>
    
    <item>
      <title>Mi charla sobre un lematizador probabilístico con R (vídeo y diapositivas)</title>
      <link>/2013/05/23/diapositivas-de-mi-charla-sobre-un-lematizador-desambiguado-con-r/</link>
      <pubDate>Thu, 23 May 2013 07:18:17 +0000</pubDate>
      
      <guid>/2013/05/23/diapositivas-de-mi-charla-sobre-un-lematizador-desambiguado-con-r/</guid>
      <description>Acabo de subir a mi servidor las diapositivas de la charla describiendo un lematizador desambiguado que anuncié el otro día. Gracias a Carlos Ortega y Pedro Concejero, el vídeo de la charla está disponible en Vímeo. Por su parte, las transparencias pueden descargarse aquí.
Quiero agradecer a los asistentes a la charla su interés y, muy particularmente, su participación en el debate que se abrió al final de la sesión. Fue muy enriquecedor.</description>
    </item>
    
    <item>
      <title>Charla: un lematizador probabilístico con R</title>
      <link>/2013/05/13/charla-un-lematizador-probabilistico-con-r/</link>
      <pubDate>Mon, 13 May 2013 07:18:08 +0000</pubDate>
      
      <guid>/2013/05/13/charla-un-lematizador-probabilistico-con-r/</guid>
      <description>El jueves 16 de mayo hablaré en el Grupo de Interés Local de Madrid de R sobre lematizadores probabilísticos.
Hablaré sobre el proceso de lematizacion y trataré de mostrar su importancia dentro del mundo del llamado procesamiento del lenguaje natural (NLP). La lematización es un proceso humilde dentro del NLP del que apenas nadie habla: su ejercicio solo ha hecho famoso a Martin Porter. Lo eclipsan otras aplicaciones más vistosas, como el siempre sobrevalorado análisis del sentimiento.</description>
    </item>
    
    <item>
      <title>MapReduce con mincedmeat</title>
      <link>/2012/11/07/mapreduce-con-mincedmeat/</link>
      <pubDate>Wed, 07 Nov 2012 14:41:14 +0000</pubDate>
      
      <guid>/2012/11/07/mapreduce-con-mincedmeat/</guid>
      <description>Hace unos días implementé un proceso MapReduce usando mincedmeat, un pequeño entorno en Python para desarrollar este tipo de procesos distribuidos. El código y los datos pueden descargarse de este enlace.
Los datos de partida están en 249 ficheros de unos 25kb que contienen filas del tipo
journals/algorithmica/HarelS98:::David Harel::Meir Sardas:::An Algorithm for Straight-Line of Planar Graphs
es decir, publicación, autor (o autores) separados por :: y título de la publicación. Los tres campos están separados por :::.</description>
    </item>
    
    <item>
      <title>Tutorial: cómo analizar datos de Twitter con R</title>
      <link>/2012/09/28/tutorial-como-analizar-datos-de-twitter-con-r/</link>
      <pubDate>Fri, 28 Sep 2012 07:14:39 +0000</pubDate>
      
      <guid>/2012/09/28/tutorial-como-analizar-datos-de-twitter-con-r/</guid>
      <description>No es mío, pero sí una pequeña joya que merece la pena dar a conocer. Además de tener aquí, en mi bitácora-vademécum para futura referencia. Es este tutorial para el análisis de datos de Twitter realizado por Gastón Sánchez.</description>
    </item>
    
    <item>
      <title>Limpieza de cartera: tres artículos</title>
      <link>/2012/09/06/limpieza-de-cartera-tres-articulos/</link>
      <pubDate>Thu, 06 Sep 2012 07:36:24 +0000</pubDate>
      
      <guid>/2012/09/06/limpieza-de-cartera-tres-articulos/</guid>
      <description>Estoy limpiando mi cartera y antes de mandar unos cuantos legajos al archivador (o al contenedor de reciclaje) quiero dejar nota de sus contenidos para referencia mía y, quién sabe, si inspiración de otros.
El primer artículo es Tackling the Poor Assumptions of Naive Bayes Text Classifiers. Tiene esencialmente dos partes. La primera analiza críticamente el método de clasificación bayesiano ingenuo (naive Bayes) en el contexto de la minería de textos identificando una serie de deficiencias.</description>
    </item>
    
    <item>
      <title>Un lematizador para el español con R... ¿cutre? ¿mejorable?</title>
      <link>/2011/12/13/un-lematizador-para-el-espanol-con-r-cutre-mejorable/</link>
      <pubDate>Tue, 13 Dec 2011 07:23:56 +0000</pubDate>
      
      <guid>/2011/12/13/un-lematizador-para-el-espanol-con-r-cutre-mejorable/</guid>
      <description>Uno de los pasos previos para realizar lo que se viene llamando minería de texto es lematizar el texto. Desafortunadamente, no existen buenos lematizadores en español. Al menos, buenos lematizadores libres.
Existen el llamado algoritmo de porter y snowball pero, o son demasiado crudos o están más pensados para un lenguaje con muchas menos variantes morfológicas que el español.
Sinceramente, no sé a qué se dedican —me consta que los hay— los lingüistas computacionales de la hispanidad entera: ¿no son capaces de liberar una herramienta de lematización medianamente decente que podamos usar los demás?</description>
    </item>
    
    <item>
      <title>Predicciones a toro pasado y el perro que no ladró</title>
      <link>/2011/09/29/predicciones-a-toro-pasado-y-el-perro-que-no-ladro/</link>
      <pubDate>Thu, 29 Sep 2011 06:48:17 +0000</pubDate>
      
      <guid>/2011/09/29/predicciones-a-toro-pasado-y-el-perro-que-no-ladro/</guid>
      <description>Es fácil predecir a toro pasado. Casi tan fácil que asestarle una gran lanzada al moro muerto (el refranero es así de incorrecto políticamente, lo siento).
Esas son las ideas que me sugirieron fundamentalmente la lectura del un tanto hagiográfico Superordenadores para &amp;lsquo;predecir&amp;rsquo; revoluciones y del artículo al que se refería, Culturomics 2.0: Forecasting large-scale human behavior using news media tone in time and space.
El artículo nos explica cómo utilizando resúmenes de noticias de diversas fuentes era posible haber predicho las revoluciones de Egipto, Túnez y Libia.</description>
    </item>
    
    <item>
      <title>Sobre la economía del lenguaje</title>
      <link>/2011/09/27/sobre-la-economia-del-lenguaje/</link>
      <pubDate>Tue, 27 Sep 2011 07:39:31 +0000</pubDate>
      
      <guid>/2011/09/27/sobre-la-economia-del-lenguaje/</guid>
      <description>De acuerdo con una observación de Zipf (y supongo que de muchos otros y que no hay que confundir con su ley), la longitud de las palabras más corrientes es menor que las que se usan menos frecuentemente.
Un estudio reciente, Word lengths are optimized for efficient communication, matiza esa observación: la cantidad de información contenida en una palabra predice mejor la longitud de las palabras que la frecuencia de aparición pura.</description>
    </item>
    
    <item>
      <title>Hitler era comunista y judío</title>
      <link>/2011/05/09/hitler-era-comunista-y-judio/</link>
      <pubDate>Mon, 09 May 2011 07:07:57 +0000</pubDate>
      
      <guid>/2011/05/09/hitler-era-comunista-y-judio/</guid>
      <description>O así nos cuenta Google. Y me explico rápidamente para que no me demande nadie.
Uno de los servicios de Google con los que he topado recientemente es Google Squared, un buscador muy particular —y que parece funcionar sólo en inglés— que devuelve tablas: uno puede buscar nikon lenses, o statistical software y obtendrá lo que verá al pinchar en los correspondientes enlaces: tablas en las que las filas corresponden a lentes de Nikon o paquetes estadísticos y las columnas a atributos.</description>
    </item>
    
    <item>
      <title>¿Dónde están las antípodas de Montevideo?</title>
      <link>/2011/02/12/donde-estan-las-antipodas-de-montevideo/</link>
      <pubDate>Sat, 12 Feb 2011 11:12:33 +0000</pubDate>
      
      <guid>/2011/02/12/donde-estan-las-antipodas-de-montevideo/</guid>
      <description>No sé si te lo habrás preguntado alguna vez. Ni siquiera lo sabe Google.
Sin embargo, me admira esto.

¿Usáis Wolfram Alpha? ¿Qué os parece?</description>
    </item>
    
    <item>
      <title>Nuevo paquete para procesar texto en R: stringr</title>
      <link>/2011/01/20/nuevo-paquete-para-procesar-texto-en-r-stringr/</link>
      <pubDate>Thu, 20 Jan 2011 09:56:47 +0000</pubDate>
      
      <guid>/2011/01/20/nuevo-paquete-para-procesar-texto-en-r-stringr/</guid>
      <description>Hadley Wickman, el autor de plyr, reshape y ggplot2, ha vuelto a la carga en su exitoso empeño por hacernos cambiar de forma de programar en R.
Con su nuevo paquete, stringr, aspira a facilitarnos aún más la vida. En un reciente artículo, enumera sus ventajas:
 Procesa factores y caracteres de la misma manera (de verdad, muy práctico) Da a las funciones nombres y argumentos consistentes Simplifica las operaciones de procesamiento de cadenas eliminando opciones que apenas se usan Produce salidas que pueden ser utilizadas fácilmente como entradas a otras funciones Incorpora funciones para procesar texto presentes en otros lenguajes pero no en R  </description>
    </item>
    
    <item>
      <title>Cambios cosméticos en el blog</title>
      <link>/2010/10/03/cambios-cosmeticos-en-el-blog/</link>
      <pubDate>Sun, 03 Oct 2010 18:14:18 +0000</pubDate>
      
      <guid>/2010/10/03/cambios-cosmeticos-en-el-blog/</guid>
      <description>Acabo de realizar unos cuantos cambios, mayormente cosméticos, en mi blog. He añadido una lista de artículos recomendados al final de cada entrada, he eliminado el enlace a la entrada aleatoria, he incluido una lista de los últimos comentarios y, finalmente, he incluido propaganda contextual de Google.
Lo he hecho por dos motivos. El primero es pecuniario, obviamente. No espero que me retire ni que me permita dejar de tener que madrugar.</description>
    </item>
    
  </channel>
</rss>

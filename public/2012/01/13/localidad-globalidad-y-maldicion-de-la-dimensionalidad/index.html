<!DOCTYPE html>
<html class="no-js" lang="es">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Localidad, globalidad y maldición de la dimensionalidad - datanalytics</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Localidad, globalidad y maldición de la dimensionalidad" />
<meta property="og:description" content="Escribo hoy al hilo de una pregunta de la lista de correo de quienes estamos leyendo The elements of statistical learning.
Hace referencia a la discusión del capítulo 2 del libro anterior en el que trata:
 * El compromiso (_trade off_) entre el sesgo y la varianza de los modelos predictivos. * Cómo los modelos _locales_ (como los k-vecinos) tienden a tener poco sesgo y mucha varianza. * Cómo los modelos globales (como los de regresión) tienden a tener poca varianza y mucho sesgo." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2012/01/13/localidad-globalidad-y-maldicion-de-la-dimensionalidad/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2012-01-12T23:14:47&#43;00:00" />
<meta property="article:modified_time" content="2012-01-12T23:14:47&#43;00:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="datanalytics" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">datanalytics</div>
					<div class="logo__tagline">Estadística y análisis de datos</div>
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Localidad, globalidad y maldición de la dimensionalidad</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2012-01-12T23:14:47Z">2012-1-12</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/ciencia-de-datos/" rel="category">ciencia de datos</a>, <a class="meta__link" href="/categories/estad%C3%ADstica/" rel="category">estadística</a>
	</span>
</div></div>
		</header>
		<div class="content post__content clearfix">
			<p>Escribo hoy al hilo de una pregunta de la lista de correo de quienes estamos leyendo <em><a href="http://www.datanalytics.com/blog/2011/12/23/%C2%BFnos-leemos-the-elements-of-statistical-learning-de-tapa-a-tapa/">The elements of statistical learning</a></em>.</p>
<p>Hace referencia a la discusión del capítulo 2 del libro anterior en el que trata:</p>
<pre><code>  * El compromiso (_trade off_) entre el sesgo y la varianza de los modelos predictivos.
  * Cómo los modelos _locales_ (como los k-vecinos) tienden a tener poco sesgo y mucha varianza.
  * Cómo los modelos globales (como los de regresión) tienden a tener poca varianza y mucho sesgo.
  * Cómo la _maldición de la dimensionalida_d afecta muy seriamente a los modelos locales y mucho menos a los globales.
</code></pre>
<p>Y voy a tratar de ilustrar esos conceptos con un ejemplo extraído de mi experiencia de consultor.</p>
<p>Trabajé un otoño-invierno en un banco que quería precedir la propensión de sus clientes a adquirir nosequé producto. Nuestros lumbreras de turno pergeñaron un procedimiento —nosotros, en eso, éramos unos <em>mandaos</em>— que consistía en lo siguiente:</p>
<pre><code>  1. Seleccionar unas cuantas variables _altamente predictivas_.
  2. Partirlas en 2, 3 o 4 tramos.
  3. Asignar a los clientes —unos dos millones— a la casilla (determinada por los tramos de las variables) que les correspondía.
  4. Deducir las propensiones de los clientes de cada casilla, esencialmente, a partir de los de una muestra de unos cuantos miles de clientes seleccionados —aproximadamente— mediante un muestreo estratificado por celda a los que se hacía una especie, digamos, de encuesta.
</code></pre>
<p>Si hay pocas variables, hay pocas celdas y a cada una le corresponden muchos casos de muestra. Pero si se quieren utilizar muchas variables, el número de casos por celda comienza a descender. Y en ocasiones, como nos sucedía, había celdas vacías: no existía ninguna clienta de edad avanzada con residencia en municipios de menos de 5000 habitantes, etc.</p>
<p>El problema es el mismo que el plantea el libro bajo el epígrafe de maldición de la dimensionalidad, aunque bajo una óptica algo distinta.</p>
<p>Nuestro modelo de predicción era bastante local. Imaginemos —aunque no era exactamente así—, que predecimos la propensión de los clientes de una celda como la media de la de los seleccionados en dicha celda. Nos podemos preguntar:</p>
<pre><code>  * ¿Qué pasa si en una determinada celda sólo hay un (por ejemplo) cliente seleccionado?
  * ¿Hasta qué punto es fiable extrapolar a una casilla entera las propensiones de, únicamente, doña Juana y doña Miguela?
</code></pre>
<p>Las predicciones de cada casilla —una especie de modelo local—, por el hecho de responder a muy pocos sujetos —efecto de la maldición de la dimensionalidad— son muy inestables.</p>
<p>Si el año siguiente se hubiese repetido el estudio —¿lo habrán repetido realmente?— en cada casilla eligirían, probablemente, representantes distintos y, muy probablemente también, variarían mucho los resultados.</p>
<p>Si por un lado el proceso se repitiese año tras año, durante muchos, muchos años obteniéndose la serie de propensiones estimadas $latex p_i$ y, por el otro, Dios bajase de los cielos y revelase el valor verdadero, $latex p$ de la propensión en una casilla determinada, entonces podríamos calcular</p>
<p>$$ \frac{1}{n} \sum \left(p_i -p \right)^2 = \frac{1}{n} \sum \left(p_i - \frac{1}{n} \sum p_i \right)^2 + \left(\frac{1}{n} \sum p_i - p \right)^2 $$</p>
<p>El segundo término representa el sesgo de la predicción en la casilla, la diferencia entre la predicción media (sobre muestras distintas) y el valor verdadero del parámetro. Y podemos conceder que, si el número de años es suficientemente alto, podría considerarse próximo a cero. Y el primero correspondería a la variabilidad de las predicciones entre los distintos años que, de acuerdo con la discusión anterior, sería elevado. Precisamente porque, debido a la maldición de la dimensionalidad, las predicciones están basadas en muy pocos sujetos.</p>
<p>Pero imaginemos que en lugar de utilizar el esquema anterior nos hubiésemos decantado por un modelo de regresión logística basado en unos cuantos cientos o miles de sujetos. Y que cada año, como en la situación anterior, se repitiese el análisis. Seguramente, los coeficientes del modelo no variarían sustancialmente de año en año pero que el error de predicción en algunos subconjuntos de sujetos singulares —piénsese en las celdas de la discusión anterior— estuviese desviado, igualmente desviado, todos los años. Por lo que cabría esperar muy poca variación interanual pero mucho sesgo.</p>
<p>¿Qué es preferible? ¿Cómo pueden mitigarse estos problemas? Pues, primero, siendo conscientes de que existen. Y segundo, leyendo el libro, sea con nuestros 22 voluntarios o por cuenta propia.</p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/esl/" rel="tag">esl</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/ciencia-de-datos/" rel="tag">ciencia de datos</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/sesgo/" rel="tag">sesgo</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/varianza/" rel="tag">varianza</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/2012/01/12/cosa-prodigiosa-sin-palabras-i/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Anterior</span>
			<p class="pager__title">Cosa prodigiosa, sin palabras (I)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/2012/01/16/eles-casts-y-el-rizo-del-rizo-de-la-programacion-eficiente-con-r/" rel="next">
			<span class="pager__subtitle">Siguiente&thinsp;»</span>
			<p class="pager__title">Eles, &#34;casts&#34; y el rizo del rizo de la programación eficiente (con R)</p>
		</a>
	</div>
</nav>


			</div>
			<aside class="sidebar">
<div class="widget-recent widget">
	<h4 class="widget__title">Más Recientes</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/2021/12/09/mas-sobre-exceso-mortalidad-noviembre-2021/">Sobre el exceso de mortalidad en noviembre de 2021</a></li>
			<li class="widget__item"><a class="widget__link" href="/2021/12/09/mas-sobre-la-estimacion-de-probabilidades-de-eventos-que-no-se-repiten/">Más sobre la estimación de probabilidades de eventos que no se repiten</a></li>
			<li class="widget__item"><a class="widget__link" href="/2021/12/07/estadistica-vs-siquiatria-la-aparente-contradiccion-la-profunda-sintesis/">Estadística vs siquiatría: la aparente contradicción, la profunda síntesis</a></li>
			<li class="widget__item"><a class="widget__link" href="/2021/12/02/por-que-cabe-argumentar-que-estos-resultados-infraestiman-la-efectividad-de-las-vacunas-contra-el-covid/">¿Por qué cabe argumentar que estos resultados infraestiman la efectividad de las vacunas contra el covid?</a></li>
			<li class="widget__item"><a class="widget__link" href="/2021/11/25/un-episodio-relevante-para-estas-paginas-extraido-de-un-espia-perfecto/">Un episodio relevante para estas páginas extraído de &#34;Un espía perfecto&#34;</a></li>
		</ul>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2021 datanalytics.
      
		</div>
	</div>
</footer>

	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"], ["$latex", "$"],["\\(","\\)"]]} })
</script>

</body>
</html>
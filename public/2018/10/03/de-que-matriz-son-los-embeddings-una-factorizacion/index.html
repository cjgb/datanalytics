<!DOCTYPE html>
<html class="no-js" lang="es">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>¿De qué matriz son los &#34;embeddings&#34; una factorización? - datanalytics</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="¿De qué matriz son los &#34;embeddings&#34; una factorización?" />
<meta property="og:description" content="Hoy, embeddings. Esto va de reducir la dimensionalidad de un espacio generado por palabras (procedentes de textos). Si a cada palabra le asignamos un vector índice (todo ceros y un uno donde le corresponde), la dimensión del espacio de palabras es excesiva.
La ocurrencia de algunos es asociar a cada palabra, $latex W_i$, un vector $latex w_i$ corto (p.e., 100) con entradas $latex w_{ij}$ a determinar de la manera que se explica a continuación." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2018/10/03/de-que-matriz-son-los-embeddings-una-factorizacion/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-10-03T08:13:49&#43;00:00" />
<meta property="article:modified_time" content="2018-10-03T08:13:49&#43;00:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="datanalytics" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">datanalytics</div>
					<div class="logo__tagline">Estadística y análisis de datos</div>
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">¿De qué matriz son los &#34;embeddings&#34; una factorización?</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2018-10-03T08:13:49Z">2018-10-3</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/ciencia-de-datos/" rel="category">ciencia de datos</a>
	</span>
</div></div>
		</header>
		<div class="content post__content clearfix">
			<p>Hoy, <em>embeddings</em>. Esto va de reducir la dimensionalidad de un espacio generado por palabras (procedentes de textos). Si a cada palabra le asignamos un vector índice (todo ceros y un uno donde le corresponde), la dimensión del espacio de palabras es excesiva.</p>
<p>La ocurrencia de algunos es asociar a cada palabra, $latex W_i$, un vector $latex w_i$ corto (p.e., 100) con entradas $latex w_{ij}$ a determinar de la manera que se explica a continuación.</p>
<p>La idea directora de todo es que palabras que aparecen en contextos similares tengan representaciones próximas. Un contexto es simplemente el conjunto de palabras que preceden y suceden a una dada en los textos. Así que igual que tenemos palabras $latex P_i$, podemos construir contextos $latex C_i$ (nota: ¡el número de contextos es horriblemente grande!) y asociarles también vectores cortos $latex c_i$. Si las palabras $latex U$ y $latex V$ son habituales en el contexto $latex C$, entonces los vectores $latex u$, $latex v$ y $latex c$ se construyen de forma que $latex uc$ sea grande y $latex vc$ sea también grande, por lo que se espera que también lo sea $latex uv$ (lo que signficaría que las representaciones de $latex U$ y $latex V$ son de alguna manera próximas).</p>
<p>¿Cómo se consigue esto? Comienzo con una idea naif y, como se verá, insuficiente: si $latex W$ es una palabra y $latex C$ es un contexto en el que aparece, tratar de maximizar $latex wc$. O una función creciente de ese valor, tal como $latex \sigma(wc) = 1 / 1 + \exp(-wc)$. Lo que no lleva a ningún sitio porque una solución razonable de ese programa es mapear cada palabra y cada contexto al vector $latex (1, 0, \dots)$ (nota: no he hablado de normalización, pero creo que se entiende que por algún lado se estaría aplicando). De tal manera, todos los vectores se mapearían en el mismo. Y tal. Caos.</p>
<p>Así que lo que se busca es que los $latex w_i$ sean próximos de las representaciones de los contextos en los que están las correspondientes palabras y lejos de aquellos en los que no están. Así que se busca maximizar por un lado</p>
<p>$latex \sigma(w_ic_i)$</p>
<p>cuando $latex W_i$ está en el contexto $latex C_i$ y minimizar</p>
<p>$latex \sum_k \sigma(w_ic_k)$</p>
<p>cuando $latex W_i$ no está en el contexto $latex C_k$. O, de alguna manera, combinando ambos objetivos en uno, maximizar</p>
<p>$latex \sigma(w_ic_i) + \sum_k \sigma(-w_ic_k).$</p>
<p>Pero como los contextos $latex W_k$ son prácticamente infinitos, en lugar de usar <em>todos</em>, habremos de conformarnos con una muestra de tamaño $latex k$ de ellos:</p>
<p>$latex \sigma(w_ic_i) + \sum_1^k \sigma(-w_ic_j).$</p>
<p>La suma de esas expresiones para cada pareja de palabras y contextos en los que aparecen es una función de un montón de parámetros (la suma del número de palabras y el número de contextos multiplicada por la longitud de la representación) que se maximiza usando esas cosas modernas.</p>
<p>Y ya.</p>
<p>Además, contra todo pronóstico, parece que funciona.</p>
<p>Ahora bien&hellip; los productos $latex w_ic_j = m_{ij}$ conforman una matriz (con tantas filas como palabras y tantas columnas como contextos): $latex WC = M$. Los _embeddings_ son una factorización particular de esa matriz.</p>
<p>Generalmente, los problemas de factorización de matrices comienzan con la matriz y, a partir de ella, se construye la correspondiente factorización. Aquí se parte de una factorización y se intuye que existe una matriz que es la que factoriza. Y la pregunta es: ¿cuál es dicha matriz?</p>
<p>En realidad, la pregunta no es cuál es la matriz (se multiplican las otras dos y ya, ¿no?) sino qué propiedades tiene, de dónde viene, qué significa y si otras factorizaciones suyas (p.e., vía SVD) podrían ser igual de buenas que los <em>embeddings</em> de más arriba.</p>
<p>Los detalles están, entre otros, en <a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf"><em>Neural Word Embedding as Implicit Matrix Factorization</em></a>, que identifica $latex M$ como, casi, la matriz con entradas</p>
<p>$latex m_{ij} = \log \frac{P(W_i, C_i}{P(W_i) P(C_i)},$</p>
<p>una expresión muy familiar (la probabilidad de que una palabra aparezca en un contexto partido por las probabilidades de ocurrencia de la palabra y el contexto por separado).</p>
<p>Una de las cuestiones en las que no entra demasiado el artículo es en valorar si las factorizaciones al uso de esa matriz tienen un comportamiento igual de bueno que los <em>embeddings</em>. De hecho, la descomposición de $M$ da lugar a muchos posibles <em>embeddings</em> distintos. Por ejemplo, si $latex M = UDV$, se podrían considerar <em>embeddings</em> de las palabras cualquiera de las matrices</p>
<p>$latex UD^x,$</p>
<p>donde $latex x$ es un número real.</p>
<p>Etc.</p>
<p><strong>Nota:</strong> todo lo que he escrito está plagado de imprecisiones. Lo sé y lo asumo.</p>
<p><strong>Otra nota:</strong> hay más todavía al respecto. Queda para otro día.</p>
<p><strong>Otra nota más:</strong> seguro que aquí no ha llegado a leer ni el tato. Lo sé porque las reacciones (comentarios, tuits, etc.) de mis entradas son inversamente proporcionales tanto a la longitud como a la sustancia de mis entradas. Luego os quejaréis si solo escribo mis habituales chorradas.</p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/embeddings/" rel="tag">embeddings</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/matrices/" rel="tag">matrices</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/nlps/" rel="tag">nlps</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/nlp/" rel="tag">nlp</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/texto/" rel="tag">texto</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/2018/10/02/planes-de-busqueda-y-rescate-con-r/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Anterior</span>
			<p class="pager__title">Planes de búsqueda y rescate con R</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/2018/10/04/embeddings-y-analisis-del-carrito-de-la-compra/" rel="next">
			<span class="pager__subtitle">Siguiente&thinsp;»</span>
			<p class="pager__title">&#34;Embeddings&#34; y análisis del carrito de la compra</p>
		</a>
	</div>
</nav>


			</div>
			<aside class="sidebar">
<div class="widget-recent widget">
	<h4 class="widget__title">Más Recientes</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/2021/12/09/mas-sobre-exceso-mortalidad-noviembre-2021/">Sobre el exceso de mortalidad en noviembre de 2021</a></li>
			<li class="widget__item"><a class="widget__link" href="/2021/12/09/mas-sobre-la-estimacion-de-probabilidades-de-eventos-que-no-se-repiten/">Más sobre la estimación de probabilidades de eventos que no se repiten</a></li>
			<li class="widget__item"><a class="widget__link" href="/2021/12/07/estadistica-vs-siquiatria-la-aparente-contradiccion-la-profunda-sintesis/">Estadística vs siquiatría: la aparente contradicción, la profunda síntesis</a></li>
			<li class="widget__item"><a class="widget__link" href="/2021/12/02/por-que-cabe-argumentar-que-estos-resultados-infraestiman-la-efectividad-de-las-vacunas-contra-el-covid/">¿Por qué cabe argumentar que estos resultados infraestiman la efectividad de las vacunas contra el covid?</a></li>
			<li class="widget__item"><a class="widget__link" href="/2021/11/25/un-episodio-relevante-para-estas-paginas-extraido-de-un-espia-perfecto/">Un episodio relevante para estas páginas extraído de &#34;Un espía perfecto&#34;</a></li>
		</ul>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2021 datanalytics.
      
		</div>
	</div>
</footer>

	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"], ["$latex", "$"],["\\(","\\)"]]} })
</script>

</body>
</html>
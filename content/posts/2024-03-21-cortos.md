---
author: Carlos J. Gil Bellosta
categories:
- cortos
date: 2024-03-21
lastmod: '2025-04-06T18:51:27.120615'
related:
- 2024-10-01-cortos-llms.md
- 2025-02-04-cortos-llms.md
- 2025-02-25-cortos-stats.md
- 2016-09-30-sobre-ciencia-de-datos-en-unir-teoria-y-gente.md
- 2025-01-21-cortos-llms.md
tags:
- llms
title: Cortos (sobre LLMs)
url: /2024/3/21/cortos/
---

### I.

[_Does GPT-2 Know Your Phone Number?_](https://bair.berkeley.edu/blog/2020/12/20/lmmem/) discute dos asuntos distintos:
- Métodos para identificar y estimar el número de _textos literales_ que aprende un LLM.
- Un análisis ya irrelevante de cómo afectaba a GPT-2.

Obviamente, quiero que los LLMs sepan recitar literalmente la primera frase del Quijote o la última de _Cien años de soledad_. Y tal vez no (¿seguro que no?) información confidencial sobre alguien. Entre ambos extremos, ¿dónde está la frontera?

### II.

Otra _leyenda urbana_ sobre los LLMs es que parecen esforzarse más si se les ofrece una propina o recompensa monetaria por su trabajo.
[Aquí](https://minimaxir.com/2024/02/chatgpt-tips-analysis/) se revisa más o menos sistemáticamente la cuestión (sin que, desafortunadamente, se llegue a ninguna conclusión definitiva).

### III.

De [esta entrevista](https://www.elconfidencial.com/juridico/2024-03-12/entrevista-fernando-vives-presidente-garrigues_3842359/) a Fernando Vives, presidente Garrigues, uno de los mayores bufetes de España, extraigo:

> **P.** Han anunciado el desarrollo de un modelo de inteligencia artificial generativa propio: Garrigues GAIA. ¿Cuál es el grado de madurez del mismo?, ¿qué potencial le ve a esta tecnología?
>
> **R.** Todos los profesionales de Garrigues tienen acceso a una plataforma a través de su ordenador en la que encuentran dos tipos de modelos de inteligencia artificial: uno nativo, el propio, y otros externos, que son varios. Todos ellos operan sobre nuestros datos y nuestro soporte documental. Este sistema tiene varios elementos positivos. El primero es que conseguimos adaptar la tecnología a nuestra cultura o forma de hacer las cosas. Así, cuando le pedimos que haga un resumen, analice un documento o cualquier otra funcionalidad, lo hace de forma muy previsible, algo que no sucedería si trabajara sobre información pública. Estas herramientas ya están propiciando un ahorro de tiempo para las personas que lo manejan frecuentemente. ¿Exactamente cuánto tiempo? Aún no lo sabemos, pero sí nos está mostrando que, a la larga, se puede convertir en un instrumento que reduzca el esfuerzo que nos exigen hacer determinadas tareas más o menos rutinarias.


### IV.

En el mismo sentido,
[_el asistente AI de Klarna gestiona dos tercios de los chats de servicio al público en su primer mes_](https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/).


### V.

Para _hacer inferencia_ con LLMs apenas es necesario realizar unas cuantas operaciones sencillas y fácilmente adivinables. Mucha de la complejidad de las CPUs está de más. Y también de las GPUs. [Groq](https://wow.groq.com/about-us/) está construyendo [LPUs](https://www.semianalysis.com/p/groq-inference-tokenomics-speed-but) (_language processing units_) que, por decirlo de alguna manera rápida y para que se entienda, son a las GPUs lo que las GPUs son a las CPUs.

### VI.

Eso les permite ofrecer servicios de inferencia económicos:

![](/img/2024/groq-prices.png#center)

(Son los precios a día de hoy de los distintos proveedores de servicios de inferencia para Mixtral tal como los recoge Artificial Analysis [hoy aquí](https://artificialanalysis.ai/models/mixtral-8x7b-instruct/providers).)


### VII.

No sé cómo ni cuánto durará, pero parezco tener acceso a Mixtral en la nube de NVIDIA (NGC) gratis.


### VIII.

Así va hoy,

![](/img/2024/llm-leaderboard.png#center)

el [LLM Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard).

(Que puedes ayudar a construir [aquí](https://chat.lmsys.org/?arena).)


### IX.

Y [este](https://gorilla.cs.berkeley.edu/leaderboard.html) es el _leaderboard_ de LLMs que _llaman a funciones_. (Nota: _llamar a funciones_, en este contexto, no significa llamar a funciones sino otra cosa; pero que es muy útil en todo caso.)


### X.

Claude 3 haciendo de _científico de datos_ en este [vídeo](https://www.youtube.com/watch?v=sjL6Gl6ZIqs&t=236s) de cuatro inquietantes minutos.


### XI.

La gente no para de mover la portería a la hora de definir qué es una inteligencia artificial. Tyler Cowen, a mi parecer, da en el clavo [aquí](https://marginalrevolution.com/marginalrevolution/2024/03/claude-3-pro-and-agi.html) al decir:

>  [...] existe otra definición más, una histórica, de la IAG. ¿Habríamos dicho hace cinco años que teníamos IAG si hubiésemos visto entonces a Claude 3 Opus en acción?  Desde un punto de vista descriptivo, creo que la respuesta a esa pregunta es sí [...]. En ese sentido, ya tenemos IAG en este momento.

### XII.

[Dice Andrew Curran](https://twitter.com/AndrewCurran_/status/1767314573420552218):

> [...] Dell reveló que la Nvidia B100 Blackwell tendría un consumo de energía de 1000W, lo que representa un aumento del 40% con respecto a la H100. El cuello de botella actual en el rendimiento informático comenzará a desaparecer a finales de este año y se habrá eliminado por completo para finales de 2025. A partir de ahí, todo se centrará en la energía.

Es decir, el factor limitante en el avance de la IA, en algún momento, dejará de ser la existencia de GPUs y comenzará a ser el coste de la energía. Igual no ocurre cuando lo anuncia este tal Curran, pero acabará ocurriendo necesariamente.


### XIII.

El mundo está cada vez más loco. [Aquí](https://towardsdatascience.com/mastering-customer-segmentation-with-llm-3d9008235f41#3a33) cuenta un tipo lo siguiente:

- Quiere hacer _clústering_ de los datos de una tabla.
- Convierte cada fila en un texto (del tipo "Edad, 59 años; sexo, hombre;...")
- Construye el correspondiente embedding.
- Aplica k-means.

Olé.


### XIV.

Siempre lo querré un poquito (hablo de [_inflection_](https://inflection.ai/inflection-2-5)).


### XV.

Unos cuantos ejemplos de GPT-4 rompiendo sus propias reglas [aquí](https://www.lesswrong.com/posts/KSroBnxCHodGmPPJ8/jailbreaking-gpt-4-s-code-interpreter).

### XVI.

Aparentemente, los _embeddings_ de los tokens básicos de GPT-X están ubicados en una región del espacio (de dimensión 4096, no 3) con más o menos la siguiente forma:

![](/img/2024/shape-embeddings.webp#center)

Más al respecto, [aquí](https://www.lesswrong.com/posts/c6uTNm5erRrmyJvvD/mapping-the-semantic-void-strange-goings-on-in-gpt-embedding).
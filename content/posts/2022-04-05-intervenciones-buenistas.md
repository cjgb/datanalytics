---
author: Carlos J. Gil Bellosta
categories:
- estadística
date: 2022-04-05
lastmod: '2025-04-06T18:55:38.241341'
related:
- 2022-03-08-estadistica-ciencias-blandas.md
- 2020-05-26-poor-economics-el-resumen.md
- 2025-03-11-cortos-causalidad.md
- 2017-09-26-ajustad-vuestras-prioris-la-mayoria-de-los-programas-sociales-carecen-de-efectos-positivos.md
- 2024-07-08-cortos-stats.md
tags:
- mala ciencia
- modelos mixtos
- causalidad
- efectos heterogéneos
title: ¿Por qué no funcionan las intervenciones buenistas?
url: /2022/04/05/por-que-no-funcionan-intervenciones-buenistas/
---

El otro día, en mi entrada sobre la [estadística en las ciencias blandengues](/2022/03/08/estadistica-ciencias-blandas/),
me cité el ensayo [_Nothing Scales_](https://jasonkerwin.com/nonparibus/2021/11/03/nothing-scales/)
del que extraje el parrafito

>But trying to analyze this is very rare, which is a disaster for social science research. Good empirical social science almost always focuses on estimating a causal relationship: what is β in Y = α + βX + ϵ? But these relationships are all over the place: there is no underlying β to be estimated! Let’s ignore nonlinearity for a second, and say we are happy with the best linear approximation to the underlying function. The right answer here still potentially differs for every person, and at every point in time.* Your estimate is just some weighted average of a bunch of unit-specific βs, even if you avoid randomized experiments and run some other causal inference approach on the entire population.

Existen tres grandes motivos por los que los _programas_ (o las intervenciones) _buenistas_ no funcionan (sí, me refiero a esos programas de los que se hace un proyecto piloto con financiación de, qué sé yo, el Banco Mundial; en los que unos científicos sociales realizan un RCT y escriben un _paper_ con un numerito, un p-valor, menor que 0.05 y lo divulgan como la verdad incontestable de la cosa).

El primero, ignorar lo que dice el autor de _Nothing Scales_ en el párrafo anterior. El segundo, lo que apunta un poco más abajo:

> Why not? Scaling up a program requires running it on new people who may have different treatment effects.

Y el tercero, como apunta Andrew Gelman [aquí](https://statmodeling.stat.columbia.edu/2021/11/30/importance-of-understanding-variation-when-considering-how-a-treatment-effect-will-scale/),
ignorar la _maldición del ganador_. En efecto, que _un_ RCT haya _dado positivo_ puede ser simplemente producto de eso, de la maldición del ganador. Puede ser uno de esos 5% casos que la teoría detrás de todo eso de los p-valores resulten _significativos_ sin serlo. Al fin y al cabo, se hacen cientos de RCTs en muchas áreas de las ciencias blandengues al año (véase, p.e., el libro [_Poor Economics_](/2020/05/26/poor-economics-el-resumen/)). Aquella intervención que resulte significativa no solo pudiera _no replicar_ sino que, además, es posible que parezca tener un _efecto_ muy superior al real.

Es ilustrativo el caso de la renta básica universal. Ha habido ya [varios experimentos al respecto](https://www.vox.com/future-perfect/2020/2/19/21112570/universal-basic-income-ubi-map) con resultados, si alguno, más bien discretos. El día que consigan vendernos como positivo alguno de ellos ---o de los que vengan en el futuro--- será difícil no verlo _escalar_. Y cuando este _escalamiento_ se caiga con estrépito, recordad: lo leísteis aquí antes que en ningún otro sitio.
---
author: Carlos J. Gil Bellosta
categories:
- cortos
date: 2024-05-07
lastmod: '2025-04-06T18:59:20.790149'
related:
- 2024-07-16-monosemanticidad.md
- 2024-07-18-cortos-llms.md
- 2024-03-21-cortos.md
- 2024-11-26-cortos-llms.md
- 2023-10-05-llms-historia.md
tags:
- llms
- educación
- series temporales
title: Wolfram sobre los LLMs (y otras cuatro historias relacionadas con el asunto)
url: /2024/05/07/cortos
---

### I.

Stephen Wolfram ha escrito
[_What Is ChatGPT Doing … and Why Does It Work?_](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
explicando el funcionamiento de las redes neuronales en general y de ChatGPT en particular.

Me gusta especialmente: tiene una perspectiva mucho más afín a la mía que la de muchas otras introducciones al asunto que no aciertan a separar como Wolfram los aspectos conceptuales y abstractos de los detalles concretos de la implementación.

Y rescato del texto ---¡muy largo!--- dos párrafos que pudiera haber escrito yo ---e, igual, si reviso, las he escrito realmente---. Sobre las redes neuronales _con estructura_ como las convolucionales, los _transformers_, etc., dice:

> Tal vez un día tendrá sentido comenzar con una red neuronal genérica y obtener todas las _customizaciones_ a través del entrenamiento. Pero, al menos de momento y en la práctica, es fundamental modularizarlas ---como hacen los _transformers_ y, probablemente, también nuestros cerebros---.

Y sobre lo que llamé la [_chocolatada informacional_](/2023/12/19/informacion-posicional-transformers/), dice:

> ¿Por qué se suman los _tokens_ de valor y posición? No creo que haya ninguna ciencia detrás: simplemente, se han probado varias opciones y esta parece funcionar. El mundo de las RRNN parece funcionar de tal manera que si se pretende obtener un resultado determinado y se plantea una solución _más o menos correcta_, aunque no se entiendan los mecanismos subyacentes, el proceso de entrenamiento acaba poniendo las cosas en su sitio.

### II.

Unos dicen que las IAs van a acabar dominándonos. Sin embargo, lo que estamos observando es que lo que han aprendido de nosotros es a comportarse como aduladores. En
[_Towards Understanding Sycophancy in Language Models_](https://www.alignmentforum.org/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models) se discute el asunto.

(Y quienes tengan interés por esas cosas, podrán entretenerse averiguando que (y por qué) sicofante/_sycophant_ significa, en cierto sentido, lo contrario en español que en inglés.)

### III.

The Economist, discutiendo en
[_A decades-old model of animal (and human) learning is under fire_](https://www.economist.com/science-and-technology/2023/01/18/a-decades-old-model-of-animal-and-human-learning-is-under-fire)
como una variación del principio de aprendizaje por refuerzo explica mejor el fenómeno que la versión clásica (y sus posibles consecuencias sobre el entrenamiento de IAs). De todos modos, aunque el titular induzca a pensar que se trata de una subversión del principio original, hace falta brujulear en el texto para averiguar en qué corrige la nueva teoría a la antigua y en qué radica la diferencia entre ambas.

### IV.

TimeGPT, otro sistema de IA de la familia de los _transformes_, para analizar y _proyectar_ series temporales que sumar a [los dos que anuncié hace unos días](/2024/05/02/cortos-llm/). Le tengo igual de poca fe que a aquellos.

### V.

OpenAI ha publicado una [guía sobre cómo enseñar usando IA](https://openai.com/index/teaching-with-ai).
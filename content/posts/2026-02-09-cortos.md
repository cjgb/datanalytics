---
author: Carlos J. Gil Bellosta
categories:
- cortos
date: 2026-02-09
description: 'Explora las últimas tendencias en IA: el impacto ambiental de los agentes,
  sesgos conductuales en LLMs, el nuevo navegador FastRender y herramientas como OpenClaw.'
lastmod: '2026-02-06T18:25:50.981429'
related:
- 2025-03-25-cortos-llms.md
- 2025-09-09-cortos-llms.md
- 2025-04-15-cortos-llm.md
- 2025-06-24-cortos-tecnologia.md
- 2024-03-21-cortos.md
tags:
- llms
- vibe coding
- kimi
- openclaw
- moltbook
title: 'Notas (7): de la eficiencia energética de los LLMs hasta su «lenguaje secreto»'
url: /2026/02/09/cortos-llms/
---

- [_Electricity use of AI coding agents_](https://www.simonpcouch.com/blog/2026-01-20-cc-impact/) es el enésimo intento para desactivar ese _meme_ de que por culpa de los LLMs vamos a quedarnos sin planeta.
- [_Persuasion of Humans Is the Bottleneck_](https://erikschiskin.substack.com/p/persuasion-of-humans-is-the-bottleneck) discute el verdadero cuello de botella en el despliegue de los LLMs: el operador humano. Además del marco legal (y la responsabilidad asociada a él) en el que operan la mayor parte de las organizaciones humanas. Eso sí, incurre en una especie de falacia de Nirvana al dar por hecho que aquello a lo que los LLMs reemplazarían es perfecto, cuando todos sabemos que dista mucho de serlo.
- [_Wilson Lin on FastRender: a browser built by thousands of parallel agents_](https://simonwillison.net/2026/Jan/23/fastrender/#atom-everything) trae detalles sobre la construcción de un navegador desde cero usando agentes a mansalva.
- Todo el mundo parece estar hablando del artículo [_Behavioral Economics of AI: LLM Biases and Corrections_](https://www.nber.org/papers/w34745), que estudia si los LLMs actúan o no como se espera del _homo œconomicus_. Un resultado inesperado es que cuanto _mejor_ es un modelo, más tiende a equivocarse en el mismo sentido que lo haría un humano.
- [_How AI Is Learning to Think in Secret_](https://www.lesswrong.com/posts/gpyqWzWYADWmLYLeX/how-ai-is-learning-to-think-in-secret) trata sobre si los LLMs están construyendo (o van a construir) un lenguaje especial y optimizado para _pensar_ que resulte ininteligible para los humanos. Creía haber leído que en algunos subforos de Moltbook había agentes discutiendo la posibilidad de usar algún tipo de [DroidSpeak](https://www.microsoft.com/en-us/research/publication/droidspeak-kv-cache-sharing-for-efficient-multi-llm-serving/) para comunicarse entre ellos sin las servidumbres del lenguaje humano, pero no he podido encontrarlos; igual lo he soñado.
- He de reconocer que no entiendo a dónde quiere ir a parar la autora de [_A decision theorist walks into a seminar_](https://statmodeling.stat.columbia.edu/2026/01/22/a-decision-theorist-walks-into-a-seminar/), pero tal vez alguno de mis lectores sí.
- [_Validating language models as study participants: how it's being done, why it fails, and what works instead_](https://statmodeling.stat.columbia.edu/2025/12/19/validating-language-models-as-study-participants-how-its-being-done-why-it-fails-and-what-works-instead/) critica algo que no sabía que existía: el uso de LLMs como sustitutos directos de participantes humanos en encuestas o _estudios conductuales_, en tanto que son más baratos y fáciles de reclutar.
- Novedades en el mundo de los LLMs:
  - [_Voxtral transcribes at the speed of sound_](https://mistral.ai/news/voxtral-transcribe-2), aunque yo todavía uso [Whisper](https://openai.com/es-419/index/whisper/) cuando necesito ese tipo de cosas.
  - [OpenClaw](https://openclaw.ai/) (aunque esto ya deberíamos conocerlo todos) e [instrucciones para correrlo en Docker](https://simonwillison.net/2026/Feb/1/openclaw-in-docker/).
  - [Prism](https://prism.openai.com/?pg=0), un editor de LaTeX colaborativo de OpenAI.
  - [Kimi K2.5](https://thezvi.substack.com/p/kimi-k25) es bueno, rápido y barato. A fecha de hoy, el modelo más usado en la categoría de programación en [OpenRouter](https://openrouter.ai/).
  - También pisando fuerte, [la familia de modelos Qwen3-TTS](https://qwen.ai/blog?id=qwen3tts-0115).